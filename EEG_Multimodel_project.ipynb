{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9111d97",
   "metadata": {
    "_sphinx_cell_id": "0f98f313-c450-494f-9cbb-093c3eb59f83"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class EEGMultimodalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset class for loading the multimodal EEG, Image, and Text data.\n",
    "    \n",
    "    This class implements the 5-step loading procedure described in the paper\n",
    "    by parsing the BIDS-formatted directory structure.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 bids_root,          # Path to the .../ds005589/ directory\n",
    "                 images_dir,         # Path to the .../All_images/ directory\n",
    "                 captions_path,      # Path to the captions.txt file\n",
    "                 subject_list,       # List of subjects to load, e.g., ['sub-02', 'sub-03']\n",
    "                 session_list,       # List of sessions to load, e.g., ['ses-01', 'ses-02']\n",
    "                 image_transform=None, # PyTorch transforms for the images\n",
    "                 clamp_thres=500     # Clamping threshold for EEG in microvolts\n",
    "                ):\n",
    "        \n",
    "        self.bids_root = bids_root\n",
    "        self.images_dir = images_dir\n",
    "        self.image_transform = image_transform\n",
    "        self.clamp_thres = clamp_thres\n",
    "\n",
    "        # --- Lists to store the aligned data triplets ---\n",
    "        self.all_eeg_trials = []\n",
    "        self.all_image_paths = []\n",
    "        self.all_captions = []\n",
    "        \n",
    "        print(\"Initializing dataset... This may take a moment.\")\n",
    "        \n",
    "        # --- Step 3 (Part 1): Load captions file ONCE ---\n",
    "        # This creates a fast lookup dictionary: {'image_name': (category, caption)}\n",
    "        print(f\"Loading captions from {captions_path}...\")\n",
    "        self.captions_dict = self._load_captions(captions_path)\n",
    "        print(f\"Loaded {len(self.captions_dict)} captions.\")\n",
    "\n",
    "        # --- Iterate through subjects, sessions, and runs ---\n",
    "        for sub in subject_list:\n",
    "            for ses in session_list:\n",
    "                # Per the paper, we only use the 4 'lowSpeed' runs\n",
    "                for run in ['01', '02', '03', '04']:\n",
    "                    \n",
    "                    # --- Build the file paths for this run ---\n",
    "                    session_path = os.path.join(self.bids_root, sub, ses)\n",
    "                    \n",
    "                    # File paths for the 3 files we need\n",
    "                    csv_path = os.path.join(session_path, f\"{sub}_{ses}_task-lowSpeed_run-{run}_image.csv\")\n",
    "                    npy_path = os.path.join(session_path, f\"{sub}_{ses}_task-lowSpeed_run-{run}_1000Hz.npy\")\n",
    "                    \n",
    "                    # Check that all required files exist\n",
    "                    if not (os.path.exists(csv_path) and os.path.exists(npy_path)):\n",
    "                        print(f\"Warning: Missing files for {sub} {ses} {run}. Skipping.\")\n",
    "                        continue\n",
    "                        \n",
    "                    # --- Execute the 5-Step Loading Procedure ---\n",
    "                    \n",
    "                    # 1. Parse metadata (the .csv)\n",
    "                    # This is the \"logbook\" that links trials to image names\n",
    "                    try:\n",
    "                        csv_data = pd.read_csv(csv_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading CSV {csv_path}: {e}. Skipping run.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # 2. Load EEG trials (the .npy)\n",
    "                    # This is the [N, 122, T] data array\n",
    "                    eeg_data = np.load(npy_path) # Shape [100, 122, T]\n",
    "                    \n",
    "                    # 5. Verify correspondence\n",
    "                    if eeg_data.shape[0] != len(csv_data):\n",
    "                        print(f\"Warning: Trial mismatch in {sub} {ses} {run}. \"\n",
    "                              f\"EEG has {eeg_data.shape[0]}, CSV has {len(csv_data)}. Skipping.\")\n",
    "                        continue\n",
    "                        \n",
    "                    # --- Loop through all 100 trials in this run ---\n",
    "                    for i, row in csv_data.iterrows():\n",
    "                        \n",
    "                        # Step 1: Get image identifier\n",
    "                        img_base_name = self._get_base_name(row['FilePath'])\n",
    "                        if not img_base_name:\n",
    "                            continue\n",
    "                        \n",
    "                        # Step 3: Merge with captions\n",
    "                        category, caption = self.captions_dict.get(img_base_name, (\"Unknown\", \"No Caption\"))\n",
    "                        \n",
    "                        # Step 4: Resolve image path\n",
    "                        img_path = self._find_image_path(img_base_name)\n",
    "                        if not img_path:\n",
    "                            # print(f\"Warning: Could not find image file for {img_base_name}. Skipping trial.\")\n",
    "                            continue # Skip this trial if image file is missing\n",
    "                            \n",
    "                        # --- We have a complete, aligned triplet ---\n",
    "                        self.all_eeg_trials.append(eeg_data[i])   # The [122, T] EEG data\n",
    "                        self.all_image_paths.append(img_path)     # The full path to the .jpg\n",
    "                        self.all_captions.append(caption)         # The caption string\n",
    "\n",
    "        print(f\"Found {len(self.all_eeg_trials)} total aligned trials.\")\n",
    "        \n",
    "        # --- EEG Preprocessing (Clamping & Normalization) ---\n",
    "        # Convert list of trials into one large NumPy array\n",
    "        eeg_dataset = np.array(self.all_eeg_trials, dtype=np.float32)\n",
    "        \n",
    "        # 1. Clamp (from your original code)\n",
    "        eeg_dataset[eeg_dataset >  self.clamp_thres] =  self.clamp_thres\n",
    "        eeg_dataset[eeg_dataset < -self.clamp_thres] = -self.clamp_thres\n",
    "        \n",
    "        # 2. Normalize (Global Z-Score, per-feature)\n",
    "        # We reshape to [N_total, 122*T] to normalize each (channel, timepoint)\n",
    "        # feature across the entire dataset.\n",
    "        sample_num, channel_num, time_num = eeg_dataset.shape\n",
    "        eeg_dataset_flat = eeg_dataset.reshape(sample_num, -1)\n",
    "        \n",
    "        mean = np.mean(eeg_dataset_flat, axis=0)\n",
    "        std = np.std(eeg_dataset_flat, axis=0)\n",
    "        \n",
    "        # Add a small epsilon to std to avoid division by zero\n",
    "        eeg_dataset_flat = (eeg_dataset_flat - mean) / (std + 1e-6)\n",
    "        \n",
    "        # Reshape back and store\n",
    "        self.eeg_dataset = eeg_dataset_flat.reshape(sample_num, channel_num, time_num)\n",
    "        \n",
    "        # Store the other aligned data\n",
    "        self.image_paths = self.all_image_paths\n",
    "        self.captions = self.all_captions\n",
    "        \n",
    "        print(\"Dataset initialization complete.\")\n",
    "\n",
    "    def _load_captions(self, captions_path):\n",
    "        \"\"\"Helper to load captions.txt into a dictionary.\"\"\"\n",
    "        captions_dict = {}\n",
    "        with open(captions_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t') # Assuming tab-separated\n",
    "                if len(parts) == 4:\n",
    "                    source, category, img_name, caption = parts\n",
    "                    captions_dict[img_name] = (category, caption)\n",
    "                # Handle different formatting, e.g., multiple spaces\n",
    "                elif len(parts) > 4:\n",
    "                    source, category, img_name = parts[0], parts[1], parts[2]\n",
    "                    caption = \" \".join(parts[3:]).strip()\n",
    "                    captions_dict[img_name] = (category, caption)\n",
    "        return captions_dict\n",
    "\n",
    "    def _get_base_name(self, file_path):\n",
    "        \"\"\"Helper to extract the base image name (e.g., '2009_005222').\"\"\"\n",
    "        try:\n",
    "            # os.path.basename gets the filename (e.g., '2009_005222.jpg')\n",
    "            # os.path.splitext splits it into ('2009_005222', '.jpg')\n",
    "            return os.path.splitext(os.path.basename(file_path))[0]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _find_image_path(self, img_base_name):\n",
    "        \"\"\"Helper to find the full image path, checking for extensions.\"\"\"\n",
    "        for ext in ['.jpg', '.jpeg', '.png', '.JPEG']:\n",
    "            img_path = os.path.join(self.images_dir, img_base_name + ext)\n",
    "            if os.path.exists(img_path):\n",
    "                return img_path\n",
    "        return None # Return None if no file is found\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of aligned trials.\"\"\"\n",
    "        return len(self.eeg_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns one aligned (EEG, Image, Text) triplet.\n",
    "        \"\"\"\n",
    "        # 1. Get EEG data\n",
    "        eeg_tensor = torch.tensor(self.eeg_dataset[idx]).float()\n",
    "        \n",
    "        # 2. Get Text data\n",
    "        caption = self.captions[idx] # Return as a string\n",
    "        \n",
    "        # 3. Get Image data\n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.image_transform:\n",
    "                image_tensor = self.image_transform(image)\n",
    "            else:\n",
    "                # Apply a default transform if none is provided\n",
    "                image_tensor = transforms.ToTensor()(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}. Returning a dummy image.\")\n",
    "            # Return a placeholder tensor if image is corrupt\n",
    "            image_tensor = torch.zeros(3, 224, 224) \n",
    "\n",
    "        return eeg_tensor, image_tensor, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd3f27",
   "metadata": {
    "_sphinx_cell_id": "4ae854a8-d47e-460f-b06c-8bec4ab7c424"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# --- 1. Define Your Paths ---\n",
    "# (Update these paths to match your system)\n",
    "BIDS_ROOT = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/ds005589'\n",
    "IMAGE_DIR = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/images'\n",
    "CAPTIONS_FILE = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/captions.txt'\n",
    "\n",
    "# --- 2. Define Your Subject List ---\n",
    "ALL_SUBJECTS = ['sub-02', 'sub-03', 'sub-05', 'sub-09', 'sub-14', 'sub-15', \n",
    "                'sub-17', 'sub-19', 'sub-20', 'sub-23', 'sub-24', 'sub-28', 'sub-29']\n",
    "\n",
    "# --- 3. Define Image Transforms (e.g., for CLIP) ---\n",
    "# (You would get the specific transforms from your model)\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 4. Create the 3 Datasets (Train/Val/Test) ---\n",
    "# This perfectly follows the paper's \"split by session\" rule.\n",
    "\n",
    "print(\"Creating Training Dataset...\")\n",
    "train_dataset = EEGMultimodalDataset(\n",
    "    bids_root=BIDS_ROOT,\n",
    "    images_dir=IMAGE_DIR,\n",
    "    captions_path=CAPTIONS_FILE,\n",
    "    subject_list=ALL_SUBJECTS,\n",
    "    session_list=['ses-01', 'ses-02', 'ses-03'], # 3 sessions for training\n",
    "    image_transform=image_transforms\n",
    ")\n",
    "\n",
    "print(\"\\nCreating Validation Dataset...\")\n",
    "val_dataset = EEGMultimodalDataset(\n",
    "    bids_root=BIDS_ROOT,\n",
    "    images_dir=IMAGE_DIR,\n",
    "    captions_path=CAPTIONS_FILE,\n",
    "    subject_list=ALL_SUBJECTS,\n",
    "    session_list=['ses-04'], # 1 session for validation\n",
    "    image_transform=image_transforms\n",
    ")\n",
    "\n",
    "print(\"\\nCreating Test Dataset...\")\n",
    "test_dataset = EEGMultimodalDataset(\n",
    "    bids_root=BIDS_ROOT,\n",
    "    images_dir=IMAGE_DIR,\n",
    "    captions_path=CAPTIONS_FILE,\n",
    "    subject_list=ALL_SUBJECTS,\n",
    "    session_list=['ses-05'], # 1 session for testing\n",
    "    image_transform=image_transforms\n",
    ")\n",
    "\n",
    "# --- 5. Create PyTorch DataLoaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# --- 6. Test the loader ---\n",
    "print(\"\\nTesting the training loader...\")\n",
    "eeg_batch, image_batch, caption_batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"EEG batch shape:   {eeg_batch.shape}\")\n",
    "print(f\"Image batch shape: {image_batch.shape}\")\n",
    "print(f\"Caption batch (first item): '{caption_batch[0]}'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
