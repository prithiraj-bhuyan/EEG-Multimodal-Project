{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e370b2e2",
   "metadata": {
    "_sphinx_cell_id": "62d5bc4b-7f45-4c18-b345-7b8fbecb180f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "from transformers import CLIPProcessor, CLIPModel # <--- NEW\n",
    "\n",
    "# Set Seed\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ… Using device: {DEVICE}\")\n",
    "\n",
    "# Constants\n",
    "BIDS_ROOT = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/ds005589'\n",
    "IMAGE_DIR = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/images'\n",
    "CAPTIONS_FILE = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/captions.txt'\n",
    "ALL_SUBJECTS = ['sub-02', 'sub-03', 'sub-05', 'sub-09', 'sub-14', 'sub-15', \n",
    "                'sub-17', 'sub-19', 'sub-20', 'sub-23', 'sub-24', 'sub-28', 'sub-29']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0200246",
   "metadata": {
    "_sphinx_cell_id": "85b38b42-7ddc-4cb0-be8e-a7607de21720"
   },
   "outputs": [],
   "source": [
    "class EEG_Paper_Replication_Dataset(Dataset):\n",
    "    def __init__(self, bids_root, images_dir, captions_path, \n",
    "                 subject_list, session_list, \n",
    "                 clamp_thres=500, stats=None):\n",
    "        \n",
    "        self.bids_root = bids_root\n",
    "        self.images_dir = images_dir\n",
    "        self.clamp_thres = clamp_thres\n",
    "        self.trial_metadata = []\n",
    "        \n",
    "        # Subject Mapping\n",
    "        self.subject_to_idx = {sub: i for i, sub in enumerate(sorted(list(set(subject_list))))}\n",
    "        \n",
    "        # 1. Load Captions Helper\n",
    "        self.captions_dict = self._load_captions(captions_path)\n",
    "        self.category_to_idx = {cat: i for i, cat in enumerate(sorted(set(c for c, _ in self.captions_dict.values())))}\n",
    "        \n",
    "        # 2. Scan Metadata\n",
    "        print(f\"Scanning metadata for {session_list}...\")\n",
    "        for sub in subject_list:\n",
    "            for ses in session_list:\n",
    "                for run in ['01', '02', '03', '04']:\n",
    "                    session_path = os.path.join(self.bids_root, sub, ses)\n",
    "                    csv_path = os.path.join(session_path, f\"{sub}_{ses}_task-lowSpeed_run-{run}_image.csv\")\n",
    "                    npy_path = os.path.join(session_path, f\"{sub}_{ses}_task-lowSpeed_run-{run}_1000Hz.npy\")\n",
    "                    \n",
    "                    if not (os.path.exists(csv_path) and os.path.exists(npy_path)):\n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        csv_data = pd.read_csv(csv_path)\n",
    "                        for i, row in csv_data.iterrows():\n",
    "                            img_base_name = self._get_base_name(row['FilePath'])\n",
    "                            if not img_base_name: continue\n",
    "                            \n",
    "                            category, caption = self.captions_dict.get(img_base_name, (None, None))\n",
    "                            if not category: continue\n",
    "                            \n",
    "                            self.trial_metadata.append({\n",
    "                                'npy_path': npy_path,\n",
    "                                'trial_index': i,\n",
    "                                'label': self.category_to_idx[category],\n",
    "                                'subject_id': self.subject_to_idx[sub],\n",
    "                                'caption': caption # <--- STORED CAPTION\n",
    "                            })\n",
    "                    except:\n",
    "                        continue\n",
    "        \n",
    "        print(f\"Found {len(self.trial_metadata)} trials.\")\n",
    "\n",
    "        # 3. Compute Global Statistics\n",
    "        if stats is None:\n",
    "            self.mean, self.std = self._compute_global_stats()\n",
    "        else:\n",
    "            self.mean, self.std = stats\n",
    "\n",
    "    def _compute_global_stats(self):\n",
    "        # (Same logic as before, abbreviated for brevity)\n",
    "        subset_indices = range(0, len(self.trial_metadata), 10)\n",
    "        sum_x = 0; sum_sq_x = 0; count = 0\n",
    "        for i in subset_indices:\n",
    "            meta = self.trial_metadata[i]\n",
    "            d = np.load(meta['npy_path'])[meta['trial_index']]\n",
    "            d = np.clip(d, -self.clamp_thres, self.clamp_thres)\n",
    "            sum_x += np.mean(d); sum_sq_x += np.mean(d**2); count += 1\n",
    "        global_mean = sum_x / count\n",
    "        global_std = np.sqrt((sum_sq_x / count) - (global_mean**2))\n",
    "        return float(global_mean), float(global_std)\n",
    "\n",
    "    def get_stats(self): return self.mean, self.std\n",
    "\n",
    "    def _load_captions(self, path):\n",
    "        d = {}\n",
    "        with open(path, 'r') as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 4: d[parts[2]] = (parts[1], parts[3])\n",
    "        return d\n",
    "\n",
    "    def _get_base_name(self, p):\n",
    "        try:\n",
    "            bn = os.path.splitext(os.path.basename(str(p).replace('\\\\', '/')))[0]\n",
    "            if bn.endswith('_resized'): return bn[:-8]\n",
    "            return bn\n",
    "        except: return None\n",
    "\n",
    "    def __len__(self): return len(self.trial_metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        meta = self.trial_metadata[idx]\n",
    "        eeg_data = np.load(meta['npy_path'])[meta['trial_index']]\n",
    "        eeg_data = np.clip(eeg_data, -self.clamp_thres, self.clamp_thres)\n",
    "        \n",
    "        # Standardization\n",
    "        eeg_data = (eeg_data - self.mean) / (self.std + 1e-6)\n",
    "        \n",
    "        return (torch.tensor(eeg_data, dtype=torch.float32), \n",
    "                torch.tensor(meta['label'], dtype=torch.long),\n",
    "                torch.tensor(meta['subject_id'], dtype=torch.long),\n",
    "                meta['caption']) # <--- RETURN CAPTION STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04fae6b",
   "metadata": {
    "_sphinx_cell_id": "beaecdf0-47cb-4052-8c75-4bcfaa3fc377"
   },
   "outputs": [],
   "source": [
    "# 1. Instantiate Training Dataset\n",
    "print(\"--- Init Train ---\")\n",
    "train_ds = EEG_Paper_Replication_Dataset(\n",
    "    bids_root=BIDS_ROOT, \n",
    "    images_dir=IMAGE_DIR, \n",
    "    captions_path=CAPTIONS_FILE, \n",
    "    subject_list=ALL_SUBJECTS, \n",
    "    session_list=['ses-01', 'ses-02', 'ses-03'], \n",
    "    clamp_thres=500\n",
    ")\n",
    "stats = train_ds.get_stats()\n",
    "\n",
    "# 2. Instantiate Validation Dataset\n",
    "print(\"\\n--- Init Validation ---\")\n",
    "val_ds = EEG_Paper_Replication_Dataset(\n",
    "    bids_root=BIDS_ROOT, \n",
    "    images_dir=IMAGE_DIR, \n",
    "    captions_path=CAPTIONS_FILE, \n",
    "    subject_list=ALL_SUBJECTS, \n",
    "    session_list=['ses-04'], \n",
    "    clamp_thres=500,\n",
    "    stats=stats\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719a6131",
   "metadata": {
    "_sphinx_cell_id": "c18b5805-f4c3-408f-b238-bbd920e51a8b"
   },
   "outputs": [],
   "source": [
    "class EEG_ViT_1D(nn.Module):\n",
    "    def __init__(self, num_subjects=13, num_classes=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Hyperparameters ---\n",
    "        self.patch_size = 50      # 50ms window \n",
    "        self.stride = 25          # 50% overlap\n",
    "        self.embed_dim = 128      # Feature size\n",
    "        self.num_heads = 4\n",
    "        self.depth = 2\n",
    "        \n",
    "        # --- 1. Tokenizer (The \"Patchify\" Step) ---\n",
    "        # Input: (Batch, 122, 500) -> Output: (Batch, 128, ~19)\n",
    "        self.tokenizer = nn.Sequential(\n",
    "            nn.Conv1d(122, self.embed_dim, kernel_size=self.patch_size, stride=self.stride, padding=0),\n",
    "            nn.BatchNorm1d(self.embed_dim),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        \n",
    "        # --- 2. Learnable \"Class Token\" ---\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.embed_dim))\n",
    "        \n",
    "        # --- 3. Positional Embedding ---\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 32, self.embed_dim) * 0.01)\n",
    "        \n",
    "        # --- 4. Transformer Encoder ---\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim, \n",
    "            nhead=self.num_heads, \n",
    "            dim_feedforward=512, \n",
    "            dropout=0.5, \n",
    "            batch_first=True,\n",
    "            norm_first=True \n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=self.depth)\n",
    "        \n",
    "        # --- 5. Subject-Specific Heads ---\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(self.embed_dim, num_classes) for _ in range(num_subjects)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, subject_ids):\n",
    "        # Input comes in as (Batch, 500, 122) -> We need (Batch, 122, 500)\n",
    "        if x.shape[1] == 500 and x.shape[2] == 122:\n",
    "            x = x.permute(0, 2, 1)\n",
    "            \n",
    "        # Safety Crop (in case data is >500)\n",
    "        if x.shape[2] > 500:\n",
    "            x = x[:, :, :500]\n",
    "            \n",
    "        # ----------------------------------------\n",
    "        \n",
    "        # 1. Tokenize\n",
    "        x = self.tokenizer(x)     # Output: (Batch, 128, 19)\n",
    "        x = x.permute(0, 2, 1)    # Output: (Batch, 19, 128) -> (Batch, Seq, Dim)\n",
    "        \n",
    "        b, seq_len, _ = x.shape\n",
    "        \n",
    "        # 2. Append CLS Token\n",
    "        cls_tokens = self.cls_token.expand(b, -1, -1) \n",
    "        x = torch.cat((cls_tokens, x), dim=1) # (Batch, 20, 128)\n",
    "        \n",
    "        # 3. Add Positional Embedding\n",
    "        x = x + self.pos_embedding[:, :seq_len + 1, :]\n",
    "        \n",
    "        # 4. Transformer Attention\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # 5. Extract ONLY the CLS token output\n",
    "        cls_output = x[:, 0, :]   # (Batch, 128)\n",
    "        \n",
    "        # 6. Subject Routing\n",
    "        logits = torch.zeros(x.shape[0], 20).to(x.device)\n",
    "        unique_subs = torch.unique(subject_ids)\n",
    "        \n",
    "        for sub in unique_subs:\n",
    "            mask = (subject_ids == sub)\n",
    "            logits[mask] = self.heads[sub.long()](cls_output[mask])\n",
    "            \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_sphinx_cell_id": "8ad8ae40-0001-4ee9-bf5d-40a20653f3ae"
   },
   "outputs": [],
   "source": [
    "class EEGClipProbing(nn.Module):\n",
    "    def __init__(self, original_model, clip_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Extract Backbone Components\n",
    "        self.embed_dim = original_model.embed_dim # Should be 128\n",
    "        self.tokenizer = original_model.tokenizer\n",
    "        self.cls_token = original_model.cls_token\n",
    "        self.pos_embedding = original_model.pos_embedding\n",
    "        self.transformer = original_model.transformer\n",
    "        \n",
    "        # 2. FREEZE BACKBONE\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # 3. Create NEW Heads (Trainable)\n",
    "        # Maps from Your Model Dim (128) -> CLIP Dim (512)\n",
    "        self.projection_heads = nn.ModuleList([\n",
    "            nn.Linear(self.embed_dim, clip_dim) \n",
    "            for _ in range(13)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, subject_ids):\n",
    "        # --- FROZEN BACKBONE PASS ---\n",
    "        with torch.no_grad():\n",
    "            if x.shape[1] == 500 and x.shape[2] == 122:\n",
    "                x = x.permute(0, 2, 1)\n",
    "            if x.shape[2] > 500:\n",
    "                x = x[:, :, :500]\n",
    "\n",
    "            x = self.tokenizer(x)     \n",
    "            x = x.permute(0, 2, 1)    \n",
    "            \n",
    "            b, seq_len, _ = x.shape\n",
    "            cls_tokens = self.cls_token.expand(b, -1, -1) \n",
    "            x = torch.cat((cls_tokens, x), dim=1) \n",
    "            \n",
    "            x = x + self.pos_embedding[:, :seq_len + 1, :]\n",
    "            x = self.transformer(x)\n",
    "            \n",
    "            # Extract CLS token\n",
    "            cls_output = x[:, 0, :]   # (Batch, 128)\n",
    "            \n",
    "        # --- TRAINABLE HEAD PASS ---\n",
    "        output = torch.zeros(b, 512, device=x.device)\n",
    "        unique_subs = torch.unique(subject_ids)\n",
    "        \n",
    "        for sub in unique_subs:\n",
    "            mask = (subject_ids == sub)\n",
    "            # This projection head has gradients!\n",
    "            output[mask] = self.projection_heads[sub.long()](cls_output[mask])\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_sphinx_cell_id": "e1016a21-d2cc-457c-8a75-0a40b18df05c"
   },
   "outputs": [],
   "source": [
    "class ClipAlignmentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, eeg_emb, text_emb):\n",
    "        # Normalize vectors\n",
    "        eeg_emb = F.normalize(eeg_emb, p=2, dim=-1)\n",
    "        text_emb = F.normalize(text_emb, p=2, dim=-1)\n",
    "        \n",
    "        # Similarity Matrix\n",
    "        logits = (eeg_emb @ text_emb.T) / self.temperature\n",
    "        \n",
    "        # Symmetric Loss\n",
    "        labels = torch.arange(logits.shape[0], device=logits.device)\n",
    "        loss_e2t = self.cross_entropy(logits, labels)\n",
    "        loss_t2e = self.cross_entropy(logits.T, labels)\n",
    "        \n",
    "        return (loss_e2t + loss_t2e) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_sphinx_cell_id": "306c0a77-9f33-48a4-868d-7d9feaa7f479"
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "TASK1_CHECKPOINT = \"best_model.pth\"       # The file from Task 1 (9.5% Acc)\n",
    "TASK2_CHECKPOINT = \"best_model_task2_frozen.pth\" # New file we will save\n",
    "RESUME_TRAINING = False                          # Set True to continue Task 2 training\n",
    "TOTAL_EPOCHS = 20                                # Epochs for Linear Probing\n",
    "LR = 1e-3                                        # Higher LR for heads\n",
    "\n",
    "# 1. Initialize WandB\n",
    "wandb.init(\n",
    "    project=\"eeg-retrieval\",\n",
    "    name=\"task2b-linear-probe\",\n",
    "    config={\n",
    "        \"strategy\": \"Frozen Backbone\",\n",
    "        \"architecture\": \"ViT-1D + Linear Projection\",\n",
    "        \"epochs\": TOTAL_EPOCHS,\n",
    "        \"lr\": LR,\n",
    "        \"batch_size\": 32,\n",
    "        \"clip_model\": \"ViT-B/32\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SETUP: LINEAR PROBING (BOTH MODELS FROZEN)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 2. Initialize Models\n",
    "# A. Original Backbone (Structure Only)\n",
    "original_model = EEG_ViT_1D(num_subjects=13, num_classes=20).to(DEVICE)\n",
    "\n",
    "# B. Load Task 1 Weights (Pre-requisite)\n",
    "# We ALWAYS load this first to ensure the backbone is initialized correctly\n",
    "if os.path.exists(TASK1_CHECKPOINT):\n",
    "    print(f\"âœ… Loading Task 1 Backbone from {TASK1_CHECKPOINT}...\")\n",
    "    checkpoint = torch.load(TASK1_CHECKPOINT, map_location=DEVICE)\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        original_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        original_model.load_state_dict(checkpoint)\n",
    "else:\n",
    "    print(f\"âš ï¸ WARNING: Task 1 Checkpoint '{TASK1_CHECKPOINT}' not found!\")\n",
    "    print(\"   The backbone will be random (Training will likely fail).\")\n",
    "\n",
    "# C. Wrap for Task 2 (Freezes backbone, adds new heads)\n",
    "model = EEGClipProbing(original_model, clip_dim=512).to(DEVICE)\n",
    "\n",
    "# D. Load CLIP Teacher (Frozen)\n",
    "print(\"â„ï¸ Loading & Freezing CLIP Teacher...\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "clip_model.eval()\n",
    "\n",
    "# 3. Optimizer & Loss\n",
    "# Only optimize the new projection heads!\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.AdamW(trainable_params, lr=LR, weight_decay=1e-4)\n",
    "criterion = ClipAlignmentLoss()\n",
    "\n",
    "print(f\"Trainable Params: {sum(p.numel() for p in trainable_params):,}\")\n",
    "\n",
    "# 4. Resume Logic (Task 2 Specific)\n",
    "start_epoch = 1\n",
    "best_val_recall = 0.0\n",
    "\n",
    "if RESUME_TRAINING and os.path.exists(TASK2_CHECKPOINT):\n",
    "    print(f\"ðŸ”„ Resuming Task 2 run from {TASK2_CHECKPOINT}...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(TASK2_CHECKPOINT, map_location=DEVICE)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_recall = checkpoint.get('val_recall', 0.0)\n",
    "        print(f\"   Resuming from Epoch {start_epoch} with Best R@5: {best_val_recall:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading Task 2 checkpoint: {e}. Starting fresh.\")\n",
    "else:\n",
    "    print(\"ðŸ†• Starting fresh Task 2 training.\")\n",
    "\n",
    "# 5. Training Loop\n",
    "print(f\"\\nðŸš€ Training from Epoch {start_epoch} to {TOTAL_EPOCHS}...\")\n",
    "\n",
    "for epoch in range(start_epoch, TOTAL_EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # --- TRAIN ---\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "        # Unpack: EEG, Label (unused), SubjectID, Caption\n",
    "        eeg, _, sub_ids, captions = batch\n",
    "        eeg, sub_ids = eeg.to(DEVICE), sub_ids.to(DEVICE)\n",
    "        \n",
    "        # A. Get CLIP Targets (No Grad)\n",
    "        with torch.no_grad():\n",
    "            text_inputs = clip_processor(\n",
    "                text=list(captions), \n",
    "                padding=True, truncation=True, return_tensors=\"pt\"\n",
    "            ).to(DEVICE)\n",
    "            text_features = clip_model.get_text_features(**text_inputs)\n",
    "            \n",
    "        # B. Get Student Predictions (Grad)\n",
    "        eeg_features = model(eeg, sub_ids)\n",
    "        \n",
    "        # C. Update\n",
    "        loss = criterion(eeg_features, text_features)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # --- VALIDATION (Recall@K) ---\n",
    "    model.eval()\n",
    "    all_eeg = []\n",
    "    all_text = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            eeg, _, sub_ids, captions = batch\n",
    "            eeg, sub_ids = eeg.to(DEVICE), sub_ids.to(DEVICE)\n",
    "            \n",
    "            # Student\n",
    "            e_emb = model(eeg, sub_ids)\n",
    "            e_emb = F.normalize(e_emb, p=2, dim=-1)\n",
    "            all_eeg.append(e_emb.cpu())\n",
    "            \n",
    "            # Teacher\n",
    "            t_inputs = clip_processor(\n",
    "                text=list(captions), \n",
    "                padding=True, truncation=True, return_tensors=\"pt\"\n",
    "            ).to(DEVICE)\n",
    "            t_features = clip_model.get_text_features(**t_inputs)\n",
    "            t_features = F.normalize(t_features, p=2, dim=-1)\n",
    "            all_text.append(t_features.cpu())\n",
    "            \n",
    "    # Compute Metrics\n",
    "    eeg_tensor = torch.cat(all_eeg)\n",
    "    text_tensor = torch.cat(all_text)\n",
    "    \n",
    "    # Similarity & Ranking\n",
    "    sims = eeg_tensor @ text_tensor.T\n",
    "    n = len(sims)\n",
    "    targets = torch.arange(n)\n",
    "    \n",
    "    # Top-K Accuracy\n",
    "    top5_preds = sims.topk(5, dim=1).indices\n",
    "    r1 = (top5_preds[:, 0] == targets).float().mean().item() * 100\n",
    "    r5 = (top5_preds == targets.unsqueeze(1)).any(dim=1).float().mean().item() * 100\n",
    "    \n",
    "    print(f\"Epoch {epoch:02d}: Loss {avg_loss:.4f} | Val R@1 {r1:.2f}% | Val R@5 {r5:.2f}%\")\n",
    "    \n",
    "    # Log to WandB\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": avg_loss,\n",
    "        \"val_r1\": r1,\n",
    "        \"val_r5\": r5\n",
    "    })\n",
    "    \n",
    "    # Save Checkpoint\n",
    "    if r5 > best_val_recall:\n",
    "        best_val_recall = r5\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_recall': r5,\n",
    "            'train_loss': avg_loss\n",
    "        }, TASK2_CHECKPOINT)\n",
    "        \n",
    "        wandb.save(TASK2_CHECKPOINT)\n",
    "        print(f\"  âœ… Best Model Saved! ({r5:.2f}%)\")\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
