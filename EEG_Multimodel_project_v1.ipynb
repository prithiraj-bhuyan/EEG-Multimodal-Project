{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9111d97",
   "metadata": {
    "_sphinx_cell_id": "0f98f313-c450-494f-9cbb-093c3eb59f83"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class EEGMultimodalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset class for loading the multimodal EEG, Image, and Text data.\n",
    "    \n",
    "    UPDATE 3: Now correctly handles CSVs WITH a header row.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 bids_root,          # Path to the .../ds005589/ directory\n",
    "                 images_dir,         # Path to the .../All_images/ directory\n",
    "                 captions_path,      # Path to the captions.txt file\n",
    "                 subject_list,       # List of subjects to load, e.g., ['sub-02', 'sub-03']\n",
    "                 session_list,       # List of sessions to load, e.g., ['ses-01', 'ses-02']\n",
    "                 image_transform=None, # PyTorch transforms for the images\n",
    "                 clamp_thres=500     # Clamping threshold for EEG in microvolts\n",
    "                ):\n",
    "        \n",
    "        self.bids_root = bids_root\n",
    "        self.images_dir = images_dir\n",
    "        self.image_transform = image_transform\n",
    "        self.clamp_thres = clamp_thres\n",
    "\n",
    "        self.all_eeg_trials = []\n",
    "        self.all_image_paths = []\n",
    "        self.all_captions = []\n",
    "        self.all_categories = []\n",
    "        \n",
    "        print(\"Initializing dataset... This may take a moment.\")\n",
    "        \n",
    "        print(f\"Loading captions from {captions_path}...\")\n",
    "        self.captions_dict = self._load_captions(captions_path)\n",
    "        print(f\"Loaded {len(self.captions_dict)} captions.\")\n",
    "\n",
    "        for sub in subject_list:\n",
    "            for ses in session_list:\n",
    "                for run in ['01', '02', '03', '04']:\n",
    "                    \n",
    "                    session_path = os.path.join(self.bids_root, sub, ses)\n",
    "                    csv_path = os.path.join(session_path, f\"{sub}_{ses}_task-lowSpeed_run-{run}_image.csv\")\n",
    "                    npy_path = os.path.join(session_path, f\"{sub}_{ses}_task-lowSpeed_run-{run}_1000Hz.npy\")\n",
    "                    \n",
    "                    if not (os.path.exists(csv_path) and os.path.exists(npy_path)):\n",
    "                        print(f\"Warning: Missing files for {sub} {ses} {run}. Skipping.\")\n",
    "                        continue\n",
    "                        \n",
    "                    # 1. Parse metadata (the .csv)\n",
    "                    try:\n",
    "                        # --- FIX 1: Read the CSV WITH its header ---\n",
    "                        csv_data = pd.read_csv(csv_path) \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading CSV {csv_path}: {e}. Skipping run.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # 2. Load EEG trials (the .npy)\n",
    "                    eeg_data = np.load(npy_path) \n",
    "                    \n",
    "                    # 5. Verify correspondence\n",
    "                    # Now csv_data (100) and eeg_data.shape[0] (100) should match\n",
    "                    if eeg_data.shape[0] != len(csv_data):\n",
    "                        print(f\"Warning: Trial mismatch in {sub} {ses} {run}. \"\n",
    "                              f\"EEG has {eeg_data.shape[0]}, CSV has {len(csv_data)}. Skipping.\")\n",
    "                        continue\n",
    "                        \n",
    "                    for i, row in csv_data.iterrows():\n",
    "                        \n",
    "                        # Step 1: Get image identifier\n",
    "                        # --- FIX 2: Access the column by its name 'FilePath' ---\n",
    "                        img_base_name = self._get_base_name(row['FilePath']) \n",
    "                        if not img_base_name:\n",
    "                            continue\n",
    "                        \n",
    "                        # Step 3: Merge with captions\n",
    "                        category, caption = self.captions_dict.get(img_base_name, (\"Unknown\", \"No Caption\"))\n",
    "                        \n",
    "                        # Step 4: Resolve image path\n",
    "                        img_path = self._find_image_path(img_base_name)\n",
    "                        if not img_path:\n",
    "                            # print(f\"Warning: Could not find image file for {img_base_name}. Skipping trial.\")\n",
    "                            continue \n",
    "                            \n",
    "                        self.all_eeg_trials.append(eeg_data[i])   \n",
    "                        self.all_image_paths.append(img_path)     \n",
    "                        self.all_captions.append(caption)         \n",
    "\n",
    "        print(f\"Found {len(self.all_eeg_trials)} total aligned trials.\")\n",
    "        \n",
    "        if len(self.all_eeg_trials) == 0:\n",
    "            print(\"ERROR: No trials were loaded. Check your BIDS_ROOT, IMAGE_DIR, and CAPTIONS_FILE paths.\")\n",
    "            self.eeg_dataset = np.array([])\n",
    "            self.all_categories.append(category)\n",
    "            self.image_paths = []\n",
    "            self.captions = []\n",
    "            return\n",
    "\n",
    "        eeg_dataset = np.array(self.all_eeg_trials, dtype=np.float32)\n",
    "        \n",
    "        # 1. Clamp\n",
    "        eeg_dataset[eeg_dataset >  self.clamp_thres] =  self.clamp_thres\n",
    "        eeg_dataset[eeg_dataset < -self.clamp_thres] = -self.clamp_thres\n",
    "        \n",
    "        # 2. Normalize\n",
    "        sample_num, channel_num, time_num = eeg_dataset.shape\n",
    "        eeg_dataset_flat = eeg_dataset.reshape(sample_num, -1)\n",
    "        \n",
    "        mean = np.mean(eeg_dataset_flat, axis=0)\n",
    "        std = np.std(eeg_dataset_flat, axis=0)\n",
    "        \n",
    "        eeg_dataset_flat = (eeg_dataset_flat - mean) / (std + 1e-6)\n",
    "        \n",
    "        self.eeg_dataset = eeg_dataset_flat.reshape(sample_num, channel_num, time_num)\n",
    "        self.image_paths = self.all_image_paths\n",
    "        self.captions = self.all_captions\n",
    "        self.categories = self.all_categories\n",
    "        \n",
    "        print(\"Dataset initialization complete.\")\n",
    "\n",
    "    def _load_captions(self, captions_path):\n",
    "            \"\"\"\n",
    "            Helper to load captions.txt into a dictionary.\n",
    "            UPDATED: Now handles TAB-separated columns and skips the header row.\n",
    "            \"\"\"\n",
    "            captions_dict = {}\n",
    "            with open(captions_path, 'r') as f:\n",
    "                # Skip the header line\n",
    "                next(f) \n",
    "                \n",
    "                for line in f:\n",
    "                    # CORRECT: Split by tab character\n",
    "                    parts = line.strip().split('\\t') \n",
    "\n",
    "                    # We expect exactly 4 parts: \n",
    "                    # [Source] [Category] [Image_ID] [Caption]\n",
    "                    if len(parts) == 4:\n",
    "                        source = parts[0]   # We don't strictly need this, but good to keep\n",
    "                        category = parts[1]\n",
    "                        img_name = parts[2]\n",
    "                        caption = parts[3]  # The caption is the 4th part\n",
    "                        \n",
    "                        captions_dict[img_name] = (category, caption)\n",
    "                    # else:\n",
    "                    #     print(f\"Warning: Skipping malformed line in captions.txt: {line.strip()}\")\n",
    "            return captions_dict\n",
    "\n",
    "    def _get_base_name(self, file_path):\n",
    "            \"\"\"\n",
    "            Helper to extract the base image name.\n",
    "            Also removes the '_resized' suffix. ADDED DEBUGGING.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # --- ADDED: Print the raw input ---\n",
    "\n",
    "                normalized_path = str(file_path).replace('\\\\', '/') \n",
    "    \n",
    "                # Use the normalized path\n",
    "                base_name_with_ext = os.path.basename(normalized_path) \n",
    "                # --- ADDED: Print after basename ---\n",
    "                # -----------------------------------\n",
    "    \n",
    "                base_name_resized = os.path.splitext(base_name_with_ext)[0]\n",
    "                \n",
    "                # -----------------------------------\n",
    "                \n",
    "                if base_name_resized.endswith('_resized'):\n",
    "                    base_name = base_name_resized[:-len('_resized')]\n",
    "                else:\n",
    "                    base_name = base_name_resized\n",
    "                \n",
    "               \n",
    "                return base_name \n",
    "            except Exception as e:\n",
    "                 # --- ADDED: Print any error ---\n",
    "                print(f\"ERROR in _get_base_name: {e}\")\n",
    "                # --------------------------------\n",
    "                return None\n",
    "\n",
    "    def _find_image_path(self, img_base_name):\n",
    "        \"\"\"Helper to find the full image path, checking for extensions.\"\"\"\n",
    "        for ext in ['.jpg', '.jpeg', '.png', '.JPEG']: \n",
    "            img_path = os.path.join(self.images_dir, img_base_name + ext)\n",
    "            if os.path.exists(img_path):\n",
    "                return img_path\n",
    "        return None \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of aligned trials.\"\"\"\n",
    "        return len(self.eeg_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns one aligned (EEG, Image, Text) triplet.\"\"\"\n",
    "        eeg_tensor = torch.tensor(self.eeg_dataset[idx]).float()\n",
    "        caption = self.captions[idx] \n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.image_transform:\n",
    "                image_tensor = self.image_transform(image)\n",
    "            else:\n",
    "                image_tensor = transforms.ToTensor()(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}. Returning a dummy image.\")\n",
    "            image_tensor = torch.zeros(3, 224, 224) \n",
    "\n",
    "        category = self.categories[idx]\n",
    "        return eeg_tensor, image_tensor, caption, category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bd3f27",
   "metadata": {
    "_sphinx_cell_id": "4ae854a8-d47e-460f-b06c-8bec4ab7c424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Training Dataset...\n",
      "Initializing dataset... This may take a moment.\n",
      "Loading captions from /ocean/projects/cis250019p/gandotra/11785-gp-eeg/captions.txt...\n",
      "Loaded 9825 captions.\n",
      "Found 15600 total aligned trials.\n",
      "Dataset initialization complete.\n",
      "\n",
      "Creating Validation Dataset...\n",
      "Initializing dataset... This may take a moment.\n",
      "Loading captions from /ocean/projects/cis250019p/gandotra/11785-gp-eeg/captions.txt...\n",
      "Loaded 9825 captions.\n",
      "Found 5200 total aligned trials.\n",
      "Dataset initialization complete.\n",
      "\n",
      "Creating Test Dataset...\n",
      "Initializing dataset... This may take a moment.\n",
      "Loading captions from /ocean/projects/cis250019p/gandotra/11785-gp-eeg/captions.txt...\n",
      "Loaded 9825 captions.\n",
      "Found 5200 total aligned trials.\n",
      "Dataset initialization complete.\n",
      "\n",
      "Testing the training loader...\n",
      "EEG batch shape:   torch.Size([32, 500, 122])\n",
      "Image batch shape: torch.Size([32, 3, 224, 224])\n",
      "Caption batch (first item): 'Bottle with message lying on sandy beach'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# --- 1. Define Your Paths ---\n",
    "# (Update these paths to match your system)\n",
    "BIDS_ROOT = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/ds005589'\n",
    "IMAGE_DIR = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/images'\n",
    "CAPTIONS_FILE = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/captions.txt'\n",
    "\n",
    "# --- 2. Define Your Subject List ---\n",
    "ALL_SUBJECTS = ['sub-02', 'sub-03', 'sub-05', 'sub-09', 'sub-14', 'sub-15', \n",
    "                'sub-17', 'sub-19', 'sub-20', 'sub-23', 'sub-24', 'sub-28', 'sub-29']\n",
    "\n",
    "# --- 3. Define Image Transforms (e.g., for CLIP) ---\n",
    "# (You would get the specific transforms from your model)\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 4. Create the 3 Datasets (Train/Val/Test) ---\n",
    "# This perfectly follows the paper's \"split by session\" rule.\n",
    "\n",
    "print(\"Creating Training Dataset...\")\n",
    "train_dataset = EEGMultimodalDataset(\n",
    "    bids_root=BIDS_ROOT,\n",
    "    images_dir=IMAGE_DIR,\n",
    "    captions_path=CAPTIONS_FILE,\n",
    "    subject_list=ALL_SUBJECTS,\n",
    "    session_list=['ses-01', 'ses-02', 'ses-03'], # 3 sessions for training\n",
    "    image_transform=image_transforms\n",
    ")\n",
    "\n",
    "print(\"\\nCreating Validation Dataset...\")\n",
    "val_dataset = EEGMultimodalDataset(\n",
    "    bids_root=BIDS_ROOT,\n",
    "    images_dir=IMAGE_DIR,\n",
    "    captions_path=CAPTIONS_FILE,\n",
    "    subject_list=ALL_SUBJECTS,\n",
    "    session_list=['ses-04'], # 1 session for validation\n",
    "    image_transform=image_transforms\n",
    ")\n",
    "\n",
    "print(\"\\nCreating Test Dataset...\")\n",
    "test_dataset = EEGMultimodalDataset(\n",
    "    bids_root=BIDS_ROOT,\n",
    "    images_dir=IMAGE_DIR,\n",
    "    captions_path=CAPTIONS_FILE,\n",
    "    subject_list=ALL_SUBJECTS,\n",
    "    session_list=['ses-05'], # 1 session for testing\n",
    "    image_transform=image_transforms\n",
    ")\n",
    "\n",
    "# --- 5. Create PyTorch DataLoaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# --- 6. Test the loader ---\n",
    "print(\"\\nTesting the training loader...\")\n",
    "eeg_batch, image_batch, caption_batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"EEG batch shape:   {eeg_batch.shape}\")\n",
    "print(f\"Image batch shape: {image_batch.shape}\")\n",
    "print(f\"Caption batch (first item): '{caption_batch[0]}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (idl_hw3p2)",
   "language": "python",
   "name": "hw3p2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
