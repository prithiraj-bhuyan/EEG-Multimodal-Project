{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG-Based Visual Recognition: Classification to Semantic Retrieval\n",
    "## Final Project Submission - Group 10\n",
    "\n",
    "**Team Members:** Madhavi Gulavani, Praneeth Chaitanya Jonnavithula, Prithiraj Bhuyan\n",
    "\n",
    "**Course:** 11-685 Introduction to Deep Learning (Fall 2025)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook implements:\n",
    "1. **Task 1**: EEG-based image classification with advanced architectures\n",
    "2. **Task 2A**: Image-Caption retrieval using pretrained CLIP\n",
    "3. **Task 2B**: EEG-Caption retrieval with various CLIP fine-tuning strategies\n",
    "4. **Comprehensive Evaluation**: All required metrics and analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from transformers import CLIPModel, CLIPProcessor, CLIPTokenizer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set Seed\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ… Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /jet/home/gulavani/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login to WandB\n",
    "wandb.login(key=\"825201e63a02e53435b53a136158ab39815c89a4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIDS_ROOT = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/ds005589'\n",
    "IMAGE_DIR = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/images'\n",
    "CAPTIONS_FILE = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/captions.txt'\n",
    "ALL_SUBJECTS = ['sub-02', 'sub-03', 'sub-05', 'sub-09', 'sub-14', 'sub-15', \n",
    "                'sub-17', 'sub-19', 'sub-20', 'sub-23', 'sub-24', 'sub-28', 'sub-29']\n",
    "\n",
    "class EEG_Paper_Replication_Dataset(Dataset):\n",
    "    def __init__(self, bids_root, images_dir, captions_path, \n",
    "                 subject_list, session_list, \n",
    "                 clamp_thres=500, stats=None):\n",
    "        \n",
    "        self.bids_root = bids_root\n",
    "        self.images_dir = images_dir\n",
    "        self.clamp_thres = clamp_thres\n",
    "        self.trial_metadata = []\n",
    "        \n",
    "        # --- Create Subject Mapping ---\n",
    "        # Map 'sub-02' -> 0, 'sub-03' -> 1, etc.\n",
    "        # We sort to ensure consistency across Train/Val/Test sets\n",
    "        self.subject_to_idx = {sub: i for i, sub in enumerate(sorted(list(set(subject_list))))}\n",
    "        \n",
    "        # 1. Load Captions Helper\n",
    "        self.captions_dict = self._load_captions(captions_path)\n",
    "        self.category_to_idx = {cat: i for i, cat in enumerate(sorted(set(c for c, _ in self.captions_dict.values())))}\n",
    "        \n",
    "        # 2. Scan Metadata\n",
    "        print(f\"Scanning metadata for {session_list}...\")\n",
    "        for sub in subject_list:\n",
    "            for ses in session_list:\n",
    "                for run in ['01', '02', '03', '04']:\n",
    "                    session_path = os.path.join(self.bids_root, sub, ses)\n",
    "                    csv_path = os.path.join(session_path, f\"{sub}_{ses}_task-lowSpeed_run-{run}_image.csv\")\n",
    "                    npy_path = os.path.join(session_path, f\"{sub}_{ses}_task-lowSpeed_run-{run}_1000Hz.npy\")\n",
    "                    \n",
    "                    if not (os.path.exists(csv_path) and os.path.exists(npy_path)):\n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        csv_data = pd.read_csv(csv_path)\n",
    "                        for i, row in csv_data.iterrows():\n",
    "                            img_base_name = self._get_base_name(row['FilePath'])\n",
    "                            if not img_base_name: continue\n",
    "                            \n",
    "                            category, caption = self.captions_dict.get(img_base_name, (None, None))\n",
    "                            if not category: continue\n",
    "                            \n",
    "                            self.trial_metadata.append({\n",
    "                                'npy_path': npy_path,\n",
    "                                'trial_index': i,\n",
    "                                'label': self.category_to_idx[category],\n",
    "                                'subject_id': self.subject_to_idx[sub],\n",
    "                                'image_name': img_base_name,\n",
    "                                'caption': caption \n",
    "                            })\n",
    "                    except:\n",
    "                        continue\n",
    "        \n",
    "        print(f\"Found {len(self.trial_metadata)} trials.\")\n",
    "\n",
    "        # 3. Compute Global Statistics (GFS)\n",
    "        if stats is None:\n",
    "            print(\"Computing Global Statistics (this takes ~1 min)...\")\n",
    "            self.mean, self.std = self._compute_global_stats()\n",
    "        else:\n",
    "            self.mean, self.std = stats\n",
    "\n",
    "    def _compute_global_stats(self):\n",
    "        subset_indices = range(0, len(self.trial_metadata), 10)\n",
    "        sum_x = 0\n",
    "        sum_sq_x = 0\n",
    "        count = 0\n",
    "        \n",
    "        for i in tqdm(subset_indices, desc=\"Calculating Stats\"):\n",
    "            meta = self.trial_metadata[i]\n",
    "            d = np.load(meta['npy_path'])[meta['trial_index']]\n",
    "            d = np.clip(d, -self.clamp_thres, self.clamp_thres)\n",
    "            \n",
    "            sum_x += np.mean(d)\n",
    "            sum_sq_x += np.mean(d**2)\n",
    "            count += 1\n",
    "            \n",
    "        global_mean = sum_x / count\n",
    "        global_std = np.sqrt((sum_sq_x / count) - (global_mean**2))\n",
    "        return float(global_mean), float(global_std)\n",
    "\n",
    "    def get_stats(self): return self.mean, self.std\n",
    "\n",
    "    def _load_captions(self, path):\n",
    "        d = {}\n",
    "        with open(path, 'r') as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 4: d[parts[2]] = (parts[1], parts[3])\n",
    "        return d\n",
    "\n",
    "    def _get_base_name(self, p):\n",
    "        try:\n",
    "            bn = os.path.splitext(os.path.basename(str(p).replace('\\\\', '/')))[0]\n",
    "            if bn.endswith('_resized'): return bn[:-8]\n",
    "            return bn\n",
    "        except: return None\n",
    "\n",
    "    def _get_image_path(self, image_name):\n",
    "        \"\"\"Find full path to image file\"\"\"\n",
    "        for ext in ['.jpg', '.jpeg', '.png', '.JPEG', '.JPG']:\n",
    "            path = os.path.join(self.images_dir, image_name + ext)\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "        return None  # Image not found\n",
    "\n",
    "    def __len__(self): return len(self.trial_metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        meta = self.trial_metadata[idx]\n",
    "        eeg_data = np.load(meta['npy_path'])[meta['trial_index']]\n",
    "        eeg_data = np.clip(eeg_data, -self.clamp_thres, self.clamp_thres)\n",
    "        # Global Feature Standardization\n",
    "        eeg_data = (eeg_data - self.mean) / (self.std + 1e-6)\n",
    "\n",
    "        img_path = self._get_image_path(meta['image_name'])\n",
    "        caption = meta['caption']\n",
    "        \n",
    "        return {\n",
    "            'eeg': torch.tensor(eeg_data, dtype=torch.float32),\n",
    "            'label': torch.tensor(meta['label'], dtype=torch.long),\n",
    "            'subject_id': torch.tensor(meta['subject_id'], dtype=torch.long),\n",
    "            'image_path': img_path,  # for Task 2\n",
    "            'caption': caption  # for Task 2\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Init Train ---\n",
      "Scanning metadata for ['ses-01', 'ses-02', 'ses-03']...\n",
      "Found 15600 trials.\n",
      "Computing Global Statistics (this takes ~1 min)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4835dfdc9f4bb39728bb89b67aaf16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Stats:   0%|          | 0/1560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Init Validation ---\n",
      "Scanning metadata for ['ses-04']...\n",
      "Found 5200 trials.\n",
      "Loading Test Set (Session 5)...\n",
      "Scanning metadata for ['ses-05']...\n",
      "Found 5200 trials.\n",
      "\n",
      "âœ… Loaders Ready: 488 training batches, 163 validation batches.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE DATASETS AND DATALOADERS\n",
    "# ============================================================================\n",
    "# 1. Create Transforms (Standard ImageNet stats)\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 2. Instantiate Training Dataset\n",
    "print(\"--- Init Train ---\")\n",
    "train_ds = EEG_Paper_Replication_Dataset(\n",
    "    bids_root=BIDS_ROOT, \n",
    "    images_dir=IMAGE_DIR, \n",
    "    captions_path=CAPTIONS_FILE, \n",
    "    subject_list=ALL_SUBJECTS, \n",
    "    session_list=['ses-01', 'ses-02', 'ses-03'], \n",
    "    clamp_thres=500\n",
    ")\n",
    "# Save stats to use for validation (Prevent data leakage)\n",
    "stats = train_ds.get_stats()\n",
    "\n",
    "# 3. Instantiate Validation Dataset\n",
    "print(\"\\n--- Init Validation ---\")\n",
    "val_ds = EEG_Paper_Replication_Dataset(\n",
    "    bids_root=BIDS_ROOT, \n",
    "    images_dir=IMAGE_DIR, \n",
    "    captions_path=CAPTIONS_FILE, \n",
    "    subject_list=ALL_SUBJECTS, \n",
    "    session_list=['ses-04'], \n",
    "    clamp_thres=500,\n",
    "    stats=stats # <--- IMPORTANT: Use training stats\n",
    ")\n",
    "\n",
    "print(\"Loading Test Set (Session 5)...\")\n",
    "test_ds = EEG_Paper_Replication_Dataset(\n",
    "    BIDS_ROOT, IMAGE_DIR, CAPTIONS_FILE, ALL_SUBJECTS, \n",
    "    ['ses-05'], # FINAL TEST SET\n",
    "    stats=stats,\n",
    "    clamp_thres=500\n",
    ")\n",
    "\n",
    "# 4. Define The Loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"\\nâœ… Loaders Ready: {len(train_loader)} training batches, {len(val_loader)} validation batches.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: EEG Classification\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "We implement a multi-head architecture with:\n",
    "1. **CNN Feature Extractor**: Extracts temporal features from EEG channels\n",
    "2. **Transformer Backbone**: Models relationships across channels (shared across subjects)\n",
    "3. **Subject-Specific Heads**: Separate classification heads for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG_ViT_1D(nn.Module):\n",
    "    def __init__(self, num_subjects=13, num_classes=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Hyperparameters ---\n",
    "        self.patch_size = 50      # 50ms window \n",
    "        self.stride = 25          # 50% overlap\n",
    "        self.embed_dim = 128      # Feature size\n",
    "        self.num_heads = 4\n",
    "        self.depth = 2\n",
    "        \n",
    "        # --- 1. Tokenizer (The \"Patchify\" Step) ---\n",
    "        # Input: (Batch, 122, 500) -> Output: (Batch, 128, ~19)\n",
    "        self.tokenizer = nn.Sequential(\n",
    "            nn.Conv1d(122, self.embed_dim, kernel_size=self.patch_size, stride=self.stride, padding=0),\n",
    "            nn.BatchNorm1d(self.embed_dim),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        \n",
    "        # --- 2. Learnable \"Class Token\" ---\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.embed_dim))\n",
    "        \n",
    "        # --- 3. Positional Embedding ---\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 32, self.embed_dim) * 0.01)\n",
    "        \n",
    "        # --- 4. Transformer Encoder ---\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim, \n",
    "            nhead=self.num_heads, \n",
    "            dim_feedforward=512, \n",
    "            dropout=0.5, \n",
    "            batch_first=True,\n",
    "            norm_first=True \n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=self.depth)\n",
    "        \n",
    "        # --- 5. Subject-Specific Heads ---\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(self.embed_dim, num_classes) for _ in range(num_subjects)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, subject_ids):\n",
    "        # Input comes in as (Batch, 500, 122) -> We need (Batch, 122, 500)\n",
    "        if x.shape[1] == 500 and x.shape[2] == 122:\n",
    "            x = x.permute(0, 2, 1)\n",
    "            \n",
    "        # Safety Crop (in case data is >500)\n",
    "        if x.shape[2] > 500:\n",
    "            x = x[:, :, :500]\n",
    "            \n",
    "        # ----------------------------------------\n",
    "        \n",
    "        # 1. Tokenize\n",
    "        x = self.tokenizer(x)     # Output: (Batch, 128, 19)\n",
    "        x = x.permute(0, 2, 1)    # Output: (Batch, 19, 128) -> (Batch, Seq, Dim)\n",
    "        \n",
    "        b, seq_len, _ = x.shape\n",
    "        \n",
    "        # 2. Append CLS Token\n",
    "        cls_tokens = self.cls_token.expand(b, -1, -1) \n",
    "        x = torch.cat((cls_tokens, x), dim=1) # (Batch, 20, 128)\n",
    "        \n",
    "        # 3. Add Positional Embedding\n",
    "        x = x + self.pos_embedding[:, :seq_len + 1, :]\n",
    "        \n",
    "        # 4. Transformer Attention\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # 5. Extract ONLY the CLS token output\n",
    "        cls_output = x[:, 0, :]   # (Batch, 128)\n",
    "        \n",
    "        # 6. Subject Routing\n",
    "        logits = torch.zeros(x.shape[0], 20).to(x.device)\n",
    "        unique_subs = torch.unique(subject_ids)\n",
    "        \n",
    "        for sub in unique_subs:\n",
    "            mask = (subject_ids == sub)\n",
    "            logits[mask] = self.heads[sub.long()](cls_output[mask])\n",
    "            \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmgulavan\u001b[0m (\u001b[33mmgulavan-carnegie-mellon-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/jet/home/gulavani/wandb/run-20251205_030616-fow6pcn4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mgulavan-carnegie-mellon-university/eeg-classification/runs/fow6pcn4' target=\"_blank\">vit-1d-run</a></strong> to <a href='https://wandb.ai/mgulavan-carnegie-mellon-university/eeg-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mgulavan-carnegie-mellon-university/eeg-classification' target=\"_blank\">https://wandb.ai/mgulavan-carnegie-mellon-university/eeg-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mgulavan-carnegie-mellon-university/eeg-classification/runs/fow6pcn4' target=\"_blank\">https://wandb.ai/mgulavan-carnegie-mellon-university/eeg-classification/runs/fow6pcn4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 1,215,492\n",
      "ðŸ†• Starting a fresh training run.\n",
      "\n",
      "ðŸš€ Training from Epoch 1 to 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/gulavani/.conda/envs/eegenv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43c528e23204b0c80093c289c00037c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     51\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     54\u001b[0m     eeg \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meeg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     55\u001b[0m     label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1482\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1482\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1434\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1433\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1434\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1435\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1436\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1275\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1275\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1278\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "RESUME_TRAINING = False\n",
    "CHECKPOINT_PATH = \"best_model_vit_1d.pth\"\n",
    "TOTAL_EPOCHS = 100\n",
    "\n",
    "wandb.init(\n",
    "    project=\"eeg-classification\",\n",
    "    name=\"vit-1d-run\",\n",
    "    config={\n",
    "        \"architecture\": \"ViT-1D\",\n",
    "        \"dataset\": \"GFS\",\n",
    "        \"epochs\": TOTAL_EPOCHS,\n",
    "        \"lr\": 1e-3\n",
    "    }\n",
    ")\n",
    "\n",
    "model = EEG_ViT_1D(num_subjects=13, num_classes=20).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "start_epoch = 1\n",
    "best_val_acc = 0.0\n",
    "\n",
    "if RESUME_TRAINING and os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"ðŸ”„ Attempting to resume from {CHECKPOINT_PATH}...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "        \n",
    "        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            if 'optimizer_state_dict' in checkpoint:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "            best_val_acc = checkpoint.get('val_acc', 0.0)\n",
    "            print(f\"âœ… Resuming from Epoch {start_epoch} with Best Acc {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "            print(f\"âš ï¸ Only weights found. Resuming from Epoch 1.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading checkpoint: {e}. Starting fresh.\")\n",
    "else:\n",
    "    print(\"ðŸ†• Starting a fresh training run.\")\n",
    "\n",
    "print(f\"\\nðŸš€ Training from Epoch {start_epoch} to {TOTAL_EPOCHS}...\")\n",
    "\n",
    "for epoch in range(start_epoch, TOTAL_EPOCHS + 1):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "        eeg = batch['eeg'].to(DEVICE)\n",
    "        label = batch['label'].to(DEVICE)\n",
    "        sub_id = batch['subject_id'].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(eeg, sub_id)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, pred = outputs.max(1)\n",
    "        correct += pred.eq(label).sum().item()\n",
    "        total += label.size(0)\n",
    "        \n",
    "    train_acc = 100. * correct / total\n",
    "    \n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            eeg = batch['eeg'].to(DEVICE)\n",
    "            label = batch['label'].to(DEVICE)\n",
    "            sub_id = batch['subject_id'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(eeg, sub_id)\n",
    "            _, pred = outputs.max(1)\n",
    "            val_correct += pred.eq(label).sum().item()\n",
    "            val_total += label.size(0)\n",
    "            \n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    \n",
    "    print(f\"Epoch {epoch:02d}: Train Acc {train_acc:.2f}% | Val Acc {val_acc:.2f}%\")\n",
    "    \n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"train_loss\": loss.item()\n",
    "    })\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'train_acc': train_acc\n",
    "        }, CHECKPOINT_PATH)\n",
    "        \n",
    "        wandb.save(CHECKPOINT_PATH) \n",
    "        \n",
    "        print(f\"  âœ… Best Model Saved & Uploaded! ({val_acc:.2f}%)\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_detailed(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    subject_results = {i: {'correct': 0, 'total': 0} for i in range(len(ALL_SUBJECTS))}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for eeg, label, sub_id in tqdm(loader, desc=\"Testing\"):\n",
    "            eeg, label, sub_id = eeg.to(device), label.to(device), sub_id.to(device)\n",
    "            \n",
    "            outputs = model(eeg, sub_id)\n",
    "            _, preds = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            \n",
    "            for i in range(len(label)):\n",
    "                sid = sub_id[i].item()\n",
    "                is_correct = (preds[i] == label[i]).item()\n",
    "                subject_results[sid]['total'] += 1\n",
    "                subject_results[sid]['correct'] += is_correct\n",
    "                \n",
    "    return all_labels, all_preds, subject_results\n",
    "\n",
    "print(\"Running Final Evaluation...\")\n",
    "y_true, y_pred, sub_metrics = evaluate_detailed(model, test_loader, DEVICE)\n",
    "\n",
    "overall_acc = accuracy_score(y_true, y_pred) * 100\n",
    "print(f\"\\nðŸ† Final Test Accuracy: {overall_acc:.2f}%\")\n",
    "\n",
    "sub_accs = []\n",
    "sub_names = []\n",
    "for sid, metrics in sub_metrics.items():\n",
    "    if metrics['total'] > 0:\n",
    "        acc = (metrics['correct'] / metrics['total']) * 100\n",
    "        sub_accs.append(acc)\n",
    "        sub_names.append(ALL_SUBJECTS[sid])\n",
    "        \n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(x=sub_names, y=sub_accs, palette=\"viridis\")\n",
    "plt.axhline(y=overall_acc, color='r', linestyle='--', label=f'Avg: {overall_acc:.1f}%')\n",
    "plt.axhline(y=5.0, color='gray', linestyle='--', label='Random Chance (5%)')\n",
    "plt.title(\"Per-Subject Classification Accuracy (Session 5)\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "cat_map = {v: k for k, v in test_ds.category_to_idx.items()}\n",
    "cat_names = [cat_map[i] for i in range(len(cat_map))]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm_norm, annot=False, fmt=\".2f\", cmap=\"Blues\", xticklabels=cat_names, yticklabels=cat_names)\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2A: Image-Caption Retrieval with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TASK 2A: IMAGE-CAPTION RETRIEVAL WITH CLIP\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ CLIP model loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 2A: IMAGE-CAPTION RETRIEVAL WITH CLIP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load pretrained CLIP\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "clip_model.eval()\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"âœ“ CLIP model loaded\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_clip_embeddings(dataset, clip_model, clip_processor, clip_tokenizer, \n",
    "                            device, batch_size=32):\n",
    "    \"\"\"Extract CLIP embeddings for images and captions\"\"\"\n",
    "    \n",
    "    # Get unique images and captions\n",
    "    unique_data = {}\n",
    "    for item in dataset.trial_metadata:\n",
    "        img_name = item['image_name']  # Changed\n",
    "        if img_name not in unique_data:\n",
    "            img_path = dataset._get_image_path(img_name)\n",
    "            unique_data[img_name] = {\n",
    "                'caption': item['caption'],\n",
    "                'category_label': item['label'],  # Changed\n",
    "                'img_path': img_path\n",
    "            }\n",
    "    \n",
    "    image_names = list(unique_data.keys())\n",
    "    captions = [unique_data[name]['caption'] for name in image_names]\n",
    "    labels = torch.tensor([unique_data[name]['category_label'] for name in image_names])\n",
    "    \n",
    "    print(f\"Extracting embeddings for {len(image_names)} unique images...\")\n",
    "    \n",
    "    image_embeddings = []\n",
    "    text_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(image_names), batch_size)):\n",
    "        batch_names = image_names[i:i+batch_size]\n",
    "        batch_captions = captions[i:i+batch_size]\n",
    "        \n",
    "        # Load images\n",
    "        batch_images = []\n",
    "        for name in batch_names:\n",
    "            try:\n",
    "                img = Image.open(unique_data[name]['img_path']).convert('RGB')\n",
    "                batch_images.append(img)\n",
    "            except:\n",
    "                batch_images.append(Image.new('RGB', (224, 224)))\n",
    "        \n",
    "        # Process\n",
    "        image_inputs = clip_processor(images=batch_images, return_tensors=\"pt\", padding=True)\n",
    "        image_inputs = {k: v.to(device) for k, v in image_inputs.items()}\n",
    "        \n",
    "        text_inputs = clip_tokenizer(batch_captions, padding=True, truncation=True, \n",
    "                                    return_tensors=\"pt\", max_length=77)\n",
    "        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "        \n",
    "        # Get embeddings\n",
    "        image_features = clip_model.get_image_features(**image_inputs)\n",
    "        text_features = clip_model.get_text_features(**text_inputs)\n",
    "        \n",
    "        # Normalize\n",
    "        image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "        text_features = F.normalize(text_features, p=2, dim=-1)\n",
    "        \n",
    "        image_embeddings.append(image_features.cpu())\n",
    "        text_embeddings.append(text_features.cpu())\n",
    "    \n",
    "    image_embeddings = torch.cat(image_embeddings, dim=0)\n",
    "    text_embeddings = torch.cat(text_embeddings, dim=0)\n",
    "    \n",
    "    return {\n",
    "        'image_names': image_names,\n",
    "        'captions': captions,\n",
    "        'labels': labels,\n",
    "        'image_embeddings': image_embeddings,\n",
    "        'text_embeddings': text_embeddings\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_recall_at_k(similarities, k_values=[1, 3, 5], class_aware=False, labels=None):\n",
    "    \"\"\"Compute Recall@K\"\"\"\n",
    "    N = similarities.shape[0]\n",
    "    \n",
    "    # Move similarities to same device for sorting\n",
    "    device = similarities.device\n",
    "    top_k_indices = torch.argsort(similarities, dim=1, descending=True)\n",
    "    \n",
    "    # Ensure labels are on the same device if using class-aware\n",
    "    if class_aware and labels is not None:\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        correct = 0\n",
    "        \n",
    "        for i in range(N):\n",
    "            top_k = top_k_indices[i, :k]\n",
    "            \n",
    "            if class_aware and labels is not None:\n",
    "                query_label = labels[i]\n",
    "                retrieved_labels = labels[top_k]\n",
    "                if (retrieved_labels == query_label).any():\n",
    "                    correct += 1\n",
    "            else:\n",
    "                if i in top_k:\n",
    "                    correct += 1\n",
    "        \n",
    "        results[f\"Recall@{k}\"] = 100.0 * correct / N\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_map(similarities, class_aware=False, labels=None):\n",
    "    \"\"\"Compute Mean Average Precision\"\"\"\n",
    "    N = similarities.shape[0]\n",
    "    device = similarities.device\n",
    "    \n",
    "    # Ensure everything is on CPU for numpy operations\n",
    "    similarities_cpu = similarities.cpu() if similarities.is_cuda else similarities\n",
    "    sorted_indices = torch.argsort(similarities_cpu, dim=1, descending=True)\n",
    "    \n",
    "    if labels is not None:\n",
    "        labels_cpu = labels.cpu() if labels.is_cuda else labels\n",
    "        labels_np = labels_cpu.numpy()\n",
    "    \n",
    "    average_precisions = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        sorted_idx = sorted_indices[i]\n",
    "        \n",
    "        if class_aware and labels is not None:\n",
    "            query_label = labels_np[i]\n",
    "            relevant_mask = (labels_np == query_label)\n",
    "        else:\n",
    "            relevant_mask = np.zeros(N, dtype=bool)\n",
    "            relevant_mask[i] = True\n",
    "        \n",
    "        precisions = []\n",
    "        num_relevant = 0\n",
    "        \n",
    "        for rank, idx in enumerate(sorted_idx, 1):\n",
    "            if relevant_mask[idx]:\n",
    "                num_relevant += 1\n",
    "                precisions.append(num_relevant / rank)\n",
    "        \n",
    "        if precisions:\n",
    "            average_precisions.append(np.mean(precisions))\n",
    "        else:\n",
    "            average_precisions.append(0.0)\n",
    "    \n",
    "    return np.mean(average_precisions)\n",
    "\n",
    "\n",
    "def evaluate_task2a(test_dataset, clip_model, clip_processor, clip_tokenizer, device):\n",
    "    \"\"\"Full evaluation for Task 2A\"\"\"\n",
    "    \n",
    "    # Extract embeddings\n",
    "    embeddings = extract_clip_embeddings(\n",
    "        test_dataset, clip_model, clip_processor, clip_tokenizer, device)\n",
    "    \n",
    "    image_emb = embeddings['image_embeddings'].to(device)\n",
    "    text_emb = embeddings['text_embeddings'].to(device)\n",
    "    labels = embeddings['labels']  # Keep on CPU initially\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = torch.mm(image_emb, text_emb.t())\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TASK 2A: IMAGE-CAPTION RETRIEVAL RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Instance-level Recall@K\n",
    "    print(\"\\n1. Instance-Level Recall@K:\")\n",
    "    instance_recall = compute_recall_at_k(similarities, k_values=[1, 3, 5])\n",
    "    for k, v in instance_recall.items():\n",
    "        print(f\"   {k}: {v:.2f}%\")\n",
    "    \n",
    "    # Class-aware Recall@K (labels will be moved to device inside function)\n",
    "    print(\"\\n2. Class-Aware Recall@K:\")\n",
    "    class_recall = compute_recall_at_k(similarities, k_values=[1, 3, 5], \n",
    "                                      class_aware=True, labels=labels)\n",
    "    for k, v in class_recall.items():\n",
    "        print(f\"   {k}: {v:.2f}%\")\n",
    "    \n",
    "    # MAP (will be computed on CPU inside function)\n",
    "    print(\"\\n3. Mean Average Precision:\")\n",
    "    instance_map = compute_map(similarities, labels=labels)\n",
    "    print(f\"   Instance-Level MAP: {instance_map:.4f}\")\n",
    "    \n",
    "    class_map = compute_map(similarities, class_aware=True, labels=labels)\n",
    "    print(f\"   Class-Aware MAP: {class_map:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'instance_recall': instance_recall,\n",
    "        'class_recall': class_recall,\n",
    "        'instance_map': instance_map,\n",
    "        'class_map': class_map,\n",
    "        'embeddings': embeddings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING TASK 2A: IMAGE-CAPTION RETRIEVAL\n",
      "============================================================\n",
      "Extracting embeddings for 4046 unique images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf04785a09534f54a6cfbbab1c5bf3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TASK 2A: IMAGE-CAPTION RETRIEVAL RESULTS\n",
      "============================================================\n",
      "\n",
      "1. Instance-Level Recall@K:\n",
      "   Recall@1: 27.63%\n",
      "   Recall@3: 44.86%\n",
      "   Recall@5: 53.51%\n",
      "\n",
      "2. Class-Aware Recall@K:\n",
      "   Recall@1: 96.69%\n",
      "   Recall@3: 98.10%\n",
      "   Recall@5: 98.64%\n",
      "\n",
      "3. Mean Average Precision:\n",
      "   Instance-Level MAP: 0.3994\n",
      "   Class-Aware MAP: 0.8308\n",
      "\n",
      "Task 2A (Image-Caption) Recall@5: 53.51%\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# TASK 2A: IMAGE-CAPTION RETRIEVAL\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TASK 2A: IMAGE-CAPTION RETRIEVAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Task 2A: Evaluate CLIP on image-caption retrieval\n",
    "task2a_results = evaluate_task2a(test_ds, clip_model, clip_processor, \n",
    "                                  clip_tokenizer, DEVICE)\n",
    "\n",
    "print(f\"\\nTask 2A (Image-Caption) Recall@5: {task2a_results['instance_recall']['Recall@5']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2B: EEG-Caption Retrieval\n",
    "\n",
    "### EEG-to-CLIP Projection Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TASK 2B: EEG-CAPTION RETRIEVAL\n",
      "============================================================\n",
      "\n",
      "[1/5] Loading trained EEG encoder from Task 1...\n",
      "âœ“ Loaded checkpoint from epoch 98\n",
      "âœ“ Task 1 validation accuracy was: 9.50%\n",
      "âœ“ EEG encoder loaded and ready\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 2B: EEG-CAPTION RETRIEVAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the trained EEG encoder from Task 1\n",
    "print(\"\\n[1/5] Loading trained EEG encoder from Task 1...\")\n",
    "\n",
    "eeg_encoder = EEG_ViT_1D(num_subjects=13, num_classes=20).to(DEVICE)\n",
    "\n",
    "# Load your friend's checkpoint\n",
    "checkpoint = torch.load('best_model_vit_1d.pth', map_location=DEVICE)\n",
    "\n",
    "if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "    eeg_encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"âœ“ Loaded checkpoint from epoch {checkpoint.get('epoch', '?')}\")\n",
    "    print(f\"âœ“ Task 1 validation accuracy was: {checkpoint.get('val_acc', 0):.2f}%\")\n",
    "else:\n",
    "    eeg_encoder.load_state_dict(checkpoint)\n",
    "    print(\"âœ“ Loaded model weights\")\n",
    "\n",
    "# Remove the classification heads - we only need the encoder part\n",
    "eeg_encoder.eval()\n",
    "\n",
    "print(\"âœ“ EEG encoder loaded and ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/5] Creating projection head for CLIP alignment...\n",
      "âœ“ Projection head created\n",
      "  Trainable parameters: 164,608\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2/5] Creating projection head for CLIP alignment...\")\n",
    "\n",
    "class EEGToClipProjection(nn.Module):\n",
    "    \"\"\"Projects EEG embeddings to CLIP text embedding space\"\"\"\n",
    "    \n",
    "    def __init__(self, eeg_dim=128, clip_dim=512, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Small MLP projection head\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(eeg_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, clip_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, eeg_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            eeg_embedding: (batch, 128) from EEG encoder's CLS token\n",
    "        Returns:\n",
    "            projected: (batch, 512) normalized CLIP-space embedding\n",
    "        \"\"\"\n",
    "        projected = self.projection(eeg_embedding)\n",
    "        # Normalize to unit length (CRITICAL for cosine similarity)\n",
    "        projected = F.normalize(projected, p=2, dim=-1)\n",
    "        return projected\n",
    "\n",
    "projection_head = EEGToClipProjection(eeg_dim=128, clip_dim=512).to(DEVICE)\n",
    "\n",
    "print(f\"âœ“ Projection head created\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in projection_head.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/5] Implementing loss functions...\n",
      "âœ“ Loss functions implemented:\n",
      "  - Cosine similarity (KD similarity-based)\n",
      "  - Debiased contrastive (InfoNCE with soft negatives)\n",
      "  - KL divergence (KD logit-based)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[3/5] Implementing loss functions...\")\n",
    "\n",
    "class Task2BLosses:\n",
    "    \"\"\"Loss functions for EEG-Caption alignment\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_similarity_loss(eeg_emb, text_emb):\n",
    "        \"\"\"\n",
    "        Knowledge Distillation: Similarity-based\n",
    "        Align EEG with ground-truth caption\n",
    "        \"\"\"\n",
    "        # Both should be normalized already\n",
    "        cosine_sim = torch.sum(eeg_emb * text_emb, dim=-1)  # (batch,)\n",
    "        loss = 1 - cosine_sim.mean()  # Want similarity = 1\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def contrastive_loss_debiased(eeg_emb, text_emb, labels, temperature=0.07):\n",
    "        \"\"\"\n",
    "        Debiased Contrastive Loss (InfoNCE with soft negatives)\n",
    "        Down-weights same-class captions in negative set\n",
    "        \n",
    "        Args:\n",
    "            eeg_emb: (batch, 512) normalized EEG embeddings\n",
    "            text_emb: (batch, 512) normalized text embeddings  \n",
    "            labels: (batch,) category labels\n",
    "            temperature: scaling factor\n",
    "        \"\"\"\n",
    "        batch_size = eeg_emb.shape[0]\n",
    "        \n",
    "        # Compute similarity matrix: (batch, batch)\n",
    "        logits = torch.mm(eeg_emb, text_emb.t()) / temperature\n",
    "        \n",
    "        # Create weight matrix for negatives\n",
    "        # Same class = lower weight, different class = full weight\n",
    "        labels_eq = labels.unsqueeze(0) == labels.unsqueeze(1)  # (batch, batch)\n",
    "        \n",
    "        # Diagonal = positives (weight doesn't matter, excluded from denominator)\n",
    "        # Same class = 0.3 weight, different class = 1.0 weight\n",
    "        weights = torch.where(labels_eq, \n",
    "                            torch.tensor(0.3, device=DEVICE),\n",
    "                            torch.tensor(1.0, device=DEVICE))\n",
    "        weights = weights.fill_diagonal_(0)  # Ignore diagonal in denominator\n",
    "        \n",
    "        # Compute loss\n",
    "        # Numerator: positive pairs (diagonal)\n",
    "        positive_logits = torch.diag(logits)  # (batch,)\n",
    "        \n",
    "        # Denominator: weighted sum of all pairs\n",
    "        exp_logits = torch.exp(logits)\n",
    "        weighted_exp = exp_logits * weights\n",
    "        \n",
    "        # Add back the positive (diagonal) \n",
    "        denominator = weighted_exp.sum(dim=1) + torch.exp(positive_logits)\n",
    "        \n",
    "        loss = -torch.log(torch.exp(positive_logits) / denominator).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def kl_divergence_loss(eeg_emb, text_emb_all, image_emb, temperature_teacher=0.07, temperature_student=0.07):\n",
    "        \"\"\"\n",
    "        Knowledge Distillation: Logit-based\n",
    "        Student (EEG) mimics Teacher (CLIP image encoder) distribution\n",
    "        \n",
    "        Args:\n",
    "            eeg_emb: (batch, 512) student embeddings\n",
    "            text_emb_all: (N_captions, 512) all caption embeddings in dataset\n",
    "            image_emb: (batch, 512) teacher (CLIP image) embeddings\n",
    "            temperature_teacher/student: softmax temperatures\n",
    "        \"\"\"\n",
    "        # Teacher distribution: image -> all captions\n",
    "        teacher_logits = torch.mm(image_emb, text_emb_all.t()) / temperature_teacher\n",
    "        teacher_probs = F.softmax(teacher_logits, dim=-1)\n",
    "        \n",
    "        # Student distribution: EEG -> all captions  \n",
    "        student_logits = torch.mm(eeg_emb, text_emb_all.t()) / temperature_student\n",
    "        student_log_probs = F.log_softmax(student_logits, dim=-1)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean')\n",
    "        \n",
    "        return kl_loss\n",
    "\n",
    "print(\"âœ“ Loss functions implemented:\")\n",
    "print(\"  - Cosine similarity (KD similarity-based)\")\n",
    "print(\"  - Debiased contrastive (InfoNCE with soft negatives)\")\n",
    "print(\"  - KL divergence (KD logit-based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/5] Setting up evaluation function...\n",
      "âœ“ Evaluation function ready\n",
      "\n",
      "============================================================\n",
      "SETUP COMPLETE - Ready to train Task 2B!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[4/5] Setting up evaluation function...\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_eeg_caption_retrieval(eeg_encoder, projection_head, clip_model, \n",
    "                                   clip_tokenizer, dataloader, device, k=5):\n",
    "    \"\"\"\n",
    "    Evaluate EEG-Caption retrieval\n",
    "    Returns Class-Aware Recall@K\n",
    "    \"\"\"\n",
    "    eeg_encoder.eval()\n",
    "    projection_head.eval()\n",
    "    clip_model.eval()\n",
    "    \n",
    "    all_eeg_emb = []\n",
    "    all_text_emb = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=f\"Evaluating Recall@{k}\", leave=False):\n",
    "        eeg_data = batch['eeg'].to(device)\n",
    "        captions = batch['caption']\n",
    "        labels = batch['label']\n",
    "        subject_ids = batch['subject_id'].to(device)\n",
    "        \n",
    "        # Get EEG embedding (same as training)\n",
    "        x = eeg_data\n",
    "        \n",
    "        # Dimension handling\n",
    "        if x.shape[1] == 500 and x.shape[2] == 122:\n",
    "            x = x.permute(0, 2, 1)\n",
    "        if x.shape[2] > 500:\n",
    "            x = x[:, :, :500]\n",
    "        \n",
    "        # Tokenize\n",
    "        x = eeg_encoder.tokenizer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        b, seq_len, _ = x.shape\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = eeg_encoder.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + eeg_encoder.pos_embedding[:, :seq_len + 1, :]\n",
    "        \n",
    "        # Transformer\n",
    "        x = eeg_encoder.transformer(x)\n",
    "        \n",
    "        # Extract CLS token\n",
    "        eeg_cls = x[:, 0, :]\n",
    "        \n",
    "        # Project to CLIP space\n",
    "        eeg_projected = projection_head(eeg_cls)\n",
    "        \n",
    "        # Get text embedding\n",
    "        text_inputs = clip_tokenizer(captions, padding=True, truncation=True,\n",
    "                                     return_tensors=\"pt\", max_length=77)\n",
    "        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "        text_emb = clip_model.get_text_features(**text_inputs)\n",
    "        text_emb = F.normalize(text_emb, p=2, dim=-1)\n",
    "        \n",
    "        all_eeg_emb.append(eeg_projected.cpu())\n",
    "        all_text_emb.append(text_emb.cpu())\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    eeg_emb = torch.cat(all_eeg_emb, dim=0).to(device)\n",
    "    text_emb = torch.cat(all_text_emb, dim=0).to(device)\n",
    "    labels = torch.cat(all_labels, dim=0).to(device)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarities = torch.mm(eeg_emb, text_emb.t())  # (N, N)\n",
    "    \n",
    "    # Get top-K indices for each query\n",
    "    top_k_indices = torch.argsort(similarities, dim=1, descending=True)[:, :k]\n",
    "    \n",
    "    # Compute class-aware Recall@K\n",
    "    correct = 0\n",
    "    for i in range(len(labels)):\n",
    "        query_label = labels[i]\n",
    "        retrieved_labels = labels[top_k_indices[i]]\n",
    "        if (retrieved_labels == query_label).any():\n",
    "            correct += 1\n",
    "    \n",
    "    recall = 100.0 * correct / len(labels)\n",
    "    return recall\n",
    "\n",
    "print(\"âœ“ Evaluation function ready\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SETUP COMPLETE - Ready to train Task 2B!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/5] Setting up training function...\n",
      "âœ“ Training function ready\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[5/5] Setting up training function...\")\n",
    "\n",
    "def train_task2b(eeg_encoder, projection_head, clip_model, clip_tokenizer, \n",
    "                 train_loader, val_loader,\n",
    "                 strategy='frozen', num_epochs=20, lr=1e-3, \n",
    "                 loss_weights={'cosine': 1.0, 'contrastive': 1.0}):\n",
    "    \"\"\"\n",
    "    Train EEG-Caption alignment with different CLIP fine-tuning strategies\n",
    "    \n",
    "    Args:\n",
    "        strategy: 'frozen', 'partial_unfreeze'\n",
    "        loss_weights: dict with keys 'cosine', 'contrastive'\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with strategy: {strategy.upper()}\")\n",
    "    print(f\"Loss weights: {loss_weights}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Configure CLIP based on strategy\n",
    "    if strategy == 'frozen':\n",
    "        print(\"Strategy: CLIP fully frozen\")\n",
    "        for param in clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        trainable_params = list(projection_head.parameters())\n",
    "        \n",
    "    elif strategy == 'partial_unfreeze':\n",
    "        print(\"Strategy: Unfreezing last 2 CLIP layers + text projection\")\n",
    "        # Freeze all first\n",
    "        for param in clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze last 2 transformer layers\n",
    "        for layer in clip_model.text_model.encoder.layers[-2:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Unfreeze text projection\n",
    "        if hasattr(clip_model, 'text_projection'):\n",
    "            for param in clip_model.text_projection.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        trainable_params = list(projection_head.parameters())\n",
    "        for layer in clip_model.text_model.encoder.layers[-2:]:\n",
    "            trainable_params.extend(list(layer.parameters()))\n",
    "        if hasattr(clip_model, 'text_projection'):\n",
    "            trainable_params.extend(list(clip_model.text_projection.parameters()))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    total_trainable = sum(p.numel() for p in trainable_params if p.requires_grad)\n",
    "    print(f\"\\nTrainable parameters: {total_trainable:,}\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(trainable_params, lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Loss computer\n",
    "    loss_computer = Task2BLosses()\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_recall = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # ===== TRAINING =====\n",
    "        eeg_encoder.eval()  # Keep EEG encoder frozen\n",
    "        projection_head.train()\n",
    "        if strategy != 'frozen':\n",
    "            clip_model.train()\n",
    "        else:\n",
    "            clip_model.eval()\n",
    "        \n",
    "        epoch_losses = {'total': 0, 'cosine': 0, 'contrastive': 0}\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "            eeg_data = batch['eeg'].to(DEVICE)\n",
    "            captions = batch['caption']\n",
    "            labels = batch['label'].to(DEVICE)\n",
    "            subject_ids = batch['subject_id'].to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get EEG embedding using model's forward pass\n",
    "            # We need to extract intermediate representation before classification heads\n",
    "            with torch.no_grad():\n",
    "                # Forward through tokenizer and transformer\n",
    "                x = eeg_data\n",
    "                \n",
    "                # Dimension handling (from model's forward)\n",
    "                if x.shape[1] == 500 and x.shape[2] == 122:\n",
    "                    x = x.permute(0, 2, 1)\n",
    "                if x.shape[2] > 500:\n",
    "                    x = x[:, :, :500]\n",
    "                \n",
    "                # Tokenize\n",
    "                x = eeg_encoder.tokenizer(x)\n",
    "                x = x.permute(0, 2, 1)\n",
    "                \n",
    "                b, seq_len, _ = x.shape\n",
    "                \n",
    "                # Add CLS token\n",
    "                cls_tokens = eeg_encoder.cls_token.expand(b, -1, -1)\n",
    "                x = torch.cat((cls_tokens, x), dim=1)\n",
    "                \n",
    "                # Add positional embedding\n",
    "                x = x + eeg_encoder.pos_embedding[:, :seq_len + 1, :]\n",
    "                \n",
    "                # Transformer\n",
    "                x = eeg_encoder.transformer(x)\n",
    "                \n",
    "                # Extract CLS token\n",
    "                eeg_cls = x[:, 0, :]  # (batch, 128)\n",
    "            \n",
    "            # Project to CLIP space\n",
    "            eeg_projected = projection_head(eeg_cls)  # (batch, 512), normalized\n",
    "            \n",
    "            # Get text embeddings from CLIP\n",
    "            text_inputs = clip_tokenizer(captions, padding=True, truncation=True,\n",
    "                                        return_tensors=\"pt\", max_length=77)\n",
    "            text_inputs = {k: v.to(DEVICE) for k, v in text_inputs.items()}\n",
    "            \n",
    "            text_emb = clip_model.get_text_features(**text_inputs)\n",
    "            text_emb = F.normalize(text_emb, p=2, dim=-1)\n",
    "            \n",
    "            # Compute losses\n",
    "            total_loss = 0\n",
    "            \n",
    "            if loss_weights.get('cosine', 0) > 0:\n",
    "                loss_cos = loss_computer.cosine_similarity_loss(eeg_projected, text_emb)\n",
    "                total_loss += loss_weights['cosine'] * loss_cos\n",
    "                epoch_losses['cosine'] += loss_cos.item()\n",
    "            \n",
    "            if loss_weights.get('contrastive', 0) > 0:\n",
    "                loss_contrast = loss_computer.contrastive_loss_debiased(\n",
    "                    eeg_projected, text_emb, labels)\n",
    "                total_loss += loss_weights['contrastive'] * loss_contrast\n",
    "                epoch_losses['contrastive'] += loss_contrast.item()\n",
    "            \n",
    "            epoch_losses['total'] += total_loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Print epoch stats\n",
    "        avg_total = epoch_losses['total'] / num_batches\n",
    "        avg_cos = epoch_losses['cosine'] / num_batches\n",
    "        avg_contrast = epoch_losses['contrastive'] / num_batches\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Total Loss: {avg_total:.4f} \" +\n",
    "              f\"(Cosine: {avg_cos:.4f}, Contrastive: {avg_contrast:.4f})\")\n",
    "        \n",
    "        # ===== VALIDATION =====\n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:  # Validate every 5 epochs and last epoch\n",
    "            val_recall = evaluate_eeg_caption_retrieval(\n",
    "                eeg_encoder, projection_head, clip_model, clip_tokenizer, \n",
    "                val_loader, DEVICE, k=5)\n",
    "            \n",
    "            print(f\"  â†’ Val Recall@5: {val_recall:.2f}%\")\n",
    "            \n",
    "            if val_recall > best_val_recall:\n",
    "                best_val_recall = val_recall\n",
    "                save_path = f'task2b_best_{strategy}.pth'\n",
    "                torch.save({\n",
    "                    'projection_head': projection_head.state_dict(),\n",
    "                    'epoch': epoch + 1,\n",
    "                    'strategy': strategy,\n",
    "                    'val_recall': val_recall,\n",
    "                    'loss_weights': loss_weights\n",
    "                }, save_path)\n",
    "                print(f\"  âœ“ Best model saved to {save_path}!\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training complete!\")\n",
    "    print(f\"Best Val Recall@5: {best_val_recall:.2f}%\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return projection_head, best_val_recall\n",
    "\n",
    "print(\"âœ“ Training function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING TASK 2B - STRATEGY: FROZEN CLIP\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training with strategy: PARTIAL_UNFREEZE\n",
      "Loss weights: {'cosine': 0.5, 'contrastive': 0.5}\n",
      "============================================================\n",
      "\n",
      "Strategy: Unfreezing last 2 CLIP layers + text projection\n",
      "\n",
      "Trainable parameters: 6,731,520\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659465ff16034c99bcf013ca13483d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Total Loss: 1.7152 (Cosine: 0.0376, Contrastive: 3.3927)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1261ea7177534c8dba5ca3163c16cfb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Total Loss: 1.6948 (Cosine: 0.0240, Contrastive: 3.3655)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f0fcb202154da688ab5ec6360158c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Total Loss: 1.6915 (Cosine: 0.0243, Contrastive: 3.3587)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1687f38032461fa31526980bf8c284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Total Loss: 1.6889 (Cosine: 0.0249, Contrastive: 3.3529)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8c8248a5e64c3d9725ef4c1c58e6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Total Loss: 1.6863 (Cosine: 0.0257, Contrastive: 3.3469)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc1c3376a5e4da5b4da7c1c9ba2d58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 13.67%\n",
      "  âœ“ Best model saved to task2b_best_partial_unfreeze.pth!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b69622ad267457cbceb856d5c97f414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Total Loss: 1.6846 (Cosine: 0.0267, Contrastive: 3.3425)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a86c6da4d24339b63ddd5aacb0de32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Total Loss: 1.6835 (Cosine: 0.0268, Contrastive: 3.3402)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a3cb7570c74fa28d6f08d7209f5bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Total Loss: 1.6822 (Cosine: 0.0271, Contrastive: 3.3374)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88aa95bc00104b9f917a25d7118a829e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Total Loss: 1.6809 (Cosine: 0.0275, Contrastive: 3.3343)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a204ce43a9f4b57bd1f2848ff2b5dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Total Loss: 1.6805 (Cosine: 0.0274, Contrastive: 3.3336)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8adbdf60ac249c881ffa4f98134df5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 13.23%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912fb79ef2ba45e490ff1a9568c0d1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Total Loss: 1.6790 (Cosine: 0.0280, Contrastive: 3.3300)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbf5072e5b147d4a435fbdaa359a271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Total Loss: 1.6777 (Cosine: 0.0285, Contrastive: 3.3268)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462db60e24b84faa9a023dcc96278654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Total Loss: 1.6759 (Cosine: 0.0288, Contrastive: 3.3230)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68dc10a4c064e93bd6d4fc38566f910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Total Loss: 1.6749 (Cosine: 0.0290, Contrastive: 3.3207)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba99098a45d491ebaef672b395d95b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Total Loss: 1.6740 (Cosine: 0.0293, Contrastive: 3.3188)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089e5e8911754a71aebe3b5385abc9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 14.81%\n",
      "  âœ“ Best model saved to task2b_best_partial_unfreeze.pth!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2814b1b6149a45d5a3696ed6d299d270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Total Loss: 1.6730 (Cosine: 0.0298, Contrastive: 3.3162)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94f8ec31b87401d9c159394171ee9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Total Loss: 1.6725 (Cosine: 0.0303, Contrastive: 3.3147)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304ac2bdb4e84fb4a0eae1537d8f044a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Total Loss: 1.6698 (Cosine: 0.0311, Contrastive: 3.3085)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd37d55709d84e05adbb66caa515f059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Total Loss: 1.6684 (Cosine: 0.0339, Contrastive: 3.3029)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acf48aea20945edbb33da24c318c0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Total Loss: 1.6660 (Cosine: 0.0356, Contrastive: 3.2964)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0e03b72f264948afc6b8c76f19c16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 14.85%\n",
      "  âœ“ Best model saved to task2b_best_partial_unfreeze.pth!\n",
      "\n",
      "============================================================\n",
      "Training complete!\n",
      "Best Val Recall@5: 14.85%\n",
      "============================================================\n",
      "\n",
      "\n",
      "âœ“ Training completed with best validation Recall@5: 14.85%\n"
     ]
    }
   ],
   "source": [
    "# Create fresh projection head\n",
    "projection_head_frozen = EEGToClipProjection(eeg_dim=128, clip_dim=512).to(DEVICE)\n",
    "\n",
    "# Train with frozen CLIP (baseline)\n",
    "trained_projection, best_recall = train_task2b(\n",
    "    eeg_encoder=eeg_encoder,\n",
    "    projection_head=projection_head_frozen,\n",
    "    clip_model=clip_model,\n",
    "    clip_tokenizer=clip_tokenizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    strategy='partial_unfreeze',\n",
    "    num_epochs=20,\n",
    "    lr=1e-3,\n",
    "    loss_weights={'cosine': 0.5, 'contrastive': 0.5}\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Training completed with best validation Recall@5: {best_recall:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frozen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241m14.12\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mfrozen\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'frozen' is not defined"
     ]
    }
   ],
   "source": [
    "# 14.12% - frozen\n",
    "# 14.85% - PARTIAL_UNFREEZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 2: FINE-TUNING WITH UNFROZEN EEG ENCODER\n",
      "============================================================\n",
      "\n",
      "[1] Loading best projection head from Stage 1...\n",
      "âœ“ Loaded projection head with val Recall@5: 14.12%\n",
      "\n",
      "[2] Loading EEG encoder (will be unfrozen)...\n",
      "âœ“ EEG encoder loaded\n",
      "âœ“ EEG encoder UNFROZEN\n",
      "âœ“ CLIP last 2 layers UNFROZEN\n",
      "\n",
      "Total trainable parameters: 7,947,012\n",
      "\n",
      "[6] Starting Stage 2 training...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a4ac9d6eda4f5bb8aa6f9a47ed06e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 1/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Loss: 1.7831 (Cos: 0.2211, Contrast: 3.3452)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58a5a3b92384da9b290295fe9b5887f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 2/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - Loss: 1.6947 (Cos: 0.0563, Contrast: 3.3331)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab3b068e72f409aa162c19e1ab8adfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 3/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - Loss: 1.6881 (Cos: 0.0448, Contrast: 3.3313)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0106e769d81249929e251dd89ca318db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 16.42%\n",
      "  âœ“ NEW BEST! Saved to task2b_best_stage2_unfrozen.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ec2699efda46a1a6f3fc2b1f1b9c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 4/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - Loss: 1.6836 (Cos: 0.0414, Contrast: 3.3257)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b8e82fa6854fb9a2a039b4675f8e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 5/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - Loss: 1.6818 (Cos: 0.0394, Contrast: 3.3242)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8e8003703540aea7c2ab387413f8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 6/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - Loss: 1.6788 (Cos: 0.0389, Contrast: 3.3187)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad291ba6b654829bb502f3d57a26186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 16.69%\n",
      "  âœ“ NEW BEST! Saved to task2b_best_stage2_unfrozen.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077b8b2b1e6d4dd7990d436b33e6fee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 7/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - Loss: 1.6776 (Cos: 0.0384, Contrast: 3.3168)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74a9df1fdd043459e7a83d9f92f10c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 8/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - Loss: 1.6768 (Cos: 0.0381, Contrast: 3.3155)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f43b34e6acd426eb0e4bb05f90e9327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 9/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - Loss: 1.6770 (Cos: 0.0377, Contrast: 3.3162)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d3370e6e7a4613a7b3d4117f39d15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 17.52%\n",
      "  âœ“ NEW BEST! Saved to task2b_best_stage2_unfrozen.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d064e674a1b041a28eadb64cd340eee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 10/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - Loss: 1.6771 (Cos: 0.0372, Contrast: 3.3170)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fb1f723d8b4ba582f04d58f43fbc5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 11/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - Loss: 1.6759 (Cos: 0.0367, Contrast: 3.3152)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5e7777f3884965b5c4e49ad5e918f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 12/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 - Loss: 1.6755 (Cos: 0.0372, Contrast: 3.3139)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f306b6b8849b4b71874d8fe4762ccecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 16.94%\n",
      "  No improvement (1/3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1c5d8f10944e328ea63cb01e957593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 13/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 - Loss: 1.6744 (Cos: 0.0367, Contrast: 3.3120)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4361e4ee8dbd4c1dbe556819a7528948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 14/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 - Loss: 1.6735 (Cos: 0.0374, Contrast: 3.3096)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3f6a7921cf45ab95ab278d12997766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 15/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 - Loss: 1.6734 (Cos: 0.0373, Contrast: 3.3096)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c49c6654b754383a96572f3cea79089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 16.77%\n",
      "  No improvement (2/3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47019f1364c04403a6e18bb96c03edaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 16/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 - Loss: 1.6730 (Cos: 0.0370, Contrast: 3.3091)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edead3941daa4b3e98bf9a81100a0294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 17/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 - Loss: 1.6720 (Cos: 0.0375, Contrast: 3.3065)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58fad1c2e804e6ca5829edd87b19ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 18/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 - Loss: 1.6719 (Cos: 0.0371, Contrast: 3.3068)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7587632a4bcf4987ba1c0c57d99a9204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 17.25%\n",
      "  No improvement (3/3)\n",
      "\n",
      "âš ï¸ Early stopping triggered!\n",
      "\n",
      "============================================================\n",
      "Stage 2 Training Complete!\n",
      "Stage 1 Best: 14.12%\n",
      "Stage 2 Best: 17.52%\n",
      "Improvement: 3.40%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STAGE 2: Fine-tune Everything Together\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 2: FINE-TUNING WITH UNFROZEN EEG ENCODER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Load BEST projection head from Stage 1\n",
    "print(\"\\n[1] Loading best projection head from Stage 1...\")\n",
    "checkpoint = torch.load('task2b_best_frozen.pth', map_location=DEVICE)\n",
    "projection_head_stage2 = EEGToClipProjection(eeg_dim=128, clip_dim=512).to(DEVICE)\n",
    "projection_head_stage2.load_state_dict(checkpoint['projection_head'])\n",
    "print(f\"âœ“ Loaded projection head with val Recall@5: {checkpoint['val_recall']:.2f}%\")\n",
    "\n",
    "# 2. Load EEG encoder (fresh copy, will be unfrozen)\n",
    "print(\"\\n[2] Loading EEG encoder (will be unfrozen)...\")\n",
    "eeg_encoder_stage2 = EEG_ViT_1D(num_subjects=13, num_classes=20).to(DEVICE)\n",
    "task1_checkpoint = torch.load('best_model_vit_1d.pth', map_location=DEVICE)\n",
    "if isinstance(task1_checkpoint, dict):\n",
    "    eeg_encoder_stage2.load_state_dict(task1_checkpoint['model_state_dict'])\n",
    "else:\n",
    "    eeg_encoder_stage2.load_state_dict(task1_checkpoint)\n",
    "print(\"âœ“ EEG encoder loaded\")\n",
    "\n",
    "# 3. Unfreeze EEG encoder\n",
    "for param in eeg_encoder_stage2.parameters():\n",
    "    param.requires_grad = True\n",
    "print(\"âœ“ EEG encoder UNFROZEN\")\n",
    "\n",
    "# 4. Unfreeze CLIP last 2 layers\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for layer in clip_model.text_model.encoder.layers[-2:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "if hasattr(clip_model, 'text_projection'):\n",
    "    for param in clip_model.text_projection.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "print(\"âœ“ CLIP last 2 layers UNFROZEN\")\n",
    "\n",
    "# 5. Setup optimizer with DIFFERENTIAL LEARNING RATES\n",
    "trainable_params = [\n",
    "    {'params': projection_head_stage2.parameters(), 'lr': 1e-4, 'name': 'projection'},\n",
    "    {'params': eeg_encoder_stage2.parameters(), 'lr': 1e-5, 'name': 'eeg_encoder'},\n",
    "]\n",
    "\n",
    "for layer in clip_model.text_model.encoder.layers[-2:]:\n",
    "    trainable_params.append({\n",
    "        'params': layer.parameters(), \n",
    "        'lr': 5e-5, \n",
    "        'name': 'clip_layers'\n",
    "    })\n",
    "\n",
    "if hasattr(clip_model, 'text_projection'):\n",
    "    trainable_params.append({\n",
    "        'params': clip_model.text_projection.parameters(),\n",
    "        'lr': 5e-5,\n",
    "        'name': 'clip_projection'\n",
    "    })\n",
    "\n",
    "optimizer_stage2 = optim.AdamW(trainable_params, weight_decay=1e-4)\n",
    "\n",
    "total_params = sum(p.numel() for group in trainable_params for p in group['params'])\n",
    "print(f\"\\nTotal trainable parameters: {total_params:,}\")\n",
    "\n",
    "# 6. Training loop with early stopping\n",
    "print(\"\\n[6] Starting Stage 2 training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_recall_stage2 = checkpoint['val_recall']  # Start from Stage 1 best\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "loss_computer = Task2BLosses()\n",
    "\n",
    "NUM_EPOCHS_STAGE2 = 25\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_STAGE2):\n",
    "    # Training\n",
    "    eeg_encoder_stage2.train()\n",
    "    projection_head_stage2.train()\n",
    "    clip_model.train()\n",
    "    \n",
    "    epoch_losses = {'total': 0, 'cosine': 0, 'contrastive': 0}\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Stage2 Epoch {epoch+1}/{NUM_EPOCHS_STAGE2}\", leave=False):\n",
    "        eeg_data = batch['eeg'].to(DEVICE)\n",
    "        captions = batch['caption']\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        subject_ids = batch['subject_id'].to(DEVICE)\n",
    "        \n",
    "        optimizer_stage2.zero_grad()\n",
    "        \n",
    "        # Forward through EEG encoder (NOW TRAINABLE)\n",
    "        x = eeg_data\n",
    "        if x.shape[1] == 500 and x.shape[2] == 122:\n",
    "            x = x.permute(0, 2, 1)\n",
    "        if x.shape[2] > 500:\n",
    "            x = x[:, :, :500]\n",
    "        \n",
    "        x = eeg_encoder_stage2.tokenizer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        b, seq_len, _ = x.shape\n",
    "        \n",
    "        cls_tokens = eeg_encoder_stage2.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + eeg_encoder_stage2.pos_embedding[:, :seq_len + 1, :]\n",
    "        x = eeg_encoder_stage2.transformer(x)\n",
    "        eeg_cls = x[:, 0, :]\n",
    "        \n",
    "        # Project\n",
    "        eeg_projected = projection_head_stage2(eeg_cls)\n",
    "        \n",
    "        # CLIP text\n",
    "        text_inputs = clip_tokenizer(captions, padding=True, truncation=True,\n",
    "                                    return_tensors=\"pt\", max_length=77)\n",
    "        text_inputs = {k: v.to(DEVICE) for k, v in text_inputs.items()}\n",
    "        text_emb = clip_model.get_text_features(**text_inputs)\n",
    "        text_emb = F.normalize(text_emb, p=2, dim=-1)\n",
    "        \n",
    "        # Losses\n",
    "        loss_cos = loss_computer.cosine_similarity_loss(eeg_projected, text_emb)\n",
    "        loss_contrast = loss_computer.contrastive_loss_debiased(eeg_projected, text_emb, labels)\n",
    "        \n",
    "        total_loss = 0.5 * loss_cos + 0.5 * loss_contrast\n",
    "        \n",
    "        epoch_losses['cosine'] += loss_cos.item()\n",
    "        epoch_losses['contrastive'] += loss_contrast.item()\n",
    "        epoch_losses['total'] += total_loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient clipping (important for stability)\n",
    "        torch.nn.utils.clip_grad_norm_(eeg_encoder_stage2.parameters(), max_norm=1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(projection_head_stage2.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer_stage2.step()\n",
    "    \n",
    "    # Print epoch stats\n",
    "    avg_total = epoch_losses['total'] / num_batches\n",
    "    avg_cos = epoch_losses['cosine'] / num_batches\n",
    "    avg_contrast = epoch_losses['contrastive'] / num_batches\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS_STAGE2} - Loss: {avg_total:.4f} \" +\n",
    "          f\"(Cos: {avg_cos:.4f}, Contrast: {avg_contrast:.4f})\")\n",
    "    \n",
    "    # Validation\n",
    "    if (epoch + 1) % 3 == 0 or epoch == NUM_EPOCHS_STAGE2 - 1:\n",
    "        val_recall = evaluate_eeg_caption_retrieval(\n",
    "            eeg_encoder_stage2, projection_head_stage2, clip_model, \n",
    "            clip_tokenizer, val_loader, DEVICE, k=5)\n",
    "        \n",
    "        print(f\"  â†’ Val Recall@5: {val_recall:.2f}%\")\n",
    "        \n",
    "        if val_recall > best_val_recall_stage2:\n",
    "            best_val_recall_stage2 = val_recall\n",
    "            patience_counter = 0\n",
    "            \n",
    "            torch.save({\n",
    "                'eeg_encoder': eeg_encoder_stage2.state_dict(),\n",
    "                'projection_head': projection_head_stage2.state_dict(),\n",
    "                'epoch': epoch + 1,\n",
    "                'val_recall': val_recall,\n",
    "                'strategy': 'stage2_unfrozen'\n",
    "            }, 'task2b_best_stage2_unfrozen.pth')\n",
    "            \n",
    "            print(f\"  âœ“ NEW BEST! Saved to task2b_best_stage2_unfrozen.pth\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{patience})\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nâš ï¸ Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Stage 2 Training Complete!\")\n",
    "print(f\"Stage 1 Best: {checkpoint['val_recall']:.2f}%\")\n",
    "print(f\"Stage 2 Best: {best_val_recall_stage2:.2f}%\")\n",
    "print(f\"Improvement: {best_val_recall_stage2 - checkpoint['val_recall']:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 3: LoRA ON EEG ENCODER\n",
      "============================================================\n",
      "\n",
      "[1] Loading Stage 1 best projection head...\n",
      "âœ“ Loaded projection head\n",
      "\n",
      "[2] Loading EEG encoder for LoRA...\n",
      "âœ“ EEG encoder loaded and frozen\n",
      "\n",
      "[3] Applying LoRA to EEG encoder transformer...\n",
      "trainable params: 24,576 || all params: 421,120 || trainable%: 5.8359\n",
      "âœ“ LoRA applied to EEG transformer\n",
      "\n",
      "[4] Applying LoRA to CLIP text encoder...\n",
      "  (Reloading CLIP model to clear previous PEFT wrappers...)\n",
      "trainable params: 196,608 || all params: 63,362,560 || trainable%: 0.3103\n",
      "âœ“ LoRA applied to CLIP\n",
      "\n",
      "[5] Setting up optimizer with LoRA parameters...\n",
      "\n",
      "Total trainable parameters: 385,792\n",
      "  (Compare to Stage 2's ~3.8M params)\n",
      "\n",
      "[6] Starting LoRA training...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e422132956124c358fc4c69575e864d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 1/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 1.7297 (Cos: 0.0902, Contrast: 3.3692)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e2baa269574d73bb87608e21b18ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 2/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Loss: 1.7044 (Cos: 0.0409, Contrast: 3.3679)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b977aeb56cd041dd95dae67fcb5dd8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 3/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Loss: 1.7005 (Cos: 0.0392, Contrast: 3.3617)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f432871eaf50445aa9ad2c940fd5ebcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 10.88%\n",
      "  No improvement (1/6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1ac277841a4186a8735f2a0504ab62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 4/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Loss: 1.6997 (Cos: 0.0372, Contrast: 3.3623)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1286aa312146c38ad047badd941c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 5/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Loss: 1.6955 (Cos: 0.0389, Contrast: 3.3520)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb08481478642758dab183f906a54ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 6/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Loss: 1.6931 (Cos: 0.0404, Contrast: 3.3457)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be2ac36fe60414aa376d9ae05df498c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 13.06%\n",
      "  No improvement (2/6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40e58a379a9455f9868f2b9928f3b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 7/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Loss: 1.6970 (Cos: 0.0366, Contrast: 3.3573)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbec5d8757343669c31bb900c4ca527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 8/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Loss: 1.6939 (Cos: 0.0373, Contrast: 3.3506)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7402d1c2364dd9800d4ee8876c92aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 9/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Loss: 1.6920 (Cos: 0.0375, Contrast: 3.3465)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a227e8d90a88439fb80040e473a58696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 13.23%\n",
      "  No improvement (3/6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b785f05fdf70482c845401d8e45234e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 10/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Loss: 1.6904 (Cos: 0.0377, Contrast: 3.3431)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efc16921ed2433fafa7b1935646fd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 11/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Loss: 1.6872 (Cos: 0.0397, Contrast: 3.3348)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e44ceb0639242798ab873f5c95fb16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 12/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Loss: 1.6861 (Cos: 0.0389, Contrast: 3.3333)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a847b0989514f408b1bb2b620735f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 13.46%\n",
      "  No improvement (4/6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a014bf11a8e4c799b74e4ed2c5796da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 13/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Loss: 1.6851 (Cos: 0.0402, Contrast: 3.3300)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28194b5f09446dfb340df02ef2e95c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 14/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Loss: 1.6844 (Cos: 0.0396, Contrast: 3.3292)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3144a31c8e4a5785ff9db4cf79b1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 15/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Loss: 1.6817 (Cos: 0.0414, Contrast: 3.3220)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c10d43185f4b5dade14f845b272b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 16.10%\n",
      "  âœ“ NEW BEST! Saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5890da500947999850677eef05f66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 16/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Loss: 1.6824 (Cos: 0.0420, Contrast: 3.3227)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d145f35289944acfbbaabb45be162396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 17/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Loss: 1.6825 (Cos: 0.0398, Contrast: 3.3252)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4282e609ce44d9b16d3899ca387a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 18/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Loss: 1.6792 (Cos: 0.0423, Contrast: 3.3161)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d382ca32f204fbba0992c173f50de6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 14.75%\n",
      "  No improvement (1/6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc20e594b74b41898b038453abb99b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 19/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Loss: 1.6805 (Cos: 0.0416, Contrast: 3.3195)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068e8f496b2f4189bde1c597c4cedd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LoRA Epoch 20/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Loss: 1.6760 (Cos: 0.0436, Contrast: 3.3084)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd948c9b8914faa98c7980246ee128a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 16.23%\n",
      "  âœ“ NEW BEST! Saved\n",
      "\n",
      "============================================================\n",
      "LoRA Training Complete!\n",
      "Stage 1 (Frozen): 14.12%\n",
      "Stage 3 (LoRA):   16.23%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# INSTALL PEFT (if not already)\n",
    "# ============================================\n",
    "# Run this in a separate cell first:\n",
    "# !pip install peft -q\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 3: LoRA ON EEG ENCODER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================\n",
    "# 1. Load Stage 1 Best Projection Head\n",
    "# ============================================\n",
    "print(\"\\n[1] Loading Stage 1 best projection head...\")\n",
    "stage1_checkpoint = torch.load('task2b_best_frozen.pth', map_location=DEVICE)\n",
    "projection_head_lora = EEGToClipProjection(eeg_dim=128, clip_dim=512).to(DEVICE)\n",
    "projection_head_lora.load_state_dict(stage1_checkpoint['projection_head'])\n",
    "print(f\"âœ“ Loaded projection head\")\n",
    "\n",
    "# ============================================\n",
    "# 2. Load Fresh EEG Encoder\n",
    "# ============================================\n",
    "print(\"\\n[2] Loading EEG encoder for LoRA...\")\n",
    "eeg_encoder_lora = EEG_ViT_1D(num_subjects=13, num_classes=20).to(DEVICE)\n",
    "task1_checkpoint = torch.load('best_model_vit_1d.pth', map_location=DEVICE)\n",
    "\n",
    "if isinstance(task1_checkpoint, dict):\n",
    "    eeg_encoder_lora.load_state_dict(task1_checkpoint['model_state_dict'])\n",
    "else:\n",
    "    eeg_encoder_lora.load_state_dict(task1_checkpoint)\n",
    "\n",
    "# Freeze base model\n",
    "for param in eeg_encoder_lora.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"âœ“ EEG encoder loaded and frozen\")\n",
    "\n",
    "# ============================================\n",
    "# 3. Apply LoRA to EEG Transformer\n",
    "# ============================================\n",
    "print(\"\\n[3] Applying LoRA to EEG encoder transformer...\")\n",
    "\n",
    "# LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                          # Rank (higher = more capacity, try 4-16)\n",
    "    lora_alpha=16,                # Scaling factor (usually 2Ã—r)\n",
    "    target_modules=[\"out_proj\", \"linear1\", \"linear2\"],  # Which layers to adapt\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    "    # task_type=\"FEATURE_EXTRACTION\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the transformer encoder\n",
    "# We need to wrap just the transformer part\n",
    "eeg_encoder_lora.transformer = get_peft_model(\n",
    "    eeg_encoder_lora.transformer, \n",
    "    lora_config\n",
    ")\n",
    "\n",
    "# Print trainable parameters\n",
    "eeg_encoder_lora.transformer.print_trainable_parameters()\n",
    "\n",
    "print(\"âœ“ LoRA applied to EEG transformer\")\n",
    "\n",
    "# ============================================\n",
    "# 4. Also Apply LoRA to CLIP (Optional)\n",
    "# ============================================\n",
    "print(\"\\n[4] Applying LoRA to CLIP text encoder...\")\n",
    "print(\"  (Reloading CLIP model to clear previous PEFT wrappers...)\")\n",
    "# Re-initialize your CLIP model here exactly as you did in Stage 1/2\n",
    "# (Assuming you are using HuggingFace CLIP)\n",
    "from transformers import CLIPModel\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "\n",
    "# Freeze CLIP completely first\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# LoRA on CLIP\n",
    "clip_lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    "    # task_type=\"FEATURE_EXTRACTION\"\n",
    ")\n",
    "\n",
    "# Apply to last 2 layers only (more targeted)\n",
    "# We'll manually add LoRA to specific layers\n",
    "from peft import LoraModel\n",
    "\n",
    "# Apply to CLIP text encoder\n",
    "clip_model.text_model = get_peft_model(clip_model.text_model, clip_lora_config)\n",
    "clip_model.text_model.print_trainable_parameters()\n",
    "\n",
    "print(\"âœ“ LoRA applied to CLIP\")\n",
    "\n",
    "# ============================================\n",
    "# 5. Setup Optimizer\n",
    "# ============================================\n",
    "print(\"\\n[5] Setting up optimizer with LoRA parameters...\")\n",
    "\n",
    "# Collect trainable parameters\n",
    "trainable_params = [\n",
    "    {'params': projection_head_lora.parameters(), 'lr': 1e-4, 'name': 'projection'},\n",
    "    {'params': [p for p in eeg_encoder_lora.parameters() if p.requires_grad], \n",
    "     'lr': 5e-4, 'name': 'eeg_lora'},\n",
    "    {'params': [p for p in clip_model.parameters() if p.requires_grad], \n",
    "     'lr': 5e-4, 'name': 'clip_lora'}\n",
    "]\n",
    "\n",
    "optimizer_lora = optim.AdamW(trainable_params, weight_decay=1e-4)\n",
    "\n",
    "# Count parameters\n",
    "total_trainable = sum(p.numel() for group in trainable_params for p in group['params'])\n",
    "print(f\"\\nTotal trainable parameters: {total_trainable:,}\")\n",
    "print(\"  (Compare to Stage 2's ~3.8M params)\")\n",
    "\n",
    "# ============================================\n",
    "# 6. Training Loop\n",
    "# ============================================\n",
    "print(\"\\n[6] Starting LoRA training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_recall_lora = stage1_checkpoint['val_recall']\n",
    "patience = 6\n",
    "patience_counter = 0\n",
    "loss_computer = Task2BLosses()\n",
    "\n",
    "NUM_EPOCHS_LORA = 20\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_LORA):\n",
    "    # Training mode\n",
    "    eeg_encoder_lora.train()\n",
    "    projection_head_lora.train()\n",
    "    clip_model.train()\n",
    "    \n",
    "    epoch_losses = {'total': 0, 'cosine': 0, 'contrastive': 0}\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"LoRA Epoch {epoch+1}/{NUM_EPOCHS_LORA}\", leave=False):\n",
    "        eeg_data = batch['eeg'].to(DEVICE)\n",
    "        captions = batch['caption']\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        subject_ids = batch['subject_id'].to(DEVICE)\n",
    "        \n",
    "        optimizer_lora.zero_grad()\n",
    "        \n",
    "        # Forward through EEG encoder (with LoRA adapters active)\n",
    "        x = eeg_data\n",
    "        if x.shape[1] == 500 and x.shape[2] == 122:\n",
    "            x = x.permute(0, 2, 1)\n",
    "        if x.shape[2] > 500:\n",
    "            x = x[:, :, :500]\n",
    "        \n",
    "        x = eeg_encoder_lora.tokenizer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        b, seq_len, _ = x.shape\n",
    "        \n",
    "        cls_tokens = eeg_encoder_lora.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + eeg_encoder_lora.pos_embedding[:, :seq_len + 1, :]\n",
    "        \n",
    "        # This now uses LoRA adapters\n",
    "        x = eeg_encoder_lora.transformer(x)\n",
    "        eeg_cls = x[:, 0, :]\n",
    "        \n",
    "        # Project\n",
    "        eeg_projected = projection_head_lora(eeg_cls)\n",
    "        \n",
    "        # CLIP text (with LoRA)\n",
    "        text_inputs = clip_tokenizer(captions, padding=True, truncation=True,\n",
    "                                    return_tensors=\"pt\", max_length=77)\n",
    "        text_inputs = {k: v.to(DEVICE) for k, v in text_inputs.items()}\n",
    "        text_emb = clip_model.get_text_features(**text_inputs)\n",
    "        text_emb = F.normalize(text_emb, p=2, dim=-1)\n",
    "        \n",
    "        # Losses\n",
    "        loss_cos = loss_computer.cosine_similarity_loss(eeg_projected, text_emb)\n",
    "        loss_contrast = loss_computer.contrastive_loss_debiased(eeg_projected, text_emb, labels)\n",
    "        \n",
    "        total_loss = 0.5 * loss_cos + 0.5 * loss_contrast\n",
    "        \n",
    "        epoch_losses['cosine'] += loss_cos.item()\n",
    "        epoch_losses['contrastive'] += loss_contrast.item()\n",
    "        epoch_losses['total'] += total_loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer_lora.step()\n",
    "    \n",
    "    # Print\n",
    "    avg_total = epoch_losses['total'] / num_batches\n",
    "    avg_cos = epoch_losses['cosine'] / num_batches\n",
    "    avg_contrast = epoch_losses['contrastive'] / num_batches\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS_LORA} - Loss: {avg_total:.4f} \" +\n",
    "          f\"(Cos: {avg_cos:.4f}, Contrast: {avg_contrast:.4f})\")\n",
    "    \n",
    "    # Validation\n",
    "    if (epoch + 1) % 3 == 0 or epoch == NUM_EPOCHS_LORA - 1:\n",
    "        val_recall = evaluate_eeg_caption_retrieval(\n",
    "            eeg_encoder_lora, projection_head_lora, clip_model,\n",
    "            clip_tokenizer, val_loader, DEVICE, k=5)\n",
    "        \n",
    "        print(f\"  â†’ Val Recall@5: {val_recall:.2f}%\")\n",
    "        \n",
    "        if val_recall > best_val_recall_lora:\n",
    "            best_val_recall_lora = val_recall\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save merged model (LoRA + base weights)\n",
    "            torch.save({\n",
    "                'eeg_encoder': eeg_encoder_lora.state_dict(),\n",
    "                'projection_head': projection_head_lora.state_dict(),\n",
    "                'epoch': epoch + 1,\n",
    "                'val_recall': val_recall,\n",
    "                'strategy': 'lora'\n",
    "            }, 'task2b_best_lora.pth')\n",
    "            \n",
    "            print(f\"  âœ“ NEW BEST! Saved\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{patience})\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nâš ï¸ Early stopping!\")\n",
    "                break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LoRA Training Complete!\")\n",
    "print(f\"Stage 1 (Frozen): {stage1_checkpoint['val_recall']:.2f}%\")\n",
    "print(f\"Stage 3 (LoRA):   {best_val_recall_lora:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG Encoder Transformer Layers:\n",
      "  layers.0.self_attn: MultiheadAttention\n",
      "  layers.0.self_attn.out_proj: NonDynamicallyQuantizableLinear\n",
      "  layers.1.self_attn: MultiheadAttention\n",
      "  layers.1.self_attn.out_proj: NonDynamicallyQuantizableLinear\n"
     ]
    }
   ],
   "source": [
    "# Run this first to see the actual layer structure\n",
    "print(\"EEG Encoder Transformer Layers:\")\n",
    "for name, module in eeg_encoder_lora.transformer.named_modules():\n",
    "    if 'attn' in name.lower() or 'proj' in name.lower():\n",
    "        print(f\"  {name}: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 3: LoRA ON EEG ENCODER (MANUAL)\n",
      "============================================================\n",
      "\n",
      "[1] Loading Stage 1 best...\n",
      "âœ“ Models loaded and frozen\n",
      "\n",
      "[2] Applying LoRA to CLIP text encoder only...\n",
      "trainable params: 196,608 || all params: 63,362,560 || trainable%: 0.3103\n",
      "âœ“ LoRA applied to CLIP\n",
      "\n",
      "[3] Unfreezing last 2 transformer layers of EEG encoder...\n",
      "   Total transformer layers: 2\n",
      "âœ“ Last 2 EEG layers unfrozen\n",
      "\n",
      "[4] Setting up optimizer...\n",
      "Total trainable parameters: 757,760\n",
      "\n",
      "[5] Starting training with LoRA on CLIP...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac60e79f93064abe8808c7418d21e06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "CLIPTextTransformer.forward() got an unexpected keyword argument 'inputs_embeds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 130\u001b[0m\n\u001b[1;32m    127\u001b[0m text_inputs \u001b[38;5;241m=\u001b[39m clip_tokenizer(captions, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    128\u001b[0m                             return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m77\u001b[39m)\n\u001b[1;32m    129\u001b[0m text_inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(DEVICE) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m text_inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 130\u001b[0m text_emb \u001b[38;5;241m=\u001b[39m \u001b[43mclip_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m text_emb \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(text_emb, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Losses\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/site-packages/transformers/utils/generic.py:809\u001b[0m, in \u001b[0;36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m         cls_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    802\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    803\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    804\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    805\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    806\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    807\u001b[0m     )\n\u001b[0;32m--> 809\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:888\u001b[0m, in \u001b[0;36mCLIPModel.get_text_features\u001b[0;34m(self, input_ids, attention_mask, position_ids)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;129m@filter_out_non_signature_kwargs\u001b[39m()\n\u001b[1;32m    862\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_text_features\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     position_ids: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    868\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m    869\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;124;03m        text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The text embeddings obtained by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    ...     text_features = model.get_text_features(**inputs)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 888\u001b[0m     text_outputs: BaseModelOutputWithPooling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m text_outputs\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[1;32m    894\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_projection(pooled_output)\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/site-packages/peft/peft_model.py:2986\u001b[0m, in \u001b[0;36mPeftModelForFeatureExtraction.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2985\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 2986\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2996\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   2997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2998\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/eegenv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:308\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: CLIPTextTransformer.forward() got an unexpected keyword argument 'inputs_embeds'"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 3: LoRA ON EEG ENCODER (MANUAL)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================\n",
    "# 1. Load Stage 1 Best\n",
    "# ============================================\n",
    "print(\"\\n[1] Loading Stage 1 best...\")\n",
    "stage1_checkpoint = torch.load('task2b_best_frozen.pth', map_location=DEVICE)\n",
    "projection_head_lora = EEGToClipProjection(eeg_dim=128, clip_dim=512).to(DEVICE)\n",
    "projection_head_lora.load_state_dict(stage1_checkpoint['projection_head'])\n",
    "\n",
    "eeg_encoder_lora = EEG_ViT_1D(num_subjects=13, num_classes=20).to(DEVICE)\n",
    "task1_checkpoint = torch.load('best_model_vit_1d.pth', map_location=DEVICE)\n",
    "if isinstance(task1_checkpoint, dict):\n",
    "    eeg_encoder_lora.load_state_dict(task1_checkpoint['model_state_dict'])\n",
    "else:\n",
    "    eeg_encoder_lora.load_state_dict(task1_checkpoint)\n",
    "\n",
    "for param in eeg_encoder_lora.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"âœ“ Models loaded and frozen\")\n",
    "\n",
    "# ============================================\n",
    "# 2. Apply LoRA to CLIP Only (Skip EEG)\n",
    "# ============================================\n",
    "print(\"\\n[2] Applying LoRA to CLIP text encoder only...\")\n",
    "\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "clip_lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # This works for HuggingFace CLIP\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"FEATURE_EXTRACTION\"\n",
    ")\n",
    "\n",
    "clip_model.text_model = get_peft_model(clip_model.text_model, clip_lora_config)\n",
    "clip_model.text_model.print_trainable_parameters()\n",
    "\n",
    "print(\"âœ“ LoRA applied to CLIP\")\n",
    "\n",
    "# ============================================\n",
    "# 3. Manually Unfreeze Last 2 Layers of EEG\n",
    "# ============================================\n",
    "print(\"\\n[3] Unfreezing last 2 transformer layers of EEG encoder...\")\n",
    "\n",
    "# Unfreeze last 2 layers as alternative to LoRA\n",
    "num_layers = len(eeg_encoder_lora.transformer.layers)\n",
    "print(f\"   Total transformer layers: {num_layers}\")\n",
    "\n",
    "for layer in eeg_encoder_lora.transformer.layers[-2:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "print(\"âœ“ Last 2 EEG layers unfrozen\")\n",
    "\n",
    "# ============================================\n",
    "# 4. Setup Optimizer\n",
    "# ============================================\n",
    "print(\"\\n[4] Setting up optimizer...\")\n",
    "\n",
    "trainable_params = [\n",
    "    {'params': projection_head_lora.parameters(), 'lr': 1e-4},\n",
    "    {'params': eeg_encoder_lora.transformer.layers[-2:].parameters(), 'lr': 1e-5},\n",
    "    {'params': [p for p in clip_model.parameters() if p.requires_grad], 'lr': 5e-4}\n",
    "]\n",
    "\n",
    "optimizer_lora = optim.AdamW(trainable_params, weight_decay=1e-4)\n",
    "\n",
    "total_trainable = sum(p.numel() for group in trainable_params for p in group['params'])\n",
    "print(f\"Total trainable parameters: {total_trainable:,}\")\n",
    "\n",
    "# ============================================\n",
    "# 5. Training Loop (Same as Before)\n",
    "# ============================================\n",
    "print(\"\\n[5] Starting training with LoRA on CLIP...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_recall_lora = stage1_checkpoint['val_recall']\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "loss_computer = Task2BLosses()\n",
    "\n",
    "NUM_EPOCHS_LORA = 20\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_LORA):\n",
    "    eeg_encoder_lora.train()\n",
    "    projection_head_lora.train()\n",
    "    clip_model.train()\n",
    "    \n",
    "    epoch_losses = {'total': 0, 'cosine': 0, 'contrastive': 0}\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS_LORA}\", leave=False):\n",
    "        eeg_data = batch['eeg'].to(DEVICE)\n",
    "        captions = batch['caption']\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        subject_ids = batch['subject_id'].to(DEVICE)\n",
    "        \n",
    "        optimizer_lora.zero_grad()\n",
    "        \n",
    "        # EEG forward\n",
    "        x = eeg_data\n",
    "        if x.shape[1] == 500 and x.shape[2] == 122:\n",
    "            x = x.permute(0, 2, 1)\n",
    "        if x.shape[2] > 500:\n",
    "            x = x[:, :, :500]\n",
    "        \n",
    "        x = eeg_encoder_lora.tokenizer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        b, seq_len, _ = x.shape\n",
    "        \n",
    "        cls_tokens = eeg_encoder_lora.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + eeg_encoder_lora.pos_embedding[:, :seq_len + 1, :]\n",
    "        x = eeg_encoder_lora.transformer(x)\n",
    "        eeg_cls = x[:, 0, :]\n",
    "        \n",
    "        eeg_projected = projection_head_lora(eeg_cls)\n",
    "        \n",
    "        # CLIP text\n",
    "        text_inputs = clip_tokenizer(captions, padding=True, truncation=True,\n",
    "                                    return_tensors=\"pt\", max_length=77)\n",
    "        text_inputs = {k: v.to(DEVICE) for k, v in text_inputs.items()}\n",
    "        text_emb = clip_model.get_text_features(**text_inputs)\n",
    "        text_emb = F.normalize(text_emb, p=2, dim=-1)\n",
    "        \n",
    "        # Losses\n",
    "        loss_cos = loss_computer.cosine_similarity_loss(eeg_projected, text_emb)\n",
    "        loss_contrast = loss_computer.contrastive_loss_debiased(eeg_projected, text_emb, labels)\n",
    "        \n",
    "        total_loss = 0.5 * loss_cos + 0.5 * loss_contrast\n",
    "        \n",
    "        epoch_losses['cosine'] += loss_cos.item()\n",
    "        epoch_losses['contrastive'] += loss_contrast.item()\n",
    "        epoch_losses['total'] += total_loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(eeg_encoder_lora.transformer.layers[-2:].parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer_lora.step()\n",
    "    \n",
    "    avg_total = epoch_losses['total'] / num_batches\n",
    "    avg_cos = epoch_losses['cosine'] / num_batches\n",
    "    avg_contrast = epoch_losses['contrastive'] / num_batches\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS_LORA} - Loss: {avg_total:.4f} \" +\n",
    "          f\"(Cos: {avg_cos:.4f}, Contrast: {avg_contrast:.4f})\")\n",
    "    \n",
    "    # Validation\n",
    "    if (epoch + 1) % 3 == 0 or epoch == NUM_EPOCHS_LORA - 1:\n",
    "        val_recall = evaluate_eeg_caption_retrieval(\n",
    "            eeg_encoder_lora, projection_head_lora, clip_model,\n",
    "            clip_tokenizer, val_loader, DEVICE, k=5)\n",
    "        \n",
    "        print(f\"  â†’ Val Recall@5: {val_recall:.2f}%\")\n",
    "        \n",
    "        if val_recall > best_val_recall_lora:\n",
    "            best_val_recall_lora = val_recall\n",
    "            patience_counter = 0\n",
    "            \n",
    "            torch.save({\n",
    "                'eeg_encoder': eeg_encoder_lora.state_dict(),\n",
    "                'projection_head': projection_head_lora.state_dict(),\n",
    "                'epoch': epoch + 1,\n",
    "                'val_recall': val_recall,\n",
    "                'strategy': 'clip_lora_eeg_partial'\n",
    "            }, 'task2b_best_lora.pth')\n",
    "            \n",
    "            print(f\"  âœ“ NEW BEST! Saved\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{patience})\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nâš ï¸ Early stopping!\")\n",
    "                break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"Stage 1 (Frozen):     {stage1_checkpoint['val_recall']:.2f}%\")\n",
    "print(f\"Stage 3 (Hybrid):     {best_val_recall_lora:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/jet/home/gulavani'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (eegenv)",
   "language": "python",
   "name": "eegenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
