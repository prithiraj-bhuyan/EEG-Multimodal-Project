{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG-Based Visual Recognition: Classification to Semantic Retrieval\n",
    "## Final Project Submission - Group 10\n",
    "\n",
    "**Team Members:** Madhavi Gulavani, Praneeth Chaitanya Jonnavithula, Prithiraj Bhuyan\n",
    "\n",
    "**Course:** 11-685 Introduction to Deep Learning (Fall 2025)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook implements:\n",
    "1. **Task 1**: EEG-based image classification with advanced architectures\n",
    "2. **Task 2A**: Image-Caption retrieval using pretrained CLIP\n",
    "3. **Task 2B**: EEG-Caption retrieval with various CLIP fine-tuning strategies\n",
    "4. **Comprehensive Evaluation**: All required metrics and analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from transformers import CLIPModel, CLIPProcessor, CLIPTokenizer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set Seed\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ… Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /jet/home/gulavani/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login to WandB\n",
    "wandb.login(key=\"825201e63a02e53435b53a136158ab39815c89a4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIDS_ROOT = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/ds005589'\n",
    "IMAGE_DIR = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/images'\n",
    "CAPTIONS_FILE = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/captions.txt'\n",
    "ALL_SUBJECTS = ['sub-02', 'sub-03', 'sub-05', 'sub-09', 'sub-14', 'sub-15', \n",
    "                'sub-17', 'sub-19', 'sub-20', 'sub-23', 'sub-24', 'sub-28', 'sub-29']\n",
    "\n",
    "class EEG_Paper_Replication_Dataset(Dataset):\n",
    "    def __init__(self, bids_root, images_dir, captions_path, \n",
    "                 subject_list, session_list, \n",
    "                 clamp_thres=500, stats=None):\n",
    "        \n",
    "        self.bids_root = bids_root\n",
    "        self.images_dir = images_dir\n",
    "        self.clamp_thres = clamp_thres\n",
    "        self.trial_metadata = []\n",
    "        \n",
    "        # --- Create Subject Mapping ---\n",
    "        # Map 'sub-02' -> 0, 'sub-03' -> 1, etc.\n",
    "        # We sort to ensure consistency across Train/Val/Test sets\n",
    "        self.subject_to_idx = {sub: i for i, sub in enumerate(sorted(list(set(subject_list))))}\n",
    "        \n",
    "        # 1. Load Captions Helper\n",
    "        self.captions_dict = self._load_captions(captions_path)\n",
    "        self.category_to_idx = {cat: i for i, cat in enumerate(sorted(set(c for c, _ in self.captions_dict.values())))}\n",
    "        \n",
    "        # 2. Scan Metadata\n",
    "        print(f\"Scanning metadata for {session_list}...\")\n",
    "        for sub in subject_list:\n",
    "            for ses in session_list:\n",
    "                for run in ['01', '02', '03', '04']:\n",
    "                    session_path = os.path.join(self.bids_root, sub, ses)\n",
    "                    csv_path = os.path.join(session_path, f\"{sub}_{ses}_task-lowSpeed_run-{run}_image.csv\")\n",
    "                    npy_path = os.path.join(session_path, f\"{sub}_{ses}_task-lowSpeed_run-{run}_1000Hz.npy\")\n",
    "                    \n",
    "                    if not (os.path.exists(csv_path) and os.path.exists(npy_path)):\n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        csv_data = pd.read_csv(csv_path)\n",
    "                        for i, row in csv_data.iterrows():\n",
    "                            img_base_name = self._get_base_name(row['FilePath'])\n",
    "                            if not img_base_name: continue\n",
    "                            \n",
    "                            category, caption = self.captions_dict.get(img_base_name, (None, None))\n",
    "                            if not category: continue\n",
    "                            \n",
    "                            self.trial_metadata.append({\n",
    "                                'npy_path': npy_path,\n",
    "                                'trial_index': i,\n",
    "                                'label': self.category_to_idx[category],\n",
    "                                'subject_id': self.subject_to_idx[sub],\n",
    "                                'image_name': img_base_name,\n",
    "                                'caption': caption \n",
    "                            })\n",
    "                    except:\n",
    "                        continue\n",
    "        \n",
    "        print(f\"Found {len(self.trial_metadata)} trials.\")\n",
    "\n",
    "        # 3. Compute Global Statistics (GFS)\n",
    "        if stats is None:\n",
    "            print(\"Computing Global Statistics (this takes ~1 min)...\")\n",
    "            self.mean, self.std = self._compute_global_stats()\n",
    "        else:\n",
    "            self.mean, self.std = stats\n",
    "\n",
    "    def _compute_global_stats(self):\n",
    "        subset_indices = range(0, len(self.trial_metadata), 10)\n",
    "        sum_x = 0\n",
    "        sum_sq_x = 0\n",
    "        count = 0\n",
    "        \n",
    "        for i in tqdm(subset_indices, desc=\"Calculating Stats\"):\n",
    "            meta = self.trial_metadata[i]\n",
    "            d = np.load(meta['npy_path'])[meta['trial_index']]\n",
    "            d = np.clip(d, -self.clamp_thres, self.clamp_thres)\n",
    "            \n",
    "            sum_x += np.mean(d)\n",
    "            sum_sq_x += np.mean(d**2)\n",
    "            count += 1\n",
    "            \n",
    "        global_mean = sum_x / count\n",
    "        global_std = np.sqrt((sum_sq_x / count) - (global_mean**2))\n",
    "        return float(global_mean), float(global_std)\n",
    "\n",
    "    def get_stats(self): return self.mean, self.std\n",
    "\n",
    "    def _load_captions(self, path):\n",
    "        d = {}\n",
    "        with open(path, 'r') as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 4: d[parts[2]] = (parts[1], parts[3])\n",
    "        return d\n",
    "\n",
    "    def _get_base_name(self, p):\n",
    "        try:\n",
    "            bn = os.path.splitext(os.path.basename(str(p).replace('\\\\', '/')))[0]\n",
    "            if bn.endswith('_resized'): return bn[:-8]\n",
    "            return bn\n",
    "        except: return None\n",
    "\n",
    "    def _get_image_path(self, image_name):\n",
    "        \"\"\"Find full path to image file\"\"\"\n",
    "        for ext in ['.jpg', '.jpeg', '.png', '.JPEG', '.JPG']:\n",
    "            path = os.path.join(self.images_dir, image_name + ext)\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "        return None  # Image not found\n",
    "\n",
    "    def __len__(self): return len(self.trial_metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        meta = self.trial_metadata[idx]\n",
    "        eeg_data = np.load(meta['npy_path'])[meta['trial_index']]\n",
    "        eeg_data = np.clip(eeg_data, -self.clamp_thres, self.clamp_thres)\n",
    "        # Global Feature Standardization\n",
    "        eeg_data = (eeg_data - self.mean) / (self.std + 1e-6)\n",
    "\n",
    "        img_path = self._get_image_path(meta['image_name'])\n",
    "        caption = meta['caption']\n",
    "        \n",
    "        return {\n",
    "            'eeg': torch.tensor(eeg_data, dtype=torch.float32),\n",
    "            'label': torch.tensor(meta['label'], dtype=torch.long),\n",
    "            'subject_id': torch.tensor(meta['subject_id'], dtype=torch.long),\n",
    "            'image_path': img_path,  # for Task 2\n",
    "            'caption': caption  # for Task 2\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Init Train ---\n",
      "Scanning metadata for ['ses-01', 'ses-02', 'ses-03']...\n",
      "Found 15600 trials.\n",
      "Computing Global Statistics (this takes ~1 min)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3f4e85342c4f22a8267aa66dbfd4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Stats:   0%|          | 0/1560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Init Validation ---\n",
      "Scanning metadata for ['ses-04']...\n",
      "Found 5200 trials.\n",
      "Loading Test Set (Session 5)...\n",
      "Scanning metadata for ['ses-05']...\n",
      "Found 5200 trials.\n",
      "\n",
      "âœ… Loaders Ready: 488 training batches, 163 validation batches.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE DATASETS AND DATALOADERS\n",
    "# ============================================================================\n",
    "# 1. Create Transforms (Standard ImageNet stats)\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 2. Instantiate Training Dataset\n",
    "print(\"--- Init Train ---\")\n",
    "train_ds = EEG_Paper_Replication_Dataset(\n",
    "    bids_root=BIDS_ROOT, \n",
    "    images_dir=IMAGE_DIR, \n",
    "    captions_path=CAPTIONS_FILE, \n",
    "    subject_list=ALL_SUBJECTS, \n",
    "    session_list=['ses-01', 'ses-02', 'ses-03'], \n",
    "    clamp_thres=500\n",
    ")\n",
    "# Save stats to use for validation (Prevent data leakage)\n",
    "stats = train_ds.get_stats()\n",
    "\n",
    "# 3. Instantiate Validation Dataset\n",
    "print(\"\\n--- Init Validation ---\")\n",
    "val_ds = EEG_Paper_Replication_Dataset(\n",
    "    bids_root=BIDS_ROOT, \n",
    "    images_dir=IMAGE_DIR, \n",
    "    captions_path=CAPTIONS_FILE, \n",
    "    subject_list=ALL_SUBJECTS, \n",
    "    session_list=['ses-04'], \n",
    "    clamp_thres=500,\n",
    "    stats=stats # <--- IMPORTANT: Use training stats\n",
    ")\n",
    "\n",
    "print(\"Loading Test Set (Session 5)...\")\n",
    "test_ds = EEG_Paper_Replication_Dataset(\n",
    "    BIDS_ROOT, IMAGE_DIR, CAPTIONS_FILE, ALL_SUBJECTS, \n",
    "    ['ses-05'], # FINAL TEST SET\n",
    "    stats=stats,\n",
    "    clamp_thres=500\n",
    ")\n",
    "\n",
    "# 4. Define The Loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"\\nâœ… Loaders Ready: {len(train_loader)} training batches, {len(val_loader)} validation batches.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: EEG Classification\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "We implement a multi-head architecture with:\n",
    "1. **CNN Feature Extractor**: Extracts temporal features from EEG channels\n",
    "2. **Transformer Backbone**: Models relationships across channels (shared across subjects)\n",
    "3. **Subject-Specific Heads**: Separate classification heads for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG_ViT_1D(nn.Module):\n",
    "    def __init__(self, num_subjects=13, num_classes=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Hyperparameters ---\n",
    "        self.patch_size = 50      # 50ms window \n",
    "        self.stride = 25          # 50% overlap\n",
    "        self.embed_dim = 128      # Feature size\n",
    "        self.num_heads = 4\n",
    "        self.depth = 2\n",
    "        \n",
    "        # --- 1. Tokenizer (The \"Patchify\" Step) ---\n",
    "        # Input: (Batch, 122, 500) -> Output: (Batch, 128, ~19)\n",
    "        self.tokenizer = nn.Sequential(\n",
    "            nn.Conv1d(122, self.embed_dim, kernel_size=self.patch_size, stride=self.stride, padding=0),\n",
    "            nn.BatchNorm1d(self.embed_dim),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        \n",
    "        # --- 2. Learnable \"Class Token\" ---\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.embed_dim))\n",
    "        \n",
    "        # --- 3. Positional Embedding ---\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 32, self.embed_dim) * 0.01)\n",
    "        \n",
    "        # --- 4. Transformer Encoder ---\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim, \n",
    "            nhead=self.num_heads, \n",
    "            dim_feedforward=512, \n",
    "            dropout=0.5, \n",
    "            batch_first=True,\n",
    "            norm_first=True \n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=self.depth)\n",
    "        \n",
    "        # --- 5. Subject-Specific Heads ---\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(self.embed_dim, num_classes) for _ in range(num_subjects)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, subject_ids):\n",
    "        # Input comes in as (Batch, 500, 122) -> We need (Batch, 122, 500)\n",
    "        if x.shape[1] == 500 and x.shape[2] == 122:\n",
    "            x = x.permute(0, 2, 1)\n",
    "            \n",
    "        # Safety Crop (in case data is >500)\n",
    "        if x.shape[2] > 500:\n",
    "            x = x[:, :, :500]\n",
    "            \n",
    "        # ----------------------------------------\n",
    "        \n",
    "        # 1. Tokenize\n",
    "        x = self.tokenizer(x)     # Output: (Batch, 128, 19)\n",
    "        x = x.permute(0, 2, 1)    # Output: (Batch, 19, 128) -> (Batch, Seq, Dim)\n",
    "        \n",
    "        b, seq_len, _ = x.shape\n",
    "        \n",
    "        # 2. Append CLS Token\n",
    "        cls_tokens = self.cls_token.expand(b, -1, -1) \n",
    "        x = torch.cat((cls_tokens, x), dim=1) # (Batch, 20, 128)\n",
    "        \n",
    "        # 3. Add Positional Embedding\n",
    "        x = x + self.pos_embedding[:, :seq_len + 1, :]\n",
    "        \n",
    "        # 4. Transformer Attention\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # 5. Extract ONLY the CLS token output\n",
    "        cls_output = x[:, 0, :]   # (Batch, 128)\n",
    "        \n",
    "        # 6. Subject Routing\n",
    "        logits = torch.zeros(x.shape[0], 20).to(x.device)\n",
    "        unique_subs = torch.unique(subject_ids)\n",
    "        \n",
    "        for sub in unique_subs:\n",
    "            mask = (subject_ids == sub)\n",
    "            logits[mask] = self.heads[sub.long()](cls_output[mask])\n",
    "            \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_TRAINING = False\n",
    "CHECKPOINT_PATH = \"best_model_vit_1d.pth\"\n",
    "TOTAL_EPOCHS = 100\n",
    "\n",
    "wandb.init(\n",
    "    project=\"eeg-classification\",\n",
    "    name=\"vit-1d-run\",\n",
    "    config={\n",
    "        \"architecture\": \"ViT-1D\",\n",
    "        \"dataset\": \"GFS\",\n",
    "        \"epochs\": TOTAL_EPOCHS,\n",
    "        \"lr\": 1e-3\n",
    "    }\n",
    ")\n",
    "\n",
    "model = EEG_ViT_1D(num_subjects=13, num_classes=20).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "start_epoch = 1\n",
    "best_val_acc = 0.0\n",
    "\n",
    "if RESUME_TRAINING and os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"ðŸ”„ Attempting to resume from {CHECKPOINT_PATH}...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "        \n",
    "        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            if 'optimizer_state_dict' in checkpoint:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "            best_val_acc = checkpoint.get('val_acc', 0.0)\n",
    "            print(f\"âœ… Resuming from Epoch {start_epoch} with Best Acc {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "            print(f\"âš ï¸ Only weights found. Resuming from Epoch 1.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading checkpoint: {e}. Starting fresh.\")\n",
    "else:\n",
    "    print(\"ðŸ†• Starting a fresh training run.\")\n",
    "\n",
    "print(f\"\\nðŸš€ Training from Epoch {start_epoch} to {TOTAL_EPOCHS}...\")\n",
    "\n",
    "for epoch in range(start_epoch, TOTAL_EPOCHS + 1):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "        eeg = batch['eeg'].to(DEVICE)\n",
    "        label = batch['label'].to(DEVICE)\n",
    "        sub_id = batch['subject_id'].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(eeg, sub_id)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, pred = outputs.max(1)\n",
    "        correct += pred.eq(label).sum().item()\n",
    "        total += label.size(0)\n",
    "        \n",
    "    train_acc = 100. * correct / total\n",
    "    \n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            eeg = batch['eeg'].to(DEVICE)\n",
    "            label = batch['label'].to(DEVICE)\n",
    "            sub_id = batch['subject_id'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(eeg, sub_id)\n",
    "            _, pred = outputs.max(1)\n",
    "            val_correct += pred.eq(label).sum().item()\n",
    "            val_total += label.size(0)\n",
    "            \n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    \n",
    "    print(f\"Epoch {epoch:02d}: Train Acc {train_acc:.2f}% | Val Acc {val_acc:.2f}%\")\n",
    "    \n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"train_loss\": loss.item()\n",
    "    })\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'train_acc': train_acc\n",
    "        }, CHECKPOINT_PATH)\n",
    "        \n",
    "        wandb.save(CHECKPOINT_PATH) \n",
    "        \n",
    "        print(f\"  âœ… Best Model Saved & Uploaded! ({val_acc:.2f}%)\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_detailed(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    subject_results = {i: {'correct': 0, 'total': 0} for i in range(len(ALL_SUBJECTS))}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for eeg, label, sub_id in tqdm(loader, desc=\"Testing\"):\n",
    "            eeg, label, sub_id = eeg.to(device), label.to(device), sub_id.to(device)\n",
    "            \n",
    "            outputs = model(eeg, sub_id)\n",
    "            _, preds = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            \n",
    "            for i in range(len(label)):\n",
    "                sid = sub_id[i].item()\n",
    "                is_correct = (preds[i] == label[i]).item()\n",
    "                subject_results[sid]['total'] += 1\n",
    "                subject_results[sid]['correct'] += is_correct\n",
    "                \n",
    "    return all_labels, all_preds, subject_results\n",
    "\n",
    "print(\"Running Final Evaluation...\")\n",
    "y_true, y_pred, sub_metrics = evaluate_detailed(model, test_loader, DEVICE)\n",
    "\n",
    "overall_acc = accuracy_score(y_true, y_pred) * 100\n",
    "print(f\"\\nðŸ† Final Test Accuracy: {overall_acc:.2f}%\")\n",
    "\n",
    "sub_accs = []\n",
    "sub_names = []\n",
    "for sid, metrics in sub_metrics.items():\n",
    "    if metrics['total'] > 0:\n",
    "        acc = (metrics['correct'] / metrics['total']) * 100\n",
    "        sub_accs.append(acc)\n",
    "        sub_names.append(ALL_SUBJECTS[sid])\n",
    "        \n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(x=sub_names, y=sub_accs, palette=\"viridis\")\n",
    "plt.axhline(y=overall_acc, color='r', linestyle='--', label=f'Avg: {overall_acc:.1f}%')\n",
    "plt.axhline(y=5.0, color='gray', linestyle='--', label='Random Chance (5%)')\n",
    "plt.title(\"Per-Subject Classification Accuracy (Session 5)\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "cat_map = {v: k for k, v in test_ds.category_to_idx.items()}\n",
    "cat_names = [cat_map[i] for i in range(len(cat_map))]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm_norm, annot=False, fmt=\".2f\", cmap=\"Blues\", xticklabels=cat_names, yticklabels=cat_names)\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2A: Image-Caption Retrieval with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TASK 2A: IMAGE-CAPTION RETRIEVAL WITH CLIP\n",
      "============================================================\n",
      "âœ“ CLIP model loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 2A: IMAGE-CAPTION RETRIEVAL WITH CLIP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load pretrained CLIP\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "clip_model.eval()\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"âœ“ CLIP model loaded\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_clip_embeddings(dataset, clip_model, clip_processor, clip_tokenizer, \n",
    "                            device, batch_size=32):\n",
    "    \"\"\"Extract CLIP embeddings for images and captions\"\"\"\n",
    "    \n",
    "    # Get unique images and captions\n",
    "    unique_data = {}\n",
    "    for item in dataset.trial_metadata:\n",
    "        img_name = item['image_name']  # Changed\n",
    "        if img_name not in unique_data:\n",
    "            img_path = dataset._get_image_path(img_name)\n",
    "            unique_data[img_name] = {\n",
    "                'caption': item['caption'],\n",
    "                'category_label': item['label'],  # Changed\n",
    "                'img_path': img_path\n",
    "            }\n",
    "    \n",
    "    image_names = list(unique_data.keys())\n",
    "    captions = [unique_data[name]['caption'] for name in image_names]\n",
    "    labels = torch.tensor([unique_data[name]['category_label'] for name in image_names])\n",
    "    \n",
    "    print(f\"Extracting embeddings for {len(image_names)} unique images...\")\n",
    "    \n",
    "    image_embeddings = []\n",
    "    text_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(image_names), batch_size)):\n",
    "        batch_names = image_names[i:i+batch_size]\n",
    "        batch_captions = captions[i:i+batch_size]\n",
    "        \n",
    "        # Load images\n",
    "        batch_images = []\n",
    "        for name in batch_names:\n",
    "            try:\n",
    "                img = Image.open(unique_data[name]['img_path']).convert('RGB')\n",
    "                batch_images.append(img)\n",
    "            except:\n",
    "                batch_images.append(Image.new('RGB', (224, 224)))\n",
    "        \n",
    "        # Process\n",
    "        image_inputs = clip_processor(images=batch_images, return_tensors=\"pt\", padding=True)\n",
    "        image_inputs = {k: v.to(device) for k, v in image_inputs.items()}\n",
    "        \n",
    "        text_inputs = clip_tokenizer(batch_captions, padding=True, truncation=True, \n",
    "                                    return_tensors=\"pt\", max_length=77)\n",
    "        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "        \n",
    "        # Get embeddings\n",
    "        image_features = clip_model.get_image_features(**image_inputs)\n",
    "        text_features = clip_model.get_text_features(**text_inputs)\n",
    "        \n",
    "        # Normalize\n",
    "        image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "        text_features = F.normalize(text_features, p=2, dim=-1)\n",
    "        \n",
    "        image_embeddings.append(image_features.cpu())\n",
    "        text_embeddings.append(text_features.cpu())\n",
    "    \n",
    "    image_embeddings = torch.cat(image_embeddings, dim=0)\n",
    "    text_embeddings = torch.cat(text_embeddings, dim=0)\n",
    "    \n",
    "    return {\n",
    "        'image_names': image_names,\n",
    "        'captions': captions,\n",
    "        'labels': labels,\n",
    "        'image_embeddings': image_embeddings,\n",
    "        'text_embeddings': text_embeddings\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_recall_at_k(similarities, k_values=[1, 3, 5], class_aware=False, labels=None):\n",
    "    \"\"\"Compute Recall@K\"\"\"\n",
    "    N = similarities.shape[0]\n",
    "    \n",
    "    # Move similarities to same device for sorting\n",
    "    device = similarities.device\n",
    "    top_k_indices = torch.argsort(similarities, dim=1, descending=True)\n",
    "    \n",
    "    # Ensure labels are on the same device if using class-aware\n",
    "    if class_aware and labels is not None:\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        correct = 0\n",
    "        \n",
    "        for i in range(N):\n",
    "            top_k = top_k_indices[i, :k]\n",
    "            \n",
    "            if class_aware and labels is not None:\n",
    "                query_label = labels[i]\n",
    "                retrieved_labels = labels[top_k]\n",
    "                if (retrieved_labels == query_label).any():\n",
    "                    correct += 1\n",
    "            else:\n",
    "                if i in top_k:\n",
    "                    correct += 1\n",
    "        \n",
    "        results[f\"Recall@{k}\"] = 100.0 * correct / N\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_map(similarities, class_aware=False, labels=None):\n",
    "    \"\"\"Compute Mean Average Precision\"\"\"\n",
    "    N = similarities.shape[0]\n",
    "    device = similarities.device\n",
    "    \n",
    "    # Ensure everything is on CPU for numpy operations\n",
    "    similarities_cpu = similarities.cpu() if similarities.is_cuda else similarities\n",
    "    sorted_indices = torch.argsort(similarities_cpu, dim=1, descending=True)\n",
    "    \n",
    "    if labels is not None:\n",
    "        labels_cpu = labels.cpu() if labels.is_cuda else labels\n",
    "        labels_np = labels_cpu.numpy()\n",
    "    \n",
    "    average_precisions = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        sorted_idx = sorted_indices[i]\n",
    "        \n",
    "        if class_aware and labels is not None:\n",
    "            query_label = labels_np[i]\n",
    "            relevant_mask = (labels_np == query_label)\n",
    "        else:\n",
    "            relevant_mask = np.zeros(N, dtype=bool)\n",
    "            relevant_mask[i] = True\n",
    "        \n",
    "        precisions = []\n",
    "        num_relevant = 0\n",
    "        \n",
    "        for rank, idx in enumerate(sorted_idx, 1):\n",
    "            if relevant_mask[idx]:\n",
    "                num_relevant += 1\n",
    "                precisions.append(num_relevant / rank)\n",
    "        \n",
    "        if precisions:\n",
    "            average_precisions.append(np.mean(precisions))\n",
    "        else:\n",
    "            average_precisions.append(0.0)\n",
    "    \n",
    "    return np.mean(average_precisions)\n",
    "\n",
    "\n",
    "def evaluate_task2a(test_dataset, clip_model, clip_processor, clip_tokenizer, device):\n",
    "    \"\"\"Full evaluation for Task 2A\"\"\"\n",
    "    \n",
    "    # Extract embeddings\n",
    "    embeddings = extract_clip_embeddings(\n",
    "        test_dataset, clip_model, clip_processor, clip_tokenizer, device)\n",
    "    \n",
    "    image_emb = embeddings['image_embeddings'].to(device)\n",
    "    text_emb = embeddings['text_embeddings'].to(device)\n",
    "    labels = embeddings['labels']  # Keep on CPU initially\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = torch.mm(image_emb, text_emb.t())\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TASK 2A: IMAGE-CAPTION RETRIEVAL RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Instance-level Recall@K\n",
    "    print(\"\\n1. Instance-Level Recall@K:\")\n",
    "    instance_recall = compute_recall_at_k(similarities, k_values=[1, 3, 5])\n",
    "    for k, v in instance_recall.items():\n",
    "        print(f\"   {k}: {v:.2f}%\")\n",
    "    \n",
    "    # Class-aware Recall@K (labels will be moved to device inside function)\n",
    "    print(\"\\n2. Class-Aware Recall@K:\")\n",
    "    class_recall = compute_recall_at_k(similarities, k_values=[1, 3, 5], \n",
    "                                      class_aware=True, labels=labels)\n",
    "    for k, v in class_recall.items():\n",
    "        print(f\"   {k}: {v:.2f}%\")\n",
    "    \n",
    "    # MAP (will be computed on CPU inside function)\n",
    "    print(\"\\n3. Mean Average Precision:\")\n",
    "    instance_map = compute_map(similarities, labels=labels)\n",
    "    print(f\"   Instance-Level MAP: {instance_map:.4f}\")\n",
    "    \n",
    "    class_map = compute_map(similarities, class_aware=True, labels=labels)\n",
    "    print(f\"   Class-Aware MAP: {class_map:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'instance_recall': instance_recall,\n",
    "        'class_recall': class_recall,\n",
    "        'instance_map': instance_map,\n",
    "        'class_map': class_map,\n",
    "        'embeddings': embeddings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING TASK 2A: IMAGE-CAPTION RETRIEVAL\n",
      "============================================================\n",
      "Extracting embeddings for 4046 unique images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf04785a09534f54a6cfbbab1c5bf3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TASK 2A: IMAGE-CAPTION RETRIEVAL RESULTS\n",
      "============================================================\n",
      "\n",
      "1. Instance-Level Recall@K:\n",
      "   Recall@1: 27.63%\n",
      "   Recall@3: 44.86%\n",
      "   Recall@5: 53.51%\n",
      "\n",
      "2. Class-Aware Recall@K:\n",
      "   Recall@1: 96.69%\n",
      "   Recall@3: 98.10%\n",
      "   Recall@5: 98.64%\n",
      "\n",
      "3. Mean Average Precision:\n",
      "   Instance-Level MAP: 0.3994\n",
      "   Class-Aware MAP: 0.8308\n",
      "\n",
      "Task 2A (Image-Caption) Recall@5: 53.51%\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# TASK 2A: IMAGE-CAPTION RETRIEVAL\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TASK 2A: IMAGE-CAPTION RETRIEVAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Task 2A: Evaluate CLIP on image-caption retrieval\n",
    "task2a_results = evaluate_task2a(test_ds, clip_model, clip_processor, \n",
    "                                  clip_tokenizer, DEVICE)\n",
    "\n",
    "print(f\"\\nTask 2A (Image-Caption) Recall@5: {task2a_results['instance_recall']['Recall@5']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2B: EEG-Caption Retrieval\n",
    "\n",
    "### EEG-to-CLIP Projection Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TASK 2B: EEG-CAPTION RETRIEVAL\n",
      "============================================================\n",
      "\n",
      "[1/5] Loading trained EEG encoder from Task 1...\n",
      "âœ“ Loaded checkpoint from epoch 98\n",
      "âœ“ Task 1 validation accuracy was: 9.50%\n",
      "âœ“ EEG encoder loaded and ready\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 2B: EEG-CAPTION RETRIEVAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the trained EEG encoder from Task 1\n",
    "print(\"\\n[1/5] Loading trained EEG encoder from Task 1...\")\n",
    "\n",
    "eeg_encoder = EEG_ViT_1D(num_subjects=13, num_classes=20).to(DEVICE)\n",
    "\n",
    "# Load your friend's checkpoint\n",
    "checkpoint = torch.load('best_model_vit_1d.pth', map_location=DEVICE)\n",
    "\n",
    "if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "    eeg_encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"âœ“ Loaded checkpoint from epoch {checkpoint.get('epoch', '?')}\")\n",
    "    print(f\"âœ“ Task 1 validation accuracy was: {checkpoint.get('val_acc', 0):.2f}%\")\n",
    "else:\n",
    "    eeg_encoder.load_state_dict(checkpoint)\n",
    "    print(\"âœ“ Loaded model weights\")\n",
    "\n",
    "# Remove the classification heads - we only need the encoder part\n",
    "eeg_encoder.eval()\n",
    "\n",
    "print(\"âœ“ EEG encoder loaded and ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/5] Creating projection head for CLIP alignment...\n",
      "âœ“ Projection head created\n",
      "  Trainable parameters: 164,608\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2/5] Creating projection head for CLIP alignment...\")\n",
    "\n",
    "class EEGToClipProjection(nn.Module):\n",
    "    \"\"\"Projects EEG embeddings to CLIP text embedding space\"\"\"\n",
    "    \n",
    "    def __init__(self, eeg_dim=128, clip_dim=512, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Small MLP projection head\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(eeg_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, clip_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, eeg_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            eeg_embedding: (batch, 128) from EEG encoder's CLS token\n",
    "        Returns:\n",
    "            projected: (batch, 512) normalized CLIP-space embedding\n",
    "        \"\"\"\n",
    "        projected = self.projection(eeg_embedding)\n",
    "        # Normalize to unit length (CRITICAL for cosine similarity)\n",
    "        projected = F.normalize(projected, p=2, dim=-1)\n",
    "        return projected\n",
    "\n",
    "projection_head = EEGToClipProjection(eeg_dim=128, clip_dim=512).to(DEVICE)\n",
    "\n",
    "print(f\"âœ“ Projection head created\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in projection_head.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/5] Implementing loss functions...\n",
      "âœ“ Loss functions implemented:\n",
      "  - Cosine similarity (KD similarity-based)\n",
      "  - Debiased contrastive (InfoNCE with soft negatives)\n",
      "  - KL divergence (KD logit-based)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[3/5] Implementing loss functions...\")\n",
    "\n",
    "class Task2BLosses:\n",
    "    \"\"\"Loss functions for EEG-Caption alignment\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_similarity_loss(eeg_emb, text_emb):\n",
    "        \"\"\"\n",
    "        Knowledge Distillation: Similarity-based\n",
    "        Align EEG with ground-truth caption\n",
    "        \"\"\"\n",
    "        # Both should be normalized already\n",
    "        cosine_sim = torch.sum(eeg_emb * text_emb, dim=-1)  # (batch,)\n",
    "        loss = 1 - cosine_sim.mean()  # Want similarity = 1\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def contrastive_loss_debiased(eeg_emb, text_emb, labels, temperature=0.07):\n",
    "        \"\"\"\n",
    "        Debiased Contrastive Loss (InfoNCE with soft negatives)\n",
    "        Down-weights same-class captions in negative set\n",
    "        \n",
    "        Args:\n",
    "            eeg_emb: (batch, 512) normalized EEG embeddings\n",
    "            text_emb: (batch, 512) normalized text embeddings  \n",
    "            labels: (batch,) category labels\n",
    "            temperature: scaling factor\n",
    "        \"\"\"\n",
    "        batch_size = eeg_emb.shape[0]\n",
    "        \n",
    "        # Compute similarity matrix: (batch, batch)\n",
    "        logits = torch.mm(eeg_emb, text_emb.t()) / temperature\n",
    "        \n",
    "        # Create weight matrix for negatives\n",
    "        # Same class = lower weight, different class = full weight\n",
    "        labels_eq = labels.unsqueeze(0) == labels.unsqueeze(1)  # (batch, batch)\n",
    "        \n",
    "        # Diagonal = positives (weight doesn't matter, excluded from denominator)\n",
    "        # Same class = 0.3 weight, different class = 1.0 weight\n",
    "        weights = torch.where(labels_eq, \n",
    "                            torch.tensor(0.3, device=DEVICE),\n",
    "                            torch.tensor(1.0, device=DEVICE))\n",
    "        weights = weights.fill_diagonal_(0)  # Ignore diagonal in denominator\n",
    "        \n",
    "        # Compute loss\n",
    "        # Numerator: positive pairs (diagonal)\n",
    "        positive_logits = torch.diag(logits)  # (batch,)\n",
    "        \n",
    "        # Denominator: weighted sum of all pairs\n",
    "        exp_logits = torch.exp(logits)\n",
    "        weighted_exp = exp_logits * weights\n",
    "        \n",
    "        # Add back the positive (diagonal) \n",
    "        denominator = weighted_exp.sum(dim=1) + torch.exp(positive_logits)\n",
    "        \n",
    "        loss = -torch.log(torch.exp(positive_logits) / denominator).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def kl_divergence_loss(eeg_emb, text_emb_all, image_emb, temperature_teacher=0.07, temperature_student=0.07):\n",
    "        \"\"\"\n",
    "        Knowledge Distillation: Logit-based\n",
    "        Student (EEG) mimics Teacher (CLIP image encoder) distribution\n",
    "        \n",
    "        Args:\n",
    "            eeg_emb: (batch, 512) student embeddings\n",
    "            text_emb_all: (N_captions, 512) all caption embeddings in dataset\n",
    "            image_emb: (batch, 512) teacher (CLIP image) embeddings\n",
    "            temperature_teacher/student: softmax temperatures\n",
    "        \"\"\"\n",
    "        # Teacher distribution: image -> all captions\n",
    "        teacher_logits = torch.mm(image_emb, text_emb_all.t()) / temperature_teacher\n",
    "        teacher_probs = F.softmax(teacher_logits, dim=-1)\n",
    "        \n",
    "        # Student distribution: EEG -> all captions  \n",
    "        student_logits = torch.mm(eeg_emb, text_emb_all.t()) / temperature_student\n",
    "        student_log_probs = F.log_softmax(student_logits, dim=-1)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean')\n",
    "        \n",
    "        return kl_loss\n",
    "\n",
    "print(\"âœ“ Loss functions implemented:\")\n",
    "print(\"  - Cosine similarity (KD similarity-based)\")\n",
    "print(\"  - Debiased contrastive (InfoNCE with soft negatives)\")\n",
    "print(\"  - KL divergence (KD logit-based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/5] Setting up evaluation function...\n",
      "âœ“ Evaluation function ready\n",
      "\n",
      "============================================================\n",
      "SETUP COMPLETE - Ready to train Task 2B!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[4/5] Setting up evaluation function...\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_eeg_caption_retrieval(eeg_encoder, projection_head, clip_model, \n",
    "                                   clip_tokenizer, dataloader, device, k=5):\n",
    "    \"\"\"\n",
    "    Evaluate EEG-Caption retrieval\n",
    "    Returns Class-Aware Recall@K\n",
    "    \"\"\"\n",
    "    eeg_encoder.eval()\n",
    "    projection_head.eval()\n",
    "    clip_model.eval()\n",
    "    \n",
    "    all_eeg_emb = []\n",
    "    all_text_emb = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=f\"Evaluating Recall@{k}\", leave=False):\n",
    "        eeg_data = batch['eeg'].to(device)\n",
    "        captions = batch['caption']\n",
    "        labels = batch['label']\n",
    "        subject_ids = batch['subject_id'].to(device)\n",
    "        \n",
    "        # Get EEG embedding (same as training)\n",
    "        x = eeg_data\n",
    "        \n",
    "        # Dimension handling\n",
    "        if x.shape[1] == 500 and x.shape[2] == 122:\n",
    "            x = x.permute(0, 2, 1)\n",
    "        if x.shape[2] > 500:\n",
    "            x = x[:, :, :500]\n",
    "        \n",
    "        # Tokenize\n",
    "        x = eeg_encoder.tokenizer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        b, seq_len, _ = x.shape\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = eeg_encoder.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + eeg_encoder.pos_embedding[:, :seq_len + 1, :]\n",
    "        \n",
    "        # Transformer\n",
    "        x = eeg_encoder.transformer(x)\n",
    "        \n",
    "        # Extract CLS token\n",
    "        eeg_cls = x[:, 0, :]\n",
    "        \n",
    "        # Project to CLIP space\n",
    "        eeg_projected = projection_head(eeg_cls)\n",
    "        \n",
    "        # Get text embedding\n",
    "        text_inputs = clip_tokenizer(captions, padding=True, truncation=True,\n",
    "                                     return_tensors=\"pt\", max_length=77)\n",
    "        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "        text_emb = clip_model.get_text_features(**text_inputs)\n",
    "        text_emb = F.normalize(text_emb, p=2, dim=-1)\n",
    "        \n",
    "        all_eeg_emb.append(eeg_projected.cpu())\n",
    "        all_text_emb.append(text_emb.cpu())\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    eeg_emb = torch.cat(all_eeg_emb, dim=0).to(device)\n",
    "    text_emb = torch.cat(all_text_emb, dim=0).to(device)\n",
    "    labels = torch.cat(all_labels, dim=0).to(device)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarities = torch.mm(eeg_emb, text_emb.t())  # (N, N)\n",
    "    \n",
    "    # Get top-K indices for each query\n",
    "    top_k_indices = torch.argsort(similarities, dim=1, descending=True)[:, :k]\n",
    "    \n",
    "    # Compute class-aware Recall@K\n",
    "    correct = 0\n",
    "    for i in range(len(labels)):\n",
    "        query_label = labels[i]\n",
    "        retrieved_labels = labels[top_k_indices[i]]\n",
    "        if (retrieved_labels == query_label).any():\n",
    "            correct += 1\n",
    "    \n",
    "    recall = 100.0 * correct / len(labels)\n",
    "    return recall\n",
    "\n",
    "print(\"âœ“ Evaluation function ready\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SETUP COMPLETE - Ready to train Task 2B!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/5] Setting up training function...\n",
      "âœ“ Training function ready\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[5/5] Setting up training function...\")\n",
    "\n",
    "def train_task2b(eeg_encoder, projection_head, clip_model, clip_tokenizer, \n",
    "                 train_loader, val_loader,\n",
    "                 strategy='frozen', num_epochs=20, lr=1e-3, \n",
    "                 loss_weights={'cosine': 1.0, 'contrastive': 1.0}):\n",
    "    \"\"\"\n",
    "    Train EEG-Caption alignment with different CLIP fine-tuning strategies\n",
    "    \n",
    "    Args:\n",
    "        strategy: 'frozen', 'partial_unfreeze'\n",
    "        loss_weights: dict with keys 'cosine', 'contrastive'\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with strategy: {strategy.upper()}\")\n",
    "    print(f\"Loss weights: {loss_weights}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Configure CLIP based on strategy\n",
    "    if strategy == 'frozen':\n",
    "        print(\"Strategy: CLIP fully frozen\")\n",
    "        for param in clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        trainable_params = list(projection_head.parameters())\n",
    "        \n",
    "    elif strategy == 'partial_unfreeze':\n",
    "        print(\"Strategy: Unfreezing last 2 CLIP layers + text projection\")\n",
    "        # Freeze all first\n",
    "        for param in clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze last 2 transformer layers\n",
    "        for layer in clip_model.text_model.encoder.layers[-2:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Unfreeze text projection\n",
    "        if hasattr(clip_model, 'text_projection'):\n",
    "            for param in clip_model.text_projection.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        trainable_params = list(projection_head.parameters())\n",
    "        for layer in clip_model.text_model.encoder.layers[-2:]:\n",
    "            trainable_params.extend(list(layer.parameters()))\n",
    "        if hasattr(clip_model, 'text_projection'):\n",
    "            trainable_params.extend(list(clip_model.text_projection.parameters()))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    total_trainable = sum(p.numel() for p in trainable_params if p.requires_grad)\n",
    "    print(f\"\\nTrainable parameters: {total_trainable:,}\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(trainable_params, lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Loss computer\n",
    "    loss_computer = Task2BLosses()\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_recall = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # ===== TRAINING =====\n",
    "        eeg_encoder.eval()  # Keep EEG encoder frozen\n",
    "        projection_head.train()\n",
    "        if strategy != 'frozen':\n",
    "            clip_model.train()\n",
    "        else:\n",
    "            clip_model.eval()\n",
    "        \n",
    "        epoch_losses = {'total': 0, 'cosine': 0, 'contrastive': 0}\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "            eeg_data = batch['eeg'].to(DEVICE)\n",
    "            captions = batch['caption']\n",
    "            labels = batch['label'].to(DEVICE)\n",
    "            subject_ids = batch['subject_id'].to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get EEG embedding using model's forward pass\n",
    "            # We need to extract intermediate representation before classification heads\n",
    "            with torch.no_grad():\n",
    "                # Forward through tokenizer and transformer\n",
    "                x = eeg_data\n",
    "                \n",
    "                # Dimension handling (from model's forward)\n",
    "                if x.shape[1] == 500 and x.shape[2] == 122:\n",
    "                    x = x.permute(0, 2, 1)\n",
    "                if x.shape[2] > 500:\n",
    "                    x = x[:, :, :500]\n",
    "                \n",
    "                # Tokenize\n",
    "                x = eeg_encoder.tokenizer(x)\n",
    "                x = x.permute(0, 2, 1)\n",
    "                \n",
    "                b, seq_len, _ = x.shape\n",
    "                \n",
    "                # Add CLS token\n",
    "                cls_tokens = eeg_encoder.cls_token.expand(b, -1, -1)\n",
    "                x = torch.cat((cls_tokens, x), dim=1)\n",
    "                \n",
    "                # Add positional embedding\n",
    "                x = x + eeg_encoder.pos_embedding[:, :seq_len + 1, :]\n",
    "                \n",
    "                # Transformer\n",
    "                x = eeg_encoder.transformer(x)\n",
    "                \n",
    "                # Extract CLS token\n",
    "                eeg_cls = x[:, 0, :]  # (batch, 128)\n",
    "            \n",
    "            # Project to CLIP space\n",
    "            eeg_projected = projection_head(eeg_cls)  # (batch, 512), normalized\n",
    "            \n",
    "            # Get text embeddings from CLIP\n",
    "            text_inputs = clip_tokenizer(captions, padding=True, truncation=True,\n",
    "                                        return_tensors=\"pt\", max_length=77)\n",
    "            text_inputs = {k: v.to(DEVICE) for k, v in text_inputs.items()}\n",
    "            \n",
    "            text_emb = clip_model.get_text_features(**text_inputs)\n",
    "            text_emb = F.normalize(text_emb, p=2, dim=-1)\n",
    "            \n",
    "            # Compute losses\n",
    "            total_loss = 0\n",
    "            \n",
    "            if loss_weights.get('cosine', 0) > 0:\n",
    "                loss_cos = loss_computer.cosine_similarity_loss(eeg_projected, text_emb)\n",
    "                total_loss += loss_weights['cosine'] * loss_cos\n",
    "                epoch_losses['cosine'] += loss_cos.item()\n",
    "            \n",
    "            if loss_weights.get('contrastive', 0) > 0:\n",
    "                loss_contrast = loss_computer.contrastive_loss_debiased(\n",
    "                    eeg_projected, text_emb, labels)\n",
    "                total_loss += loss_weights['contrastive'] * loss_contrast\n",
    "                epoch_losses['contrastive'] += loss_contrast.item()\n",
    "            \n",
    "            epoch_losses['total'] += total_loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Print epoch stats\n",
    "        avg_total = epoch_losses['total'] / num_batches\n",
    "        avg_cos = epoch_losses['cosine'] / num_batches\n",
    "        avg_contrast = epoch_losses['contrastive'] / num_batches\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Total Loss: {avg_total:.4f} \" +\n",
    "              f\"(Cosine: {avg_cos:.4f}, Contrastive: {avg_contrast:.4f})\")\n",
    "        \n",
    "        # ===== VALIDATION =====\n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:  # Validate every 5 epochs and last epoch\n",
    "            val_recall = evaluate_eeg_caption_retrieval(\n",
    "                eeg_encoder, projection_head, clip_model, clip_tokenizer, \n",
    "                val_loader, DEVICE, k=5)\n",
    "            \n",
    "            print(f\"  â†’ Val Recall@5: {val_recall:.2f}%\")\n",
    "            \n",
    "            if val_recall > best_val_recall:\n",
    "                best_val_recall = val_recall\n",
    "                save_path = f'task2b_best_{strategy}.pth'\n",
    "                torch.save({\n",
    "                    'projection_head': projection_head.state_dict(),\n",
    "                    'epoch': epoch + 1,\n",
    "                    'strategy': strategy,\n",
    "                    'val_recall': val_recall,\n",
    "                    'loss_weights': loss_weights\n",
    "                }, save_path)\n",
    "                print(f\"  âœ“ Best model saved to {save_path}!\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training complete!\")\n",
    "    print(f\"Best Val Recall@5: {best_val_recall:.2f}%\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return projection_head, best_val_recall\n",
    "\n",
    "print(\"âœ“ Training function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING TASK 2B - STRATEGY: FROZEN CLIP\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training with strategy: PARTIAL_UNFREEZE\n",
      "Loss weights: {'cosine': 0.5, 'contrastive': 0.5}\n",
      "============================================================\n",
      "\n",
      "Strategy: Unfreezing last 2 CLIP layers + text projection\n",
      "\n",
      "Trainable parameters: 6,731,520\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659465ff16034c99bcf013ca13483d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Total Loss: 1.7152 (Cosine: 0.0376, Contrastive: 3.3927)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1261ea7177534c8dba5ca3163c16cfb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Total Loss: 1.6948 (Cosine: 0.0240, Contrastive: 3.3655)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f0fcb202154da688ab5ec6360158c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Total Loss: 1.6915 (Cosine: 0.0243, Contrastive: 3.3587)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1687f38032461fa31526980bf8c284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Total Loss: 1.6889 (Cosine: 0.0249, Contrastive: 3.3529)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8c8248a5e64c3d9725ef4c1c58e6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Total Loss: 1.6863 (Cosine: 0.0257, Contrastive: 3.3469)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc1c3376a5e4da5b4da7c1c9ba2d58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 13.67%\n",
      "  âœ“ Best model saved to task2b_best_partial_unfreeze.pth!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b69622ad267457cbceb856d5c97f414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Total Loss: 1.6846 (Cosine: 0.0267, Contrastive: 3.3425)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a86c6da4d24339b63ddd5aacb0de32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Total Loss: 1.6835 (Cosine: 0.0268, Contrastive: 3.3402)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a3cb7570c74fa28d6f08d7209f5bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Total Loss: 1.6822 (Cosine: 0.0271, Contrastive: 3.3374)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88aa95bc00104b9f917a25d7118a829e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Total Loss: 1.6809 (Cosine: 0.0275, Contrastive: 3.3343)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a204ce43a9f4b57bd1f2848ff2b5dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Total Loss: 1.6805 (Cosine: 0.0274, Contrastive: 3.3336)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8adbdf60ac249c881ffa4f98134df5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 13.23%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912fb79ef2ba45e490ff1a9568c0d1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Total Loss: 1.6790 (Cosine: 0.0280, Contrastive: 3.3300)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbf5072e5b147d4a435fbdaa359a271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Total Loss: 1.6777 (Cosine: 0.0285, Contrastive: 3.3268)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462db60e24b84faa9a023dcc96278654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Total Loss: 1.6759 (Cosine: 0.0288, Contrastive: 3.3230)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68dc10a4c064e93bd6d4fc38566f910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Total Loss: 1.6749 (Cosine: 0.0290, Contrastive: 3.3207)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba99098a45d491ebaef672b395d95b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Total Loss: 1.6740 (Cosine: 0.0293, Contrastive: 3.3188)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089e5e8911754a71aebe3b5385abc9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 14.81%\n",
      "  âœ“ Best model saved to task2b_best_partial_unfreeze.pth!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2814b1b6149a45d5a3696ed6d299d270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Total Loss: 1.6730 (Cosine: 0.0298, Contrastive: 3.3162)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94f8ec31b87401d9c159394171ee9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Total Loss: 1.6725 (Cosine: 0.0303, Contrastive: 3.3147)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304ac2bdb4e84fb4a0eae1537d8f044a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Total Loss: 1.6698 (Cosine: 0.0311, Contrastive: 3.3085)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd37d55709d84e05adbb66caa515f059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Total Loss: 1.6684 (Cosine: 0.0339, Contrastive: 3.3029)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acf48aea20945edbb33da24c318c0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/20:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Total Loss: 1.6660 (Cosine: 0.0356, Contrastive: 3.2964)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0e03b72f264948afc6b8c76f19c16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 14.85%\n",
      "  âœ“ Best model saved to task2b_best_partial_unfreeze.pth!\n",
      "\n",
      "============================================================\n",
      "Training complete!\n",
      "Best Val Recall@5: 14.85%\n",
      "============================================================\n",
      "\n",
      "\n",
      "âœ“ Training completed with best validation Recall@5: 14.85%\n"
     ]
    }
   ],
   "source": [
    "# Create fresh projection head\n",
    "projection_head_frozen = EEGToClipProjection(eeg_dim=128, clip_dim=512).to(DEVICE)\n",
    "\n",
    "# Train with frozen CLIP (baseline)\n",
    "trained_projection, best_recall = train_task2b(\n",
    "    eeg_encoder=eeg_encoder,\n",
    "    projection_head=projection_head_frozen,\n",
    "    clip_model=clip_model,\n",
    "    clip_tokenizer=clip_tokenizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    strategy='partial_unfreeze',\n",
    "    num_epochs=20,\n",
    "    lr=1e-3,\n",
    "    loss_weights={'cosine': 0.5, 'contrastive': 0.5}\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Training completed with best validation Recall@5: {best_recall:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frozen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241m14.12\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mfrozen\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'frozen' is not defined"
     ]
    }
   ],
   "source": [
    "# 14.12% - frozen\n",
    "# 14.85% - PARTIAL_UNFREEZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 2: FINE-TUNING WITH UNFROZEN EEG ENCODER\n",
      "============================================================\n",
      "\n",
      "[1] Loading best projection head from Stage 1...\n",
      "âœ“ Loaded projection head with val Recall@5: 14.12%\n",
      "\n",
      "[2] Loading EEG encoder (will be unfrozen)...\n",
      "âœ“ EEG encoder loaded\n",
      "âœ“ EEG encoder UNFROZEN\n",
      "âœ“ CLIP last 2 layers UNFROZEN\n",
      "\n",
      "Total trainable parameters: 7,947,012\n",
      "\n",
      "[6] Starting Stage 2 training...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/gulavani/.conda/envs/eegenv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945236ccedce4209bdb00c35a91acc5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 1/15:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Loss: 1.8453 (Cos: 0.3225, Contrast: 3.3680)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4e8b68250f4f39949c3b4a0270ecc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 2/15:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 - Loss: 1.7068 (Cos: 0.0661, Contrast: 3.3474)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f428d21a7a48cf9794db67409c0814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 3/15:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 - Loss: 1.6928 (Cos: 0.0421, Contrast: 3.3434)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c567240df745639ef702757da57c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 14.92%\n",
      "  âœ“ NEW BEST! Saved to task2b_best_stage2_unfrozen.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7871a9d7cab04200a0796818838f4e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 4/15:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 - Loss: 1.6877 (Cos: 0.0371, Contrast: 3.3383)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40952e73ebf8467692357a2d2f6fe677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 5/15:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 - Loss: 1.6844 (Cos: 0.0367, Contrast: 3.3321)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297a39e9e0bf4f63937b17a044f6bd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 6/15:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 - Loss: 1.6843 (Cos: 0.0348, Contrast: 3.3338)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04377e42aac4e9aa8c56979fefee539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 15.27%\n",
      "  âœ“ NEW BEST! Saved to task2b_best_stage2_unfrozen.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59061227417d48ec9b8929f2dbc29ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 7/15:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 - Loss: 1.6827 (Cos: 0.0350, Contrast: 3.3304)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c06e0e70f84f02951a96aac1cd4ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 8/15:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 - Loss: 1.6806 (Cos: 0.0351, Contrast: 3.3261)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87296081242d4afbb395a04780fbe345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 9/15:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 - Loss: 1.6806 (Cos: 0.0351, Contrast: 3.3262)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336d73927edb417f9e9d7e39a595b2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 15.63%\n",
      "  âœ“ NEW BEST! Saved to task2b_best_stage2_unfrozen.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f561e8b753544e28a2f300afcd94fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 10/15:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 - Loss: 1.6814 (Cos: 0.0343, Contrast: 3.3285)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662c0d5a058243718984980d1653080c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 11/15:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 - Loss: 1.6773 (Cos: 0.0354, Contrast: 3.3191)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d032778524b84c489e02c70b260ed16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 12/15:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 - Loss: 1.6776 (Cos: 0.0352, Contrast: 3.3200)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97b1e56ec8848c491ca5a3a33483d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 16.06%\n",
      "  âœ“ NEW BEST! Saved to task2b_best_stage2_unfrozen.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6636fc985ffa423c89cf5e4a0ec1b626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 13/15:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 - Loss: 1.6778 (Cos: 0.0345, Contrast: 3.3210)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14158ac834d24ebb92cdaad2a785a645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 14/15:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 - Loss: 1.6781 (Cos: 0.0345, Contrast: 3.3216)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684f0b1c4e094b8daf0edae2f85db141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage2 Epoch 15/15:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Loss: 1.6767 (Cos: 0.0353, Contrast: 3.3182)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f03290681e84babb679db4ffd48a873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@5:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Val Recall@5: 17.13%\n",
      "  âœ“ NEW BEST! Saved to task2b_best_stage2_unfrozen.pth\n",
      "\n",
      "============================================================\n",
      "Stage 2 Training Complete!\n",
      "Stage 1 Best: 14.12%\n",
      "Stage 2 Best: 17.13%\n",
      "Improvement: 3.02%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STAGE 2: Fine-tune Everything Together\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 2: FINE-TUNING WITH UNFROZEN EEG ENCODER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Load BEST projection head from Stage 1\n",
    "print(\"\\n[1] Loading best projection head from Stage 1...\")\n",
    "checkpoint = torch.load('task2b_best_frozen.pth', map_location=DEVICE)\n",
    "projection_head_stage2 = EEGToClipProjection(eeg_dim=128, clip_dim=512).to(DEVICE)\n",
    "projection_head_stage2.load_state_dict(checkpoint['projection_head'])\n",
    "print(f\"âœ“ Loaded projection head with val Recall@5: {checkpoint['val_recall']:.2f}%\")\n",
    "\n",
    "# 2. Load EEG encoder (fresh copy, will be unfrozen)\n",
    "print(\"\\n[2] Loading EEG encoder (will be unfrozen)...\")\n",
    "eeg_encoder_stage2 = EEG_ViT_1D(num_subjects=13, num_classes=20).to(DEVICE)\n",
    "task1_checkpoint = torch.load('best_model_vit_1d.pth', map_location=DEVICE)\n",
    "if isinstance(task1_checkpoint, dict):\n",
    "    eeg_encoder_stage2.load_state_dict(task1_checkpoint['model_state_dict'])\n",
    "else:\n",
    "    eeg_encoder_stage2.load_state_dict(task1_checkpoint)\n",
    "print(\"âœ“ EEG encoder loaded\")\n",
    "\n",
    "# 3. Unfreeze EEG encoder\n",
    "for param in eeg_encoder_stage2.parameters():\n",
    "    param.requires_grad = True\n",
    "print(\"âœ“ EEG encoder UNFROZEN\")\n",
    "\n",
    "# 4. Unfreeze CLIP last 2 layers\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for layer in clip_model.text_model.encoder.layers[-2:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "if hasattr(clip_model, 'text_projection'):\n",
    "    for param in clip_model.text_projection.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "print(\"âœ“ CLIP last 2 layers UNFROZEN\")\n",
    "\n",
    "# 5. Setup optimizer with DIFFERENTIAL LEARNING RATES\n",
    "trainable_params = [\n",
    "    {'params': projection_head_stage2.parameters(), 'lr': 1e-4, 'name': 'projection'},\n",
    "    {'params': eeg_encoder_stage2.parameters(), 'lr': 1e-5, 'name': 'eeg_encoder'},\n",
    "]\n",
    "\n",
    "for layer in clip_model.text_model.encoder.layers[-2:]:\n",
    "    trainable_params.append({\n",
    "        'params': layer.parameters(), \n",
    "        'lr': 5e-5, \n",
    "        'name': 'clip_layers'\n",
    "    })\n",
    "\n",
    "if hasattr(clip_model, 'text_projection'):\n",
    "    trainable_params.append({\n",
    "        'params': clip_model.text_projection.parameters(),\n",
    "        'lr': 5e-5,\n",
    "        'name': 'clip_projection'\n",
    "    })\n",
    "\n",
    "optimizer_stage2 = optim.AdamW(trainable_params, weight_decay=1e-4)\n",
    "\n",
    "total_params = sum(p.numel() for group in trainable_params for p in group['params'])\n",
    "print(f\"\\nTotal trainable parameters: {total_params:,}\")\n",
    "\n",
    "# 6. Training loop with early stopping\n",
    "print(\"\\n[6] Starting Stage 2 training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_recall_stage2 = checkpoint['val_recall']  # Start from Stage 1 best\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "loss_computer = Task2BLosses()\n",
    "\n",
    "NUM_EPOCHS_STAGE2 = 15\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_STAGE2):\n",
    "    # Training\n",
    "    eeg_encoder_stage2.train()\n",
    "    projection_head_stage2.train()\n",
    "    clip_model.train()\n",
    "    \n",
    "    epoch_losses = {'total': 0, 'cosine': 0, 'contrastive': 0}\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Stage2 Epoch {epoch+1}/{NUM_EPOCHS_STAGE2}\", leave=False):\n",
    "        eeg_data = batch['eeg'].to(DEVICE)\n",
    "        captions = batch['caption']\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        subject_ids = batch['subject_id'].to(DEVICE)\n",
    "        \n",
    "        optimizer_stage2.zero_grad()\n",
    "        \n",
    "        # Forward through EEG encoder (NOW TRAINABLE)\n",
    "        x = eeg_data\n",
    "        if x.shape[1] == 500 and x.shape[2] == 122:\n",
    "            x = x.permute(0, 2, 1)\n",
    "        if x.shape[2] > 500:\n",
    "            x = x[:, :, :500]\n",
    "        \n",
    "        x = eeg_encoder_stage2.tokenizer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        b, seq_len, _ = x.shape\n",
    "        \n",
    "        cls_tokens = eeg_encoder_stage2.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + eeg_encoder_stage2.pos_embedding[:, :seq_len + 1, :]\n",
    "        x = eeg_encoder_stage2.transformer(x)\n",
    "        eeg_cls = x[:, 0, :]\n",
    "        \n",
    "        # Project\n",
    "        eeg_projected = projection_head_stage2(eeg_cls)\n",
    "        \n",
    "        # CLIP text\n",
    "        text_inputs = clip_tokenizer(captions, padding=True, truncation=True,\n",
    "                                    return_tensors=\"pt\", max_length=77)\n",
    "        text_inputs = {k: v.to(DEVICE) for k, v in text_inputs.items()}\n",
    "        text_emb = clip_model.get_text_features(**text_inputs)\n",
    "        text_emb = F.normalize(text_emb, p=2, dim=-1)\n",
    "        \n",
    "        # Losses\n",
    "        loss_cos = loss_computer.cosine_similarity_loss(eeg_projected, text_emb)\n",
    "        loss_contrast = loss_computer.contrastive_loss_debiased(eeg_projected, text_emb, labels)\n",
    "        \n",
    "        total_loss = 0.5 * loss_cos + 0.5 * loss_contrast\n",
    "        \n",
    "        epoch_losses['cosine'] += loss_cos.item()\n",
    "        epoch_losses['contrastive'] += loss_contrast.item()\n",
    "        epoch_losses['total'] += total_loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient clipping (important for stability)\n",
    "        torch.nn.utils.clip_grad_norm_(eeg_encoder_stage2.parameters(), max_norm=1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(projection_head_stage2.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer_stage2.step()\n",
    "    \n",
    "    # Print epoch stats\n",
    "    avg_total = epoch_losses['total'] / num_batches\n",
    "    avg_cos = epoch_losses['cosine'] / num_batches\n",
    "    avg_contrast = epoch_losses['contrastive'] / num_batches\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS_STAGE2} - Loss: {avg_total:.4f} \" +\n",
    "          f\"(Cos: {avg_cos:.4f}, Contrast: {avg_contrast:.4f})\")\n",
    "    \n",
    "    # Validation\n",
    "    if (epoch + 1) % 3 == 0 or epoch == NUM_EPOCHS_STAGE2 - 1:\n",
    "        val_recall = evaluate_eeg_caption_retrieval(\n",
    "            eeg_encoder_stage2, projection_head_stage2, clip_model, \n",
    "            clip_tokenizer, val_loader, DEVICE, k=5)\n",
    "        \n",
    "        print(f\"  â†’ Val Recall@5: {val_recall:.2f}%\")\n",
    "        \n",
    "        if val_recall > best_val_recall_stage2:\n",
    "            best_val_recall_stage2 = val_recall\n",
    "            patience_counter = 0\n",
    "            \n",
    "            torch.save({\n",
    "                'eeg_encoder': eeg_encoder_stage2.state_dict(),\n",
    "                'projection_head': projection_head_stage2.state_dict(),\n",
    "                'epoch': epoch + 1,\n",
    "                'val_recall': val_recall,\n",
    "                'strategy': 'stage2_unfrozen'\n",
    "            }, 'task2b_best_stage2_unfrozen.pth')\n",
    "            \n",
    "            print(f\"  âœ“ NEW BEST! Saved to task2b_best_stage2_unfrozen.pth\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{patience})\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nâš ï¸ Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Stage 2 Training Complete!\")\n",
    "print(f\"Stage 1 Best: {checkpoint['val_recall']:.2f}%\")\n",
    "print(f\"Stage 2 Best: {best_val_recall_stage2:.2f}%\")\n",
    "print(f\"Improvement: {best_val_recall_stage2 - checkpoint['val_recall']:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (eegenv)",
   "language": "python",
   "name": "eegenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
