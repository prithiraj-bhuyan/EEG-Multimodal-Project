{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb52390d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./eeg-env/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in ./eeg-env/lib/python3.12/site-packages (2.9.0)\n",
      "Requirement already satisfied: pandas in ./eeg-env/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: pillow in ./eeg-env/lib/python3.12/site-packages (12.0.0)\n",
      "Requirement already satisfied: scikit-learn in ./eeg-env/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: bert_score in ./eeg-env/lib/python3.12/site-packages (0.3.13)\n",
      "Requirement already satisfied: tqdm in ./eeg-env/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in ./eeg-env/lib/python3.12/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./eeg-env/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./eeg-env/lib/python3.12/site-packages (from transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./eeg-env/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./eeg-env/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./eeg-env/lib/python3.12/site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in ./eeg-env/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./eeg-env/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./eeg-env/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./eeg-env/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./eeg-env/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./eeg-env/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./eeg-env/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./eeg-env/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./eeg-env/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./eeg-env/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./eeg-env/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./eeg-env/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./eeg-env/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./eeg-env/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./eeg-env/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./eeg-env/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./eeg-env/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./eeg-env/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./eeg-env/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./eeg-env/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./eeg-env/lib/python3.12/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./eeg-env/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./eeg-env/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./eeg-env/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in ./eeg-env/lib/python3.12/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./eeg-env/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./eeg-env/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./eeg-env/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./eeg-env/lib/python3.12/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./eeg-env/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./eeg-env/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: matplotlib in ./eeg-env/lib/python3.12/site-packages (from bert_score) (3.10.7)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./eeg-env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in ./eeg-env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./eeg-env/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./eeg-env/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./eeg-env/lib/python3.12/site-packages (from matplotlib->bert_score) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./eeg-env/lib/python3.12/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./eeg-env/lib/python3.12/site-packages (from matplotlib->bert_score) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./eeg-env/lib/python3.12/site-packages (from matplotlib->bert_score) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in ./eeg-env/lib/python3.12/site-packages (from matplotlib->bert_score) (3.2.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./eeg-env/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./eeg-env/lib/python3.12/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./eeg-env/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./eeg-env/lib/python3.12/site-packages (from requests->transformers) (2025.10.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to install required libraries\n",
    "!pip install transformers torch pandas pillow scikit-learn bert_score tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7194fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import average_precision_score\n",
    "from bert_score import score as bert_scorer\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc86da1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# Update these paths to match your PSC directory structure\n",
    "IMAGE_DIR = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/images'\n",
    "CAPTIONS_FILE = '/ocean/projects/cis250019p/gandotra/11785-gp-eeg/captions.txt'\n",
    "MODEL_ID = 'openai/clip-vit-base-patch32'\n",
    "BATCH_SIZE = 128  # Adjust based on your GPU memory\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8094e69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP model and processor loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load the processor (handles tokenization and image preprocessing)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Load the pretrained model\n",
    "model = CLIPModel.from_pretrained(MODEL_ID).to(device)\n",
    "\n",
    "# Set model to evaluation mode (disables dropout, etc.)\n",
    "model.eval()\n",
    "\n",
    "print(\"CLIP model and processor loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b37f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading captions from: /ocean/projects/cis250019p/gandotra/11785-gp-eeg/captions.txt\n",
      "Loaded 9825 unique image/caption pairs.\n",
      "Found 20 unique categories.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading captions from: {CAPTIONS_FILE}\")\n",
    "\n",
    "all_image_names = []\n",
    "all_captions = []\n",
    "all_categories = []\n",
    "\n",
    "# Use the loading logic from your EEGMultimodalDataset._load_captions\n",
    "# This parses the tab-separated file correctly.\n",
    "captions_data = {}\n",
    "try:\n",
    "    with open(CAPTIONS_FILE, 'r') as f:\n",
    "        header = next(f) # Skip header\n",
    "        \n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 4:\n",
    "                # Based on your _load_captions method:\n",
    "                category = parts[1]\n",
    "                img_name = parts[2]\n",
    "                caption = '\\t'.join(parts[3:]).strip()\n",
    "                \n",
    "                # Store in a dict to get unique image_name -> (caption, category) pairs\n",
    "                # This matches the set of ~10,000 unique images\n",
    "                captions_data[img_name] = (caption, category)\n",
    "    \n",
    "    # Now, convert the unique dictionary items into parallel lists\n",
    "    # The order is fixed and will match for images and text\n",
    "    for img_name, (caption, category) in captions_data.items():\n",
    "        all_image_names.append(img_name)\n",
    "        all_captions.append(caption)\n",
    "        all_categories.append(category)\n",
    "\n",
    "    N = len(all_captions)\n",
    "    unique_categories = sorted(list(set(all_categories)))\n",
    "    \n",
    "    # Create mappings for class-aware metrics\n",
    "    category_to_idx = {cat: i for i, cat in enumerate(unique_categories)}\n",
    "    all_category_indices = [category_to_idx[cat] for cat in all_categories]\n",
    "    \n",
    "    print(f\"Loaded {N} unique image/caption pairs.\")\n",
    "    print(f\"Found {len(unique_categories)} unique categories.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {CAPTIONS_FILE}: {e}\")\n",
    "    print(\"Please ensure the file path is correct and it is a TAB-separated file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ecea1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b904275ef137447f8b0c5c516caab326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating text embeddings:   0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embeddings shape: torch.Size([9825, 512])\n"
     ]
    }
   ],
   "source": [
    "all_text_embeds = []\n",
    "with torch.no_grad():\n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, N, BATCH_SIZE), desc=\"Generating text embeddings\"):\n",
    "        batch_captions = all_captions[i : i + BATCH_SIZE]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        inputs = processor(text=batch_captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=77).to(device)\n",
    "        \n",
    "        # Get text features\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "        \n",
    "        # Normalize the embeddings (L2-norm)\n",
    "        text_features = F.normalize(text_features, p=2, dim=-1)\n",
    "        \n",
    "        all_text_embeds.append(text_features.cpu())\n",
    "\n",
    "# Concatenate all batch embeddings into a single tensor\n",
    "text_embeds = torch.cat(all_text_embeds, dim=0)\n",
    "print(f\"Text embeddings shape: {text_embeds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c54bad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875138bf29cf4af0b7bca35e9bddcb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating image embeddings:   0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embeddings shape: torch.Size([9825, 512])\n"
     ]
    }
   ],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading images.\"\"\"\n",
    "    def __init__(self, image_names, image_dir, processor):\n",
    "        self.image_names = image_names\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.extensions = ['.jpg', '.jpeg', '.png', '.JPEG'] # From your _find_image_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def _find_image_path(self, img_base_name):\n",
    "        \"\"\"Find image file with any extension\"\"\"\n",
    "        for ext in self.extensions:\n",
    "            img_path = os.path.join(self.image_dir, img_base_name + ext)\n",
    "            if os.path.exists(img_path):\n",
    "                return img_path\n",
    "        # If no extension worked, try the original name (in case it had .jpg)\n",
    "        img_path = os.path.join(self.image_dir, img_base_name)\n",
    "        if os.path.exists(img_path):\n",
    "            return img_path\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_base_name = self.image_names[idx]\n",
    "        img_path = self._find_image_path(img_base_name)\n",
    "        \n",
    "        if img_path is None:\n",
    "            print(f\"Warning: Could not find image file for {img_base_name}\")\n",
    "            return torch.zeros((3, 224, 224))\n",
    "            \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            # Process the image\n",
    "            processed_image = self.processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "            return processed_image\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load image {img_path}. Error: {e}\")\n",
    "            # Return a tensor of zeros if image is corrupt\n",
    "            return torch.zeros((3, 224, 224))\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "image_dataset = ImageDataset(all_image_names, IMAGE_DIR, processor)\n",
    "image_loader = DataLoader(image_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "all_image_embeds = []\n",
    "with torch.no_grad():\n",
    "    for batch_images in tqdm(image_loader, desc=\"Generating image embeddings\"):\n",
    "        batch_images = batch_images.to(device)\n",
    "        \n",
    "        # Get image features\n",
    "        image_features = model.get_image_features(pixel_values=batch_images)\n",
    "        \n",
    "        # Normalize the embeddings (L2-norm)\n",
    "        image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "        \n",
    "        all_image_embeds.append(image_features.cpu())\n",
    "\n",
    "# Concatenate all batch embeddings into a single tensor\n",
    "image_embeds = torch.cat(all_image_embeds, dim=0)\n",
    "print(f\"Image embeddings shape: {image_embeds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4e77cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating similarity matrix...\n",
      "Similarity matrix shape: torch.Size([9825, 9825])\n"
     ]
    }
   ],
   "source": [
    "# Move embeddings to GPU for fast matrix multiplication\n",
    "image_embeds_gpu = image_embeds.to(device)\n",
    "text_embeds_gpu = text_embeds.to(device)\n",
    "\n",
    "# Calculate the cosine similarity matrix (N_images x N_captions)\n",
    "# sim = (image_embeds @ text_embeds.T)\n",
    "print(\"Calculating similarity matrix...\")\n",
    "similarity_matrix = (image_embeds_gpu @ text_embeds_gpu.T).cpu()\n",
    "\n",
    "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79eb42ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ“ˆ Recall@K Evaluation ---\n",
      "\n",
      "Instance-Level Recall (Exact Match):\n",
      "Recall@1: 19.77%\n",
      "Recall@3: 32.71%\n",
      "Recall@5: 39.49%\n",
      "\n",
      "Class-Aware Recall (Correct Category):\n",
      "Recall@1: 96.99%\n",
      "Recall@3: 98.25%\n",
      "Recall@5: 98.51%\n"
     ]
    }
   ],
   "source": [
    "print(\"--- ðŸ“ˆ Recall@K Evaluation ---\")\n",
    "\n",
    "# Get the indices of the top 5 most similar captions for each image\n",
    "# dim=1 means we are ranking along the caption dimension\n",
    "top_k_indices = torch.topk(similarity_matrix, k=5, dim=1).indices\n",
    "\n",
    "# Ground truth is that image 'i' matches caption 'i'\n",
    "gt_indices = torch.arange(N).unsqueeze(1)\n",
    "\n",
    "# --- Instance-Level Recall@K ---\n",
    "# Check if the correct caption index is in the top-k results\n",
    "r1_hits = (top_k_indices[:, :1] == gt_indices).any(dim=1).float().sum()\n",
    "r3_hits = (top_k_indices[:, :3] == gt_indices).any(dim=1).float().sum()\n",
    "r5_hits = (top_k_indices[:, :5] == gt_indices).any(dim=1).float().sum()\n",
    "\n",
    "print(f\"\\nInstance-Level Recall (Exact Match):\")\n",
    "print(f\"Recall@1: {100 * r1_hits / N:.2f}%\")\n",
    "print(f\"Recall@3: {100 * r3_hits / N:.2f}%\")\n",
    "print(f\"Recall@5: {100 * r5_hits / N:.2f}%\")\n",
    "\n",
    "# --- Class-Aware Recall@K ---\n",
    "# We need to check if any of the top-k retrieved captions belong to the same class as the ground truth\n",
    "gt_categories = torch.tensor(all_category_indices, dtype=torch.long)\n",
    "retrieved_categories = torch.tensor(all_category_indices)[top_k_indices]\n",
    "\n",
    "# Check if the correct category index is in the top-k retrieved categories\n",
    "class_r1_hits = (retrieved_categories[:, :1] == gt_categories.unsqueeze(1)).any(dim=1).float().sum()\n",
    "class_r3_hits = (retrieved_categories[:, :3] == gt_categories.unsqueeze(1)).any(dim=1).float().sum()\n",
    "class_r5_hits = (retrieved_categories[:, :5] == gt_categories.unsqueeze(1)).any(dim=1).float().sum()\n",
    "\n",
    "print(f\"\\nClass-Aware Recall (Correct Category):\")\n",
    "print(f\"Recall@1: {100 * class_r1_hits / N:.2f}%\")\n",
    "print(f\"Recall@3: {100 * class_r3_hits / N:.2f}%\")\n",
    "print(f\"Recall@5: {100 * class_r5_hits / N:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e91916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ðŸ“ˆ Mean Average Precision (MAP) Evaluation ---\n",
      "Caption-Level MAP (Instance): 0.2978\n",
      "Class-Aware MAP: 0.8305\n",
      "\n",
      "Per-Class MAP (mean of class MAPs): 0.8308\n",
      "Per-Class MAP (details):\n",
      "category\n",
      "aeroplane      0.952461\n",
      "bicycle        0.923959\n",
      "bird           0.835170\n",
      "boat           0.871740\n",
      "bottle         0.906255\n",
      "bus            0.927476\n",
      "car            0.690873\n",
      "cat            0.816477\n",
      "chair          0.727544\n",
      "cow            0.760897\n",
      "diningtable    0.932633\n",
      "dog            0.721748\n",
      "flower         0.965426\n",
      "horse          0.943866\n",
      "motorbike      0.768518\n",
      "person         0.532131\n",
      "sheep          0.721595\n",
      "sofa           0.741440\n",
      "train          0.920891\n",
      "tvmonitor      0.954820\n",
      "Name: ap_class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- ðŸ“ˆ Mean Average Precision (MAP) Evaluation ---\")\n",
    "\n",
    "# --- Caption-Level MAP (Instance) ---\n",
    "# y_true[i, j] = 1 if j is the correct caption for image i, else 0\n",
    "y_true_instance = np.eye(N, dtype=int)\n",
    "aps_instance = []\n",
    "for i in range(N):\n",
    "    y_true_i = y_true_instance[i]\n",
    "    y_score_i = similarity_matrix[i].numpy()\n",
    "    aps_instance.append(average_precision_score(y_true_i, y_score_i))\n",
    "\n",
    "map_instance = np.mean(aps_instance)\n",
    "print(f\"Caption-Level MAP (Instance): {map_instance:.4f}\")\n",
    "\n",
    "# --- Class-Aware MAP ---\n",
    "# y_true[i, j] = 1 if caption j is in the same class as image i, else 0\n",
    "y_true_class = np.zeros((N, N), dtype=int)\n",
    "gt_categories_np = gt_categories.numpy()\n",
    "for i in range(N):\n",
    "    correct_class_idx = gt_categories_np[i]\n",
    "    # Find all captions with the same class\n",
    "    relevant_indices = np.where(gt_categories_np == correct_class_idx)[0]\n",
    "    y_true_class[i, relevant_indices] = 1\n",
    "\n",
    "aps_class = []\n",
    "for i in range(N):\n",
    "    y_true_i = y_true_class[i]\n",
    "    y_score_i = similarity_matrix[i].numpy()\n",
    "    aps_class.append(average_precision_score(y_true_i, y_score_i))\n",
    "\n",
    "map_class_aware = np.mean(aps_class)\n",
    "print(f\"Class-Aware MAP: {map_class_aware:.4f}\")\n",
    "\n",
    "# --- Per-Class MAP ---\n",
    "df_results = pd.DataFrame({\n",
    "    'category': all_categories,\n",
    "    'ap_class': aps_class\n",
    "})\n",
    "map_per_class = df_results.groupby('category')['ap_class'].mean()\n",
    "print(f\"\\nPer-Class MAP (mean of class MAPs): {map_per_class.mean():.4f}\")\n",
    "print(\"Per-Class MAP (details):\")\n",
    "print(map_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f86fe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ðŸ“ˆ BERTScore Evaluation ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2216bdd923394697b92c19036fef961d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ff6d2f91d14e4d9666893867e4b952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72122c38a3ca4076ac84297ac554f474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b13bba90924071aecc51fb38377fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30477164560847d8aca2e3f6c23ada03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.86 seconds, 2021.29 sentences/sec\n",
      "\n",
      "Average BERTScore F1 (Top-1 vs. GT): 0.8874\n",
      "Success Rate (F1 > 0.7): 99.97%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- ðŸ“ˆ BERTScore Evaluation ---\")\n",
    "\n",
    "# Get the top-1 retrieved caption for each image\n",
    "top_1_indices = top_k_indices[:, 0].tolist()\n",
    "retrieved_captions = [all_captions[i] for i in top_1_indices]\n",
    "ground_truth_captions = all_captions # Assumes order is matched\n",
    "\n",
    "# ----- MODIFIED LINE -----\n",
    "# We specify a smaller, faster model (DistilBERT)\n",
    "# This will download a ~250MB model instead of the 1.42GB one.\n",
    "(P, R, F1) = bert_scorer(retrieved_captions, ground_truth_captions,\n",
    "                       model_type='distilbert-base-uncased', \n",
    "                       lang='en', \n",
    "                       verbose=True)\n",
    "# -------------------------\n",
    "\n",
    "print(f\"\\nAverage BERTScore F1 (Top-1 vs. GT): {F1.mean():.4f}\")\n",
    "\n",
    "# Report success rate for F1 > 0.7\n",
    "success_rate = (F1 > 0.7).float().mean()\n",
    "print(f\"Success Rate (F1 > 0.7): {100 * success_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "858babdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ðŸ“ˆ CLIPScore Distribution Analysis ---\n",
      "\n",
      "Matched (Correct) Pairs:\n",
      "  Mean: 0.3021\n",
      "  Std:  0.0304\n",
      "  Min:  0.1528\n",
      "  Max:  0.4579\n",
      "\n",
      "Mismatched (Incorrect) Pairs:\n",
      "  Mean: 0.1446\n",
      "  Std:  0.0430\n",
      "  Min:  -0.0691\n",
      "  Max:  0.4197\n",
      "\n",
      "Analysis: A good separation between the matched and mismatched mean scores indicates\n",
      "that CLIP can effectively distinguish correct pairs from incorrect ones.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- ðŸ“ˆ CLIPScore Distribution Analysis ---\")\n",
    "\n",
    "# Matched scores are the diagonal of the similarity matrix\n",
    "matched_scores = similarity_matrix.diag()\n",
    "\n",
    "# Mismatched scores are all off-diagonal elements\n",
    "# Create a mask to select only off-diagonal elements\n",
    "mask = torch.ones_like(similarity_matrix, dtype=torch.bool)\n",
    "mask.fill_diagonal_(False)\n",
    "mismatched_scores = similarity_matrix[mask]\n",
    "\n",
    "print(f\"\\nMatched (Correct) Pairs:\")\n",
    "print(f\"  Mean: {matched_scores.mean():.4f}\")\n",
    "print(f\"  Std:  {matched_scores.std():.4f}\")\n",
    "print(f\"  Min:  {matched_scores.min():.4f}\")\n",
    "print(f\"  Max:  {matched_scores.max():.4f}\")\n",
    "\n",
    "print(f\"\\nMismatched (Incorrect) Pairs:\")\n",
    "print(f\"  Mean: {mismatched_scores.mean():.4f}\")\n",
    "print(f\"  Std:  {mismatched_scores.std():.4f}\")\n",
    "print(f\"  Min:  {mismatched_scores.min():.4f}\")\n",
    "print(f\"  Max:  {mismatched_scores.max():.4f}\")\n",
    "\n",
    "print(\"\\nAnalysis: A good separation between the matched and mismatched mean scores indicates\")\n",
    "print(\"that CLIP can effectively distinguish correct pairs from incorrect ones.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip eeg",
   "language": "python",
   "name": "eeg-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
